{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["epqaV4bo4uuI","cGX9iHAz_UKX","bS_7sy-8BMBu"],"gpuType":"T4","machine_shape":"hm","toc_visible":true,"authorship_tag":"ABX9TyONJIriUyYLjhHPXgNRzTFl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fdf81665426a482297cf28f4fa4bd8c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f73b02f4fc7142e8b4e967c37d25a79a","IPY_MODEL_74b346aa36f04165a3b4da436951bf48","IPY_MODEL_14fc8552070b4781943be0d0c703b98c"],"layout":"IPY_MODEL_7ceda695f62f425ab65931dbcfcd2693"}},"f73b02f4fc7142e8b4e967c37d25a79a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f341ba52a34f24a39731aa47477f61","placeholder":"​","style":"IPY_MODEL_f3e572e9160d4046bba586cf31959cea","value":"config.json: 100%"}},"74b346aa36f04165a3b4da436951bf48":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_829b7b441867436e91f8c9c3c7f78ecc","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb87edf4edf7407094794ba3dac75156","value":665}},"14fc8552070b4781943be0d0c703b98c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e45ad37d3c8e45d19936d9281a54334c","placeholder":"​","style":"IPY_MODEL_18301c5a16484461a4a7bc7d1126e166","value":" 665/665 [00:00&lt;00:00, 47.2kB/s]"}},"7ceda695f62f425ab65931dbcfcd2693":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5f341ba52a34f24a39731aa47477f61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3e572e9160d4046bba586cf31959cea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"829b7b441867436e91f8c9c3c7f78ecc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb87edf4edf7407094794ba3dac75156":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e45ad37d3c8e45d19936d9281a54334c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18301c5a16484461a4a7bc7d1126e166":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55812678118a4e6da7254425df7be8d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f0b4e1fe754b4fca84304409d7d0f1cd","IPY_MODEL_b7be25d0678848aab48340ba25932930","IPY_MODEL_ee7d355d2118490688a337ce0067bacd"],"layout":"IPY_MODEL_081386d3853545c4b8e969919bd30d0f"}},"f0b4e1fe754b4fca84304409d7d0f1cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9573cd1abfe943c8b8cc4fbfeadc78cf","placeholder":"​","style":"IPY_MODEL_339804b57b3a430fb33d4f3c558faea5","value":"model.safetensors: 100%"}},"b7be25d0678848aab48340ba25932930":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46f0d826fb0e433aa659dbe25c0b8052","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b45f8212e8d1436f9b88a3bc3a1cf96d","value":548105171}},"ee7d355d2118490688a337ce0067bacd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d94aea5d3144a8890d5a9acb6019173","placeholder":"​","style":"IPY_MODEL_cadaf3fda1d8448f90cbe2045c0deaf8","value":" 548M/548M [00:09&lt;00:00, 61.8MB/s]"}},"081386d3853545c4b8e969919bd30d0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9573cd1abfe943c8b8cc4fbfeadc78cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"339804b57b3a430fb33d4f3c558faea5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46f0d826fb0e433aa659dbe25c0b8052":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b45f8212e8d1436f9b88a3bc3a1cf96d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d94aea5d3144a8890d5a9acb6019173":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cadaf3fda1d8448f90cbe2045c0deaf8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c10333e7b2dd40d58885cd14734d527d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a95cd541c30643a4a20abc50b5d2a5a8","IPY_MODEL_76cb04ae13b74adbbf67429e17fc1b8e","IPY_MODEL_fab057fd4efd48dcab7131e8a150463a"],"layout":"IPY_MODEL_311dd3eacd2e46fdbccab64f616f22d6"}},"a95cd541c30643a4a20abc50b5d2a5a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eab1e53c7d094c779498e97fc1d8774a","placeholder":"​","style":"IPY_MODEL_7edbda320adb43b1bb4c9cfde4fd11e4","value":"generation_config.json: 100%"}},"76cb04ae13b74adbbf67429e17fc1b8e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc119b7f4e874c5483aad80f1722a45e","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_683d03ee40464b919f08cf570a1381ee","value":124}},"fab057fd4efd48dcab7131e8a150463a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d357a313fae4c1f89b0ef3040b7feae","placeholder":"​","style":"IPY_MODEL_2e9dd4015ddc4a17afb28111b9e92bf2","value":" 124/124 [00:00&lt;00:00, 7.47kB/s]"}},"311dd3eacd2e46fdbccab64f616f22d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eab1e53c7d094c779498e97fc1d8774a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7edbda320adb43b1bb4c9cfde4fd11e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc119b7f4e874c5483aad80f1722a45e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"683d03ee40464b919f08cf570a1381ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d357a313fae4c1f89b0ef3040b7feae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e9dd4015ddc4a17afb28111b9e92bf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"966097df3c1341048272a657d27fc267":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8735d5c26aeb40d5819d021a8cd937d2","IPY_MODEL_091730642ce2473b8d4f664e4f4b55e1","IPY_MODEL_80f27f59e5cb476b9a94e364847ea076"],"layout":"IPY_MODEL_6d1a59ade09344e4a43ef71c14f7583a"}},"8735d5c26aeb40d5819d021a8cd937d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1d64ad0007949b7b388ab5552c0441d","placeholder":"​","style":"IPY_MODEL_fd7378ab8d3544eca90d463c3d84cd73","value":"vocab.json: 100%"}},"091730642ce2473b8d4f664e4f4b55e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d669dea99966436587831e76f41693e3","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd6e54731e3d4a5faf6ea12a13f8c0fd","value":1042301}},"80f27f59e5cb476b9a94e364847ea076":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85c02c9727664e3c9ead300f37793bc9","placeholder":"​","style":"IPY_MODEL_31047b82a07742b9a787ac5ddf9390d7","value":" 1.04M/1.04M [00:00&lt;00:00, 1.33MB/s]"}},"6d1a59ade09344e4a43ef71c14f7583a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1d64ad0007949b7b388ab5552c0441d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd7378ab8d3544eca90d463c3d84cd73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d669dea99966436587831e76f41693e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd6e54731e3d4a5faf6ea12a13f8c0fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85c02c9727664e3c9ead300f37793bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31047b82a07742b9a787ac5ddf9390d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1720202150904fb6bf5e9531766d4eb5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9a9de0abb964eecb8472c3c1354e534","IPY_MODEL_0b39d1d58c154bd8ac2ab84162765f65","IPY_MODEL_b193434f08ff4d85a3dd98f825df27d1"],"layout":"IPY_MODEL_58588f268e684afe9723e1f49f0f72a0"}},"c9a9de0abb964eecb8472c3c1354e534":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc2cca38ee904bae96bd7350ff7f4b61","placeholder":"​","style":"IPY_MODEL_6de3523d76e54036bc443c2922afc778","value":"merges.txt: 100%"}},"0b39d1d58c154bd8ac2ab84162765f65":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8c736af6e9249859dcc5905805d864d","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a44deaabc14e481fa0beae7a428f8e06","value":456318}},"b193434f08ff4d85a3dd98f825df27d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bf8857ec5124d3a9488da2c5b738ea7","placeholder":"​","style":"IPY_MODEL_360b8480d5bd4c8aa959097a3fc93844","value":" 456k/456k [00:00&lt;00:00, 779kB/s]"}},"58588f268e684afe9723e1f49f0f72a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc2cca38ee904bae96bd7350ff7f4b61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6de3523d76e54036bc443c2922afc778":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8c736af6e9249859dcc5905805d864d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a44deaabc14e481fa0beae7a428f8e06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bf8857ec5124d3a9488da2c5b738ea7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"360b8480d5bd4c8aa959097a3fc93844":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a2db6c0b8dd4de79c0e2e2da0dde07a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3f5f3fde8b542b29737e886670630a0","IPY_MODEL_4f3c449a5fc0459cbe78c20c6ce4a52c","IPY_MODEL_e94d73adf99c4e4aac6dff14c181d949"],"layout":"IPY_MODEL_add7aa55b3aa4ae4aedab71baf67aa3f"}},"d3f5f3fde8b542b29737e886670630a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47bc99aece5e45539f014cd160cb2398","placeholder":"​","style":"IPY_MODEL_0124222f5b4b4b77a01510e9ec501f37","value":"tokenizer.json: 100%"}},"4f3c449a5fc0459cbe78c20c6ce4a52c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ede25a8f2b248879c70f9cdd4eb4b85","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a0a8e077e03474397b7bc751cfe68c9","value":1355256}},"e94d73adf99c4e4aac6dff14c181d949":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1ad36897fa14434ba4668178bbcc8e5","placeholder":"​","style":"IPY_MODEL_9c31235d4aed4d15bada155472253c09","value":" 1.36M/1.36M [00:00&lt;00:00, 1.39MB/s]"}},"add7aa55b3aa4ae4aedab71baf67aa3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47bc99aece5e45539f014cd160cb2398":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0124222f5b4b4b77a01510e9ec501f37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ede25a8f2b248879c70f9cdd4eb4b85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a0a8e077e03474397b7bc751cfe68c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1ad36897fa14434ba4668178bbcc8e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c31235d4aed4d15bada155472253c09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"b13177b7"},"source":["<a href=\"https://colab.research.google.com/github/wlg100/numseqcont_circuit_expms/blob/main/notebook_templates/headFNs_expms_template.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."]},{"cell_type":"markdown","metadata":{"id":"epqaV4bo4uuI"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3ANUnkaP41tk","executionInfo":{"status":"ok","timestamp":1702705976486,"user_tz":300,"elapsed":22700,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["%%capture\n","%pip install git+https://github.com/redwoodresearch/Easy-Transformer.git\n","%pip install einops datasets transformers fancy_einsum"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rmhOgC9o4uuI","executionInfo":{"status":"ok","timestamp":1702705983481,"user_tz":300,"elapsed":6998,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["from copy import deepcopy\n","import torch\n","\n","assert torch.cuda.device_count() == 1\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import torch as t\n","from easy_transformer.EasyTransformer import (\n","    EasyTransformer,\n",")\n","from time import ctime\n","from functools import partial\n","\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","\n","from easy_transformer.experiments import (\n","    ExperimentMetric,\n","    AblationConfig,\n","    EasyAblation,\n","    EasyPatching,\n","    PatchingConfig,\n",")\n","import plotly.express as px\n","import plotly.io as pio\n","import plotly.graph_objects as go\n","import random\n","import einops\n","from IPython import get_ipython\n","from copy import deepcopy\n","from easy_transformer.ioi_dataset import (\n","    IOIDataset,\n",")\n","from easy_transformer.ioi_utils import (\n","    path_patching,\n","    max_2d,\n","    CLASS_COLORS,\n","    show_pp,\n","    show_attention_patterns,\n","    scatter_attention_and_contribution,\n",")\n","from random import randint as ri\n","from easy_transformer.ioi_circuit_extraction import (\n","    do_circuit_extraction,\n","    get_heads_circuit,\n","    CIRCUIT,\n",")\n","from easy_transformer.ioi_utils import logit_diff, probs\n","from easy_transformer.ioi_utils import get_top_tokens_and_probs as g\n","\n","ipython = get_ipython()\n","if ipython is not None:\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"markdown","metadata":{"id":"q2AuFrzz4uuJ"},"source":[" Initialise model (use larger N or fewer templates for no warnings about in-template ablation)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["fdf81665426a482297cf28f4fa4bd8c7","f73b02f4fc7142e8b4e967c37d25a79a","74b346aa36f04165a3b4da436951bf48","14fc8552070b4781943be0d0c703b98c","7ceda695f62f425ab65931dbcfcd2693","c5f341ba52a34f24a39731aa47477f61","f3e572e9160d4046bba586cf31959cea","829b7b441867436e91f8c9c3c7f78ecc","eb87edf4edf7407094794ba3dac75156","e45ad37d3c8e45d19936d9281a54334c","18301c5a16484461a4a7bc7d1126e166","55812678118a4e6da7254425df7be8d7","f0b4e1fe754b4fca84304409d7d0f1cd","b7be25d0678848aab48340ba25932930","ee7d355d2118490688a337ce0067bacd","081386d3853545c4b8e969919bd30d0f","9573cd1abfe943c8b8cc4fbfeadc78cf","339804b57b3a430fb33d4f3c558faea5","46f0d826fb0e433aa659dbe25c0b8052","b45f8212e8d1436f9b88a3bc3a1cf96d","1d94aea5d3144a8890d5a9acb6019173","cadaf3fda1d8448f90cbe2045c0deaf8","c10333e7b2dd40d58885cd14734d527d","a95cd541c30643a4a20abc50b5d2a5a8","76cb04ae13b74adbbf67429e17fc1b8e","fab057fd4efd48dcab7131e8a150463a","311dd3eacd2e46fdbccab64f616f22d6","eab1e53c7d094c779498e97fc1d8774a","7edbda320adb43b1bb4c9cfde4fd11e4","dc119b7f4e874c5483aad80f1722a45e","683d03ee40464b919f08cf570a1381ee","0d357a313fae4c1f89b0ef3040b7feae","2e9dd4015ddc4a17afb28111b9e92bf2","966097df3c1341048272a657d27fc267","8735d5c26aeb40d5819d021a8cd937d2","091730642ce2473b8d4f664e4f4b55e1","80f27f59e5cb476b9a94e364847ea076","6d1a59ade09344e4a43ef71c14f7583a","c1d64ad0007949b7b388ab5552c0441d","fd7378ab8d3544eca90d463c3d84cd73","d669dea99966436587831e76f41693e3","cd6e54731e3d4a5faf6ea12a13f8c0fd","85c02c9727664e3c9ead300f37793bc9","31047b82a07742b9a787ac5ddf9390d7","1720202150904fb6bf5e9531766d4eb5","c9a9de0abb964eecb8472c3c1354e534","0b39d1d58c154bd8ac2ab84162765f65","b193434f08ff4d85a3dd98f825df27d1","58588f268e684afe9723e1f49f0f72a0","cc2cca38ee904bae96bd7350ff7f4b61","6de3523d76e54036bc443c2922afc778","e8c736af6e9249859dcc5905805d864d","a44deaabc14e481fa0beae7a428f8e06","2bf8857ec5124d3a9488da2c5b738ea7","360b8480d5bd4c8aa959097a3fc93844","6a2db6c0b8dd4de79c0e2e2da0dde07a","d3f5f3fde8b542b29737e886670630a0","4f3c449a5fc0459cbe78c20c6ce4a52c","e94d73adf99c4e4aac6dff14c181d949","add7aa55b3aa4ae4aedab71baf67aa3f","47bc99aece5e45539f014cd160cb2398","0124222f5b4b4b77a01510e9ec501f37","2ede25a8f2b248879c70f9cdd4eb4b85","6a0a8e077e03474397b7bc751cfe68c9","f1ad36897fa14434ba4668178bbcc8e5","9c31235d4aed4d15bada155472253c09"],"height":0},"executionInfo":{"elapsed":20435,"status":"ok","timestamp":1702706003912,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":300},"id":"J_cd_q8q4uuK","outputId":"5e1764c0-4636-4398-e47b-5a83422baec0"},"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdf81665426a482297cf28f4fa4bd8c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55812678118a4e6da7254425df7be8d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10333e7b2dd40d58885cd14734d527d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966097df3c1341048272a657d27fc267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1720202150904fb6bf5e9531766d4eb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a2db6c0b8dd4de79c0e2e2da0dde07a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/easy_transformer/components.py:616: UserWarning: Moved LN1 to the attention block\n","  warnings.warn(\"Moved LN1 to the attention block\")\n"]},{"output_type":"stream","name":"stdout","text":["Moving model to device:  cuda\n","Finished loading pretrained model gpt2 into EasyTransformer!\n"]}],"source":["model = EasyTransformer.from_pretrained(\"gpt2\").cuda()\n","# model = EasyTransformer.from_pretrained(\"gpt2\")\n","model.set_use_attn_result(True)"]},{"cell_type":"code","source":["import pdb"],"metadata":{"id":"Vfmr8jQt-5_6","executionInfo":{"status":"ok","timestamp":1702706003912,"user_tz":300,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Generate dataset with multiple prompts"],"metadata":{"id":"cGX9iHAz_UKX"}},{"cell_type":"code","source":["def generate_prompts_list():\n","    prompts_list = []\n","    for i in range(1, 98):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'text': f\"{i} {i+1} {i+2} {i+3}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list()"],"metadata":{"id":"tub8nC_9Mp6a","executionInfo":{"status":"ok","timestamp":1702706608039,"user_tz":300,"elapsed":295,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if key != 'text']:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"4wXBNWj5FwVn","executionInfo":{"status":"ok","timestamp":1702706608528,"user_tz":300,"elapsed":1,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"b8HJww_CPuzj","executionInfo":{"status":"ok","timestamp":1702706608529,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmnpvUF-4uuN"},"source":["# Next score"]},{"cell_type":"code","source":["def get_next_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_moved = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        # for word in words:\n","        word = words[-1]\n","        pred_tokens = [\n","            model.tokenizer.decode(token)\n","            for token in torch.topk(\n","                logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","            ).indices\n","        ]\n","\n","        # get next member after digit prompt[word]\n","        next_word = str(int(prompt[word]) + 1)\n","\n","        nextToken_in_topK = 'no'\n","        if \" \" + next_word in pred_tokens or next_word in pred_tokens:\n","            n_right += 1\n","            words_moved.append(prompt[word])\n","            nextToken_in_topK = 'yes'\n","        pred_tokens_dict[prompt[word]] = (pred_tokens, next_word, nextToken_in_topK)\n","\n","    # percent_right = (n_right / (dataset.N * len(words))) * 100\n","    percent_right = (n_right / (dataset.N )) * 100\n","    if percent_right > 0:\n","        print(f\"Next circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        return pred_tokens_dict\n","    else:\n","        # return words_moved\n","        return percent_right"],"metadata":{"id":"7Tp-ILOn9ae2","executionInfo":{"status":"ok","timestamp":1702706385223,"user_tz":300,"elapsed":303,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["get_next_scores(model, 9, 1, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LkRcYNi-yZs","executionInfo":{"status":"ok","timestamp":1694801042697,"user_tz":240,"elapsed":977,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8b028355-c01e-4c36-cbd8-546a6974c61b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Next circuit for head 9.1 (sign=1) : Top 5 accuracy: 87.37113402061856%\n"]},{"output_type":"execute_result","data":{"text/plain":["{'1': ([' one', ' needed', ' preferred', ' single', '2'], '2', 'yes'),\n"," '2': ([' third', ' fourth', 'third', '3', ' fifth'], '3', 'yes'),\n"," '3': ([' fourth', ' third', ' fifth', 'Fourth', 'fourth'], '4', 'no'),\n"," '4': ([' fifth', ' sixth', ' seventh', 'fifth', 'five'], '5', 'no'),\n"," '5': ([' sixth', ' seventh', ' fifth', '6', ' eighth'], '6', 'yes'),\n"," '6': ([' seventh', ' Seventh', ' sixth', '7', ' eighth'], '7', 'yes'),\n"," '7': ([' seventh', ' eighth', ' ninth', ' VIII', ' Seventh'], '8', 'no'),\n"," '8': ([' ninth', ' eighth', 'ighth', ' seventh', '9'], '9', 'yes'),\n"," '9': ([' ninth', ' seventh', ' tenth', ' eighth', ' sixth'], '10', 'no'),\n"," '10': ([' tenth', ' seventh', ' eighth', ' ninth', ' sixth'], '11', 'no'),\n"," '11': ([' eighth', ' seventh', ' specific', ' 12', '12'], '12', 'yes'),\n"," '12': ([' seventh', ' eighth', '13', ' specific', '14'], '13', 'yes'),\n"," '13': ([' seventh', '14', ' 14', ' eighth', 'Only'], '14', 'yes'),\n"," '14': ([' eighth', ' seventh', ' fifth', ' maximum', ' ninth'], '15', 'no'),\n"," '15': ([' seventh', ' eighth', ' sixth', ' fifth', ' final'], '16', 'no'),\n"," '16': ([' final', ' seventh', 'ighth', 'each', ' ninth'], '17', 'no'),\n"," '17': ([' seventh', ' fifth', ' eighth', ' sixth', 'ighth'], '18', 'no'),\n"," '18': (['ighth', ' 19', ' twentieth', 'Only', ' nineteen'], '19', 'yes'),\n"," '19': ([' seventh', ' sixth', ' eighth', ' third', ' fifth'], '20', 'no'),\n"," '20': (['maximum', ' third', ' maximum', ' final', ' seventh'], '21', 'no'),\n"," '21': ([' seventh', ' eighth', 'each', ' maximum', ' third'], '22', 'no'),\n"," '22': ([' third', ' seventh', 'single', ' sufficient', ' eighth'],\n","  '23',\n","  'no'),\n"," '23': ([' third', ' fourth', ' fifth', ' seventh', 'Fourth'], '24', 'no'),\n"," '24': ([' fifth', ' seventh', ' third', ' fourth', ' sixth'], '25', 'no'),\n"," '25': ([' seventh', ' third', ' sixth', ' fifth', ' fourth'], '26', 'no'),\n"," '26': ([' seventh', ' third', 'third', ' sixth', ' fifth'], '27', 'no'),\n"," '27': ([' seventh', ' third', ' eighth', 'third', ' fourth'], '28', 'no'),\n"," '28': ([' 29', ' third', 'ighth', ' 30', '29'], '29', 'yes'),\n"," '29': (['310', ' third', ' seventh', ' 30', 'never'], '30', 'yes'),\n"," '30': ([' third', '310', ' maximum', '31', ' 31'], '31', 'yes'),\n"," '31': ([' third', ' seventh', ' 32', ' single', 'third'], '32', 'yes'),\n"," '32': ([' third', ' fourth', ' seventh', 'third', 'Fourth'], '33', 'no'),\n"," '33': ([' third', ' fourth', ' seventh', ' fifth', 'third'], '34', 'no'),\n"," '34': ([' fifth', ' seventh', ' third', ' sixth', 'fifth'], '35', 'no'),\n"," '35': ([' seventh', ' sixth', ' fifth', ' third', ' fourth'], '36', 'no'),\n"," '36': ([' seventh', 'ighth', 'each', ' 57', ' 58'], '37', 'no'),\n"," '37': ([' seventh', ' 78', ' eighth', ' 58', ' 59'], '38', 'no'),\n"," '38': ([' 79', ' 60', ' 80', ' ninety', ' sixty'], '39', 'no'),\n"," '39': ([' 80', ' seventh', ' 60', ' 61', ' 81'], '40', 'no'),\n"," '40': ([' 81', ' eighty', ' 80', ' ninety', ' 90'], '41', 'no'),\n"," '41': ([' 81', 'each', ' 83', ' ninety', ' 82'], '42', 'no'),\n"," '42': ([' seventh', ' fifth', ' final', ' ninety', ' 83'], '43', 'no'),\n"," '43': ([' fifth', ' seventh', ' fourth', ' sixth', 'five'], '44', 'no'),\n"," '44': ([' fifth', ' seventh', 'single', 'ubis', ' 95'], '45', 'no'),\n"," '45': ([' seventh', ' fifth', ' 95', ' 60', ' sixth'], '46', 'no'),\n"," '46': ([' particular', ' seventh', ' 52', 'ighth', ' specific'], '47', 'no'),\n"," '47': ([' 80', ' particular', ' 81', ' 90', ' 78'], '48', 'no'),\n"," '48': (['ighth', ' particular', ' fifth', 'single', ' final'], '49', 'no'),\n"," '49': ([' seventh', ' fifth', ' sixth', ' eighth', ' ninth'], '50', 'no'),\n"," '50': ([' ninety', ' eighty', ' sixty', ' 81', ' 75'], '51', 'no'),\n"," '51': ([' seventh', ' specific', ' particular', ' 55', ' 52'], '52', 'yes'),\n"," '52': ([' seventh', ' sixth', ' fifth', ' 55', ' whichever'], '53', 'no'),\n"," '53': ([' seventh', ' sixth', ' fifth', ' fourth', ' third'], '54', 'no'),\n"," '54': ([' seventh', ' sixth', ' fifth', ' 55', '59'], '55', 'yes'),\n"," '55': ([' seventh', ' sixth', ' fifth', ' eighth', ' third'], '56', 'no'),\n"," '56': ([' seventh', ' 57', ' 58', ' sixth', ' fifth'], '57', 'yes'),\n"," '57': ([' seventh', ' sixth', ' 58', '59', ' fifth'], '58', 'yes'),\n"," '58': ([' 59', ' sixty', '59', ' 60', ' 61'], '59', 'yes'),\n"," '59': ([' 61', ' 60', '59', ' seventh', ' 59'], '60', 'yes'),\n"," '60': ([' 81', ' 61', ' 80', '81', ' 76'], '61', 'yes'),\n"," '61': ([' 62', ' 61', '62', ' 81', ' seventh'], '62', 'yes'),\n"," '62': ([' 78', ' seventh', ' 79', ' 77', ' 63'], '63', 'yes'),\n"," '63': ([' seventh', ' 79', ' 78', ' 64', ' 65'], '64', 'yes'),\n"," '64': ([' seventh', ' sixth', ' must', ' fifth', ' 65'], '65', 'yes'),\n"," '65': ([' seventh', ' sixth', ' fifth', ' 75', ' 76'], '66', 'no'),\n"," '66': ([' seventh', ' 77', ' 78', ' 67', ' 68'], '67', 'yes'),\n"," '67': ([' seventh', ' 78', ' 79', ' 77', ' eighth'], '68', 'no'),\n"," '68': ([' 79', ' 78', ' 70', ' 80', ' seventh'], '69', 'no'),\n"," '69': ([' 80', ' 79', ' 81', ' 78', ' 76'], '70', 'no'),\n"," '70': ([' 81', ' 80', ' 76', ' 75', ' 78'], '71', 'no'),\n"," '71': ([' 81', ' 78', ' 76', ' 82', ' 77'], '72', 'no'),\n"," '72': ([' 76', ' 81', ' 77', ' 78', ' 79'], '73', 'no'),\n"," '73': ([' seventh', ' occasional', ' 78', ' 75', ' 76'], '74', 'no'),\n"," '74': ([' seventh', ' eighth', ' 76', 'ighth', ' 78'], '75', 'no'),\n"," '75': ([' seventh', ' sixth', ' eighth', ' 76', ' 78'], '76', 'yes'),\n"," '76': ([' 78', ' 77', ' seventh', ' 79', ' 81'], '77', 'yes'),\n"," '77': ([' 78', ' 79', ' 80', ' 81', '78'], '78', 'yes'),\n"," '78': ([' 79', ' 80', ' 81', ' ninety', 'ighth'], '79', 'yes'),\n"," '79': ([' 81', ' 80', ' 79', ' 90', ' eighty'], '80', 'yes'),\n"," '80': ([' 81', ' 80', ' ninety', ' 90', ' eighty'], '81', 'yes'),\n"," '81': ([' 81', ' 82', ' 83', ' occasional', ' 84'], '82', 'yes'),\n"," '82': ([' 83', ' 84', ' 85', ' ninety', ' 81'], '83', 'yes'),\n"," '83': ([' 84', ' 85', ' 83', ' occasional', ' ninety'], '84', 'yes'),\n"," '84': ([' 85', ' occasional', ' 86', '85', ' ninety'], '85', 'yes'),\n"," '85': ([' ninety', ' 86', ' occasional', ' 85', ' 90'], '86', 'yes'),\n"," '86': ([' 87', ' 88', ' 86', ' 91', ' 90'], '87', 'yes'),\n"," '87': ([' 88', ' 90', ' ninety', ' occasional', ' 91'], '88', 'yes'),\n"," '88': ([' 90', ' ninety', ' 91', ' 89', ' 98'], '89', 'yes'),\n"," '89': ([' ninety', ' 91', ' 95', ' 90', ' 94'], '90', 'yes'),\n"," '90': ([' 91', ' ninety', ' 90', ' 95', '91'], '91', 'yes'),\n"," '91': ([' 92', ' 93', ' 95', ' 94', ' 91'], '92', 'yes'),\n"," '92': ([' 95', ' 94', ' 93', ' 96', '95'], '93', 'yes'),\n"," '93': ([' 95', ' 94', ' occasional', '95', ' 96'], '94', 'yes'),\n"," '94': ([' 95', ' 96', ' occasional', '95', ' particular'], '95', 'yes'),\n"," '95': ([' 96', ' seventh', ' occasional', ' 95', ' sixth'], '96', 'yes'),\n"," '96': ([' seventh', ' 97', ' 98', ' 96', ' occasional'], '97', 'yes'),\n"," '97': ([' 98', ' 99', ' seventh', ' 97', ' occasional'], '98', 'yes'),\n"," '98': ([' 99', ' 100', ' 102', ' 103', ' 105'], '99', 'yes'),\n"," '99': ([' 99', ' 100', ' 102', ' 101', ' 105'], '100', 'yes'),\n"," '100': ([' 100', ' 101', '100', '101', ' 200'], '101', 'yes')}"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["# Compare next scores to copy scores"],"metadata":{"id":"bS_7sy-8BMBu"}},{"cell_type":"code","source":["def get_copy_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_moved = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        # for word in words:\n","        word = words[-1]\n","        pred_tokens = [\n","            model.tokenizer.decode(token)\n","            for token in torch.topk(\n","                logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","            ).indices\n","        ]\n","\n","        token_in_topK = 'no'\n","        if \" \" + prompt[word] in pred_tokens or prompt[word] in pred_tokens:\n","            n_right += 1\n","            words_moved.append(prompt[word])\n","            token_in_topK = 'yes'\n","        pred_tokens_dict[prompt[word]] = (pred_tokens, token_in_topK)\n","\n","    # percent_right = (n_right / (dataset.N * len(words))) * 100\n","    percent_right = (n_right / (dataset.N )) * 100\n","    print(f\"Copy circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        return pred_tokens_dict\n","    else:\n","        return words_moved"],"metadata":{"id":"v_ZNkjTzBP3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_copy_scores(model, 9, 1, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rTl9Yd0Bcro","executionInfo":{"status":"ok","timestamp":1694801212305,"user_tz":240,"elapsed":354,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e6f41f2d-12a2-40f6-96ee-09b87873e73f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Copy circuit for head 9.1 (sign=1) : Top 5 accuracy: 59.27835051546392%\n"]},{"output_type":"execute_result","data":{"text/plain":["{'1': ([' one', ' needed', ' preferred', ' single', '2'], 'no'),\n"," '2': ([' third', ' fourth', 'third', '3', ' fifth'], 'no'),\n"," '3': ([' fourth', ' third', ' fifth', 'Fourth', 'fourth'], 'no'),\n"," '4': ([' fifth', ' sixth', ' seventh', 'fifth', 'five'], 'no'),\n"," '5': ([' sixth', ' seventh', ' fifth', '6', ' eighth'], 'no'),\n"," '6': ([' seventh', ' Seventh', ' sixth', '7', ' eighth'], 'no'),\n"," '7': ([' seventh', ' eighth', ' ninth', ' VIII', ' Seventh'], 'no'),\n"," '8': ([' ninth', ' eighth', 'ighth', ' seventh', '9'], 'no'),\n"," '9': ([' ninth', ' seventh', ' tenth', ' eighth', ' sixth'], 'no'),\n"," '10': ([' tenth', ' seventh', ' eighth', ' ninth', ' sixth'], 'no'),\n"," '11': ([' eighth', ' seventh', ' specific', ' 12', '12'], 'no'),\n"," '12': ([' seventh', ' eighth', '13', ' specific', '14'], 'no'),\n"," '13': ([' seventh', '14', ' 14', ' eighth', 'Only'], 'no'),\n"," '14': ([' eighth', ' seventh', ' fifth', ' maximum', ' ninth'], 'no'),\n"," '15': ([' seventh', ' eighth', ' sixth', ' fifth', ' final'], 'no'),\n"," '16': ([' final', ' seventh', 'ighth', 'each', ' ninth'], 'no'),\n"," '17': ([' seventh', ' fifth', ' eighth', ' sixth', 'ighth'], 'no'),\n"," '18': (['ighth', ' 19', ' twentieth', 'Only', ' nineteen'], 'no'),\n"," '19': ([' seventh', ' sixth', ' eighth', ' third', ' fifth'], 'no'),\n"," '20': (['maximum', ' third', ' maximum', ' final', ' seventh'], 'no'),\n"," '21': ([' seventh', ' eighth', 'each', ' maximum', ' third'], 'no'),\n"," '22': ([' third', ' seventh', 'single', ' sufficient', ' eighth'], 'no'),\n"," '23': ([' third', ' fourth', ' fifth', ' seventh', 'Fourth'], 'no'),\n"," '24': ([' fifth', ' seventh', ' third', ' fourth', ' sixth'], 'no'),\n"," '25': ([' seventh', ' third', ' sixth', ' fifth', ' fourth'], 'no'),\n"," '26': ([' seventh', ' third', 'third', ' sixth', ' fifth'], 'no'),\n"," '27': ([' seventh', ' third', ' eighth', 'third', ' fourth'], 'no'),\n"," '28': ([' 29', ' third', 'ighth', ' 30', '29'], 'no'),\n"," '29': (['310', ' third', ' seventh', ' 30', 'never'], 'no'),\n"," '30': ([' third', '310', ' maximum', '31', ' 31'], 'no'),\n"," '31': ([' third', ' seventh', ' 32', ' single', 'third'], 'no'),\n"," '32': ([' third', ' fourth', ' seventh', 'third', 'Fourth'], 'no'),\n"," '33': ([' third', ' fourth', ' seventh', ' fifth', 'third'], 'no'),\n"," '34': ([' fifth', ' seventh', ' third', ' sixth', 'fifth'], 'no'),\n"," '35': ([' seventh', ' sixth', ' fifth', ' third', ' fourth'], 'no'),\n"," '36': ([' seventh', 'ighth', 'each', ' 57', ' 58'], 'no'),\n"," '37': ([' seventh', ' 78', ' eighth', ' 58', ' 59'], 'no'),\n"," '38': ([' 79', ' 60', ' 80', ' ninety', ' sixty'], 'no'),\n"," '39': ([' 80', ' seventh', ' 60', ' 61', ' 81'], 'no'),\n"," '40': ([' 81', ' eighty', ' 80', ' ninety', ' 90'], 'no'),\n"," '41': ([' 81', 'each', ' 83', ' ninety', ' 82'], 'no'),\n"," '42': ([' seventh', ' fifth', ' final', ' ninety', ' 83'], 'no'),\n"," '43': ([' fifth', ' seventh', ' fourth', ' sixth', 'five'], 'no'),\n"," '44': ([' fifth', ' seventh', 'single', 'ubis', ' 95'], 'no'),\n"," '45': ([' seventh', ' fifth', ' 95', ' 60', ' sixth'], 'no'),\n"," '46': ([' particular', ' seventh', ' 52', 'ighth', ' specific'], 'no'),\n"," '47': ([' 80', ' particular', ' 81', ' 90', ' 78'], 'no'),\n"," '48': (['ighth', ' particular', ' fifth', 'single', ' final'], 'no'),\n"," '49': ([' seventh', ' fifth', ' sixth', ' eighth', ' ninth'], 'no'),\n"," '50': ([' ninety', ' eighty', ' sixty', ' 81', ' 75'], 'no'),\n"," '51': ([' seventh', ' specific', ' particular', ' 55', ' 52'], 'no'),\n"," '52': ([' seventh', ' sixth', ' fifth', ' 55', ' whichever'], 'no'),\n"," '53': ([' seventh', ' sixth', ' fifth', ' fourth', ' third'], 'no'),\n"," '54': ([' seventh', ' sixth', ' fifth', ' 55', '59'], 'no'),\n"," '55': ([' seventh', ' sixth', ' fifth', ' eighth', ' third'], 'no'),\n"," '56': ([' seventh', ' 57', ' 58', ' sixth', ' fifth'], 'no'),\n"," '57': ([' seventh', ' sixth', ' 58', '59', ' fifth'], 'no'),\n"," '58': ([' 59', ' sixty', '59', ' 60', ' 61'], 'no'),\n"," '59': ([' 61', ' 60', '59', ' seventh', ' 59'], 'yes'),\n"," '60': ([' 81', ' 61', ' 80', '81', ' 76'], 'no'),\n"," '61': ([' 62', ' 61', '62', ' 81', ' seventh'], 'yes'),\n"," '62': ([' 78', ' seventh', ' 79', ' 77', ' 63'], 'no'),\n"," '63': ([' seventh', ' 79', ' 78', ' 64', ' 65'], 'no'),\n"," '64': ([' seventh', ' sixth', ' must', ' fifth', ' 65'], 'no'),\n"," '65': ([' seventh', ' sixth', ' fifth', ' 75', ' 76'], 'no'),\n"," '66': ([' seventh', ' 77', ' 78', ' 67', ' 68'], 'no'),\n"," '67': ([' seventh', ' 78', ' 79', ' 77', ' eighth'], 'no'),\n"," '68': ([' 79', ' 78', ' 70', ' 80', ' seventh'], 'no'),\n"," '69': ([' 80', ' 79', ' 81', ' 78', ' 76'], 'no'),\n"," '70': ([' 81', ' 80', ' 76', ' 75', ' 78'], 'no'),\n"," '71': ([' 81', ' 78', ' 76', ' 82', ' 77'], 'no'),\n"," '72': ([' 76', ' 81', ' 77', ' 78', ' 79'], 'no'),\n"," '73': ([' seventh', ' occasional', ' 78', ' 75', ' 76'], 'no'),\n"," '74': ([' seventh', ' eighth', ' 76', 'ighth', ' 78'], 'no'),\n"," '75': ([' seventh', ' sixth', ' eighth', ' 76', ' 78'], 'no'),\n"," '76': ([' 78', ' 77', ' seventh', ' 79', ' 81'], 'no'),\n"," '77': ([' 78', ' 79', ' 80', ' 81', '78'], 'no'),\n"," '78': ([' 79', ' 80', ' 81', ' ninety', 'ighth'], 'no'),\n"," '79': ([' 81', ' 80', ' 79', ' 90', ' eighty'], 'yes'),\n"," '80': ([' 81', ' 80', ' ninety', ' 90', ' eighty'], 'yes'),\n"," '81': ([' 81', ' 82', ' 83', ' occasional', ' 84'], 'yes'),\n"," '82': ([' 83', ' 84', ' 85', ' ninety', ' 81'], 'no'),\n"," '83': ([' 84', ' 85', ' 83', ' occasional', ' ninety'], 'yes'),\n"," '84': ([' 85', ' occasional', ' 86', '85', ' ninety'], 'no'),\n"," '85': ([' ninety', ' 86', ' occasional', ' 85', ' 90'], 'yes'),\n"," '86': ([' 87', ' 88', ' 86', ' 91', ' 90'], 'yes'),\n"," '87': ([' 88', ' 90', ' ninety', ' occasional', ' 91'], 'no'),\n"," '88': ([' 90', ' ninety', ' 91', ' 89', ' 98'], 'no'),\n"," '89': ([' ninety', ' 91', ' 95', ' 90', ' 94'], 'no'),\n"," '90': ([' 91', ' ninety', ' 90', ' 95', '91'], 'yes'),\n"," '91': ([' 92', ' 93', ' 95', ' 94', ' 91'], 'yes'),\n"," '92': ([' 95', ' 94', ' 93', ' 96', '95'], 'no'),\n"," '93': ([' 95', ' 94', ' occasional', '95', ' 96'], 'no'),\n"," '94': ([' 95', ' 96', ' occasional', '95', ' particular'], 'no'),\n"," '95': ([' 96', ' seventh', ' occasional', ' 95', ' sixth'], 'yes'),\n"," '96': ([' seventh', ' 97', ' 98', ' 96', ' occasional'], 'yes'),\n"," '97': ([' 98', ' 99', ' seventh', ' 97', ' occasional'], 'yes'),\n"," '98': ([' 99', ' 100', ' 102', ' 103', ' 105'], 'no'),\n"," '99': ([' 99', ' 100', ' 102', ' 101', ' 105'], 'yes'),\n"," '100': ([' 100', ' 101', '100', '101', ' 200'], 'yes')}"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["# Find more next heads"],"metadata":{"id":"7Ir5Ct8U-VUE"}},{"cell_type":"markdown","source":["## Get important heads"],"metadata":{"id":"h3Rp4RAjyZWe"}},{"cell_type":"markdown","source":["Find what heads are specific to certain inputs, and what's common to the template.\n","\n","Get important heads from: circuit_expms_template.ipynb (Section: print top heads. Copy output of 'top_indices'; put on one line using chatgpt)\n","\n","NOTE: not all attention heads just copy, so use attention patterns to determine which ones copy to refine this list of heads\n","\n","(Eg. if you copy all the top heads from IOI, only 9.9 and 10.0 are name movers while other heads are \"S-inhibition\", \"induction\", \"duplicate\", so only the name movers + backup NM will have top accuracy)"],"metadata":{"id":"aoEXQeXhymq1"}},{"cell_type":"markdown","source":["## last token only"],"metadata":{"id":"N5mHRUvYteDv"}},{"cell_type":"code","source":["top_val = [(1,5), (4,4), (6,1), (7, 10), (7, 11), (8, 11), (9, 1)]\n","for index, (layer, head) in enumerate(top_val):\n","    print(index, get_copy_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wXDkx5z1rFGb","executionInfo":{"status":"ok","timestamp":1699107650824,"user_tz":240,"elapsed":1037,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"52a1d570-ed41-467a-d604-419dde4d3f61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Copy circuit for head 1.5 (sign=1) : Top 5 accuracy: 0.0%\n","0 []\n","Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 0.0%\n","1 []\n","Copy circuit for head 6.1 (sign=1) : Top 5 accuracy: 1.0309278350515463%\n","2 ['18']\n","Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n","3 ['4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n","Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 23.711340206185564%\n","4 ['4', '5', '6', '10', '11', '13', '14', '15', '20', '29', '39', '40', '41', '45', '49', '54', '59', '60', '61', '65', '69', '79', '100']\n","Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 98.96907216494846%\n","5 ['4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n","Copy circuit for head 9.1 (sign=1) : Top 5 accuracy: 75.25773195876289%\n","6 ['4', '5', '6', '7', '8', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '24', '25', '27', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '43', '44', '45', '46', '47', '50', '51', '52', '53', '54', '57', '58', '59', '61', '62', '63', '67', '69', '71', '72', '74', '75', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '90', '92', '93', '94', '95', '96', '97', '99', '100']\n"]}]},{"cell_type":"code","source":["top_val = [(1,5), (4,4), (6,1), (7, 10), (7, 11), (8, 11), (9, 1)]\n","for index, (layer, head) in enumerate(top_val):\n","    print(index, get_next_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CWaeNGWMrBVc","executionInfo":{"status":"ok","timestamp":1699107652455,"user_tz":240,"elapsed":868,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4654402f-0909-461f-bff6-9dfe11b3945d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Next circuit for head 1.5 (sign=1) : Top 5 accuracy: 0.0%\n","0 []\n","Next circuit for head 4.4 (sign=1) : Top 5 accuracy: 0.0%\n","1 []\n","Next circuit for head 6.1 (sign=1) : Top 5 accuracy: 18.556701030927837%\n","2 ['4', '5', '6', '7', '8', '9', '11', '12', '13', '15', '17', '18', '20', '21', '45', '46', '65', '90']\n","Next circuit for head 7.10 (sign=1) : Top 5 accuracy: 63.91752577319587%\n","3 ['4', '11', '16', '17', '21', '22', '26', '27', '28', '31', '32', '33', '35', '37', '38', '41', '42', '43', '44', '45', '46', '47', '48', '49', '51', '52', '53', '55', '56', '57', '58', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '73', '76', '77', '78', '80', '81', '82', '83', '84', '85', '86', '87', '88', '90', '91', '92', '93', '96', '97']\n","Next circuit for head 7.11 (sign=1) : Top 5 accuracy: 8.24742268041237%\n","4 ['4', '5', '10', '22', '48', '52', '53', '58']\n","Next circuit for head 8.11 (sign=1) : Top 5 accuracy: 32.98969072164948%\n","5 ['27', '33', '35', '45', '47', '48', '55', '57', '58', '61', '63', '66', '67', '68', '70', '71', '73', '75', '76', '77', '78', '80', '81', '82', '83', '85', '86', '87', '88', '91', '93', '97']\n","Next circuit for head 9.1 (sign=1) : Top 5 accuracy: 100.0%\n","6 ['4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n"]}]},{"cell_type":"markdown","source":["### loop over all heads"],"metadata":{"id":"LcnpGWW8M015"}},{"cell_type":"markdown","source":["only print out if there's a match"],"metadata":{"id":"dbBYZp_iM016"}},{"cell_type":"code","source":["all_next_scores = []\n","all_heads = [(layer, head) for layer in range(12) for head in range(12)]\n","for index, (layer, head) in enumerate(all_heads):\n","    all_next_scores.append(get_next_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706398925,"user_tz":300,"elapsed":11448,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0a48b5c3-e853-4763-9196-af67f3e6e4ca","id":"PwkZQlRFM016"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Next circuit for head 3.5 (sign=1) : Top 5 accuracy: 4.123711340206185%\n","Next circuit for head 5.0 (sign=1) : Top 5 accuracy: 8.24742268041237%\n","Next circuit for head 5.1 (sign=1) : Top 5 accuracy: 3.0927835051546393%\n","Next circuit for head 6.1 (sign=1) : Top 5 accuracy: 18.556701030927837%\n","Next circuit for head 6.9 (sign=1) : Top 5 accuracy: 76.28865979381443%\n","Next circuit for head 7.2 (sign=1) : Top 5 accuracy: 43.29896907216495%\n","Next circuit for head 7.7 (sign=1) : Top 5 accuracy: 3.0927835051546393%\n","Next circuit for head 7.8 (sign=1) : Top 5 accuracy: 2.0618556701030926%\n","Next circuit for head 7.10 (sign=1) : Top 5 accuracy: 63.91752577319587%\n","Next circuit for head 7.11 (sign=1) : Top 5 accuracy: 8.24742268041237%\n","Next circuit for head 8.0 (sign=1) : Top 5 accuracy: 1.0309278350515463%\n","Next circuit for head 8.1 (sign=1) : Top 5 accuracy: 43.29896907216495%\n","Next circuit for head 8.8 (sign=1) : Top 5 accuracy: 75.25773195876289%\n","Next circuit for head 8.11 (sign=1) : Top 5 accuracy: 32.98969072164948%\n","Next circuit for head 9.1 (sign=1) : Top 5 accuracy: 100.0%\n","Next circuit for head 9.3 (sign=1) : Top 5 accuracy: 2.0618556701030926%\n","Next circuit for head 9.9 (sign=1) : Top 5 accuracy: 1.0309278350515463%\n","Next circuit for head 9.11 (sign=1) : Top 5 accuracy: 4.123711340206185%\n","Next circuit for head 10.2 (sign=1) : Top 5 accuracy: 90.72164948453609%\n","Next circuit for head 11.2 (sign=1) : Top 5 accuracy: 1.0309278350515463%\n","Next circuit for head 11.4 (sign=1) : Top 5 accuracy: 6.185567010309279%\n"]}]},{"cell_type":"code","source":["sum(all_next_scores)/len(all_next_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706410166,"user_tz":300,"elapsed":312,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e45b624a-f85a-4ae2-b303-74a375480c85","id":"gTtk45ucM016"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.08791523482245"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["all_next_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKyLfdB7M0V4","executionInfo":{"status":"ok","timestamp":1702706352273,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"caed26da-dd00-47fc-8fb5-e733fe9cdbd9"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['7', '11', '15', '17'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['4', '5', '7', '13', '17', '27', '41', '51'],\n"," ['51', '61', '67'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['4',\n","  '5',\n","  '6',\n","  '7',\n","  '8',\n","  '9',\n","  '11',\n","  '12',\n","  '13',\n","  '15',\n","  '17',\n","  '18',\n","  '20',\n","  '21',\n","  '45',\n","  '46',\n","  '65',\n","  '90'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['4',\n","  '5',\n","  '6',\n","  '7',\n","  '8',\n","  '9',\n","  '10',\n","  '11',\n","  '12',\n","  '13',\n","  '15',\n","  '16',\n","  '18',\n","  '19',\n","  '20',\n","  '21',\n","  '22',\n","  '23',\n","  '24',\n","  '25',\n","  '26',\n","  '27',\n","  '28',\n","  '29',\n","  '32',\n","  '33',\n","  '36',\n","  '38',\n","  '41',\n","  '42',\n","  '43',\n","  '44',\n","  '45',\n","  '46',\n","  '47',\n","  '48',\n","  '49',\n","  '51',\n","  '52',\n","  '53',\n","  '55',\n","  '56',\n","  '57',\n","  '58',\n","  '59',\n","  '60',\n","  '61',\n","  '62',\n","  '63',\n","  '66',\n","  '67',\n","  '68',\n","  '69',\n","  '71',\n","  '72',\n","  '73',\n","  '76',\n","  '77',\n","  '78',\n","  '79',\n","  '82',\n","  '83',\n","  '85',\n","  '86',\n","  '87',\n","  '88',\n","  '89',\n","  '91',\n","  '92',\n","  '93',\n","  '95',\n","  '96',\n","  '97',\n","  '100'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['6',\n","  '11',\n","  '12',\n","  '13',\n","  '15',\n","  '18',\n","  '19',\n","  '21',\n","  '22',\n","  '26',\n","  '27',\n","  '28',\n","  '29',\n","  '38',\n","  '39',\n","  '40',\n","  '42',\n","  '43',\n","  '46',\n","  '48',\n","  '49',\n","  '52',\n","  '56',\n","  '57',\n","  '58',\n","  '62',\n","  '66',\n","  '68',\n","  '69',\n","  '72',\n","  '76',\n","  '78',\n","  '79',\n","  '80',\n","  '81',\n","  '83',\n","  '87',\n","  '88',\n","  '92',\n","  '93',\n","  '98',\n","  '99'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['9', '13', '20'],\n"," ['43', '93'],\n"," [],\n"," ['4',\n","  '11',\n","  '16',\n","  '17',\n","  '21',\n","  '22',\n","  '26',\n","  '27',\n","  '28',\n","  '31',\n","  '32',\n","  '33',\n","  '35',\n","  '37',\n","  '38',\n","  '41',\n","  '42',\n","  '43',\n","  '44',\n","  '45',\n","  '46',\n","  '47',\n","  '48',\n","  '49',\n","  '51',\n","  '52',\n","  '53',\n","  '55',\n","  '56',\n","  '57',\n","  '58',\n","  '60',\n","  '61',\n","  '62',\n","  '63',\n","  '64',\n","  '65',\n","  '66',\n","  '67',\n","  '68',\n","  '69',\n","  '70',\n","  '71',\n","  '73',\n","  '76',\n","  '77',\n","  '78',\n","  '80',\n","  '81',\n","  '82',\n","  '83',\n","  '84',\n","  '85',\n","  '86',\n","  '87',\n","  '88',\n","  '90',\n","  '91',\n","  '92',\n","  '93',\n","  '96',\n","  '97'],\n"," ['4', '5', '10', '22', '48', '52', '53', '58'],\n"," ['24'],\n"," ['4',\n","  '5',\n","  '6',\n","  '7',\n","  '8',\n","  '11',\n","  '13',\n","  '15',\n","  '16',\n","  '18',\n","  '19',\n","  '21',\n","  '27',\n","  '28',\n","  '29',\n","  '33',\n","  '39',\n","  '41',\n","  '43',\n","  '45',\n","  '48',\n","  '50',\n","  '51',\n","  '53',\n","  '57',\n","  '58',\n","  '61',\n","  '62',\n","  '66',\n","  '67',\n","  '68',\n","  '71',\n","  '77',\n","  '78',\n","  '79',\n","  '81',\n","  '83',\n","  '87',\n","  '88',\n","  '90',\n","  '97',\n","  '98'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['4',\n","  '5',\n","  '6',\n","  '7',\n","  '8',\n","  '9',\n","  '10',\n","  '11',\n","  '12',\n","  '13',\n","  '14',\n","  '15',\n","  '16',\n","  '17',\n","  '18',\n","  '19',\n","  '20',\n","  '21',\n","  '22',\n","  '23',\n","  '26',\n","  '27',\n","  '28',\n","  '29',\n","  '31',\n","  '32',\n","  '33',\n","  '34',\n","  '36',\n","  '37',\n","  '38',\n","  '39',\n","  '41',\n","  '42',\n","  '43',\n","  '44',\n","  '45',\n","  '51',\n","  '52',\n","  '53',\n","  '54',\n","  '58',\n","  '59',\n","  '60',\n","  '61',\n","  '62',\n","  '63',\n","  '64',\n","  '65',\n","  '66',\n","  '67',\n","  '68',\n","  '71',\n","  '72',\n","  '73',\n","  '74',\n","  '76',\n","  '77',\n","  '78',\n","  '79',\n","  '81',\n","  '82',\n","  '83',\n","  '84',\n","  '86',\n","  '87',\n","  '89',\n","  '92',\n","  '93',\n","  '94',\n","  '96',\n","  '97',\n","  '98'],\n"," [],\n"," [],\n"," ['27',\n","  '33',\n","  '35',\n","  '45',\n","  '47',\n","  '48',\n","  '55',\n","  '57',\n","  '58',\n","  '61',\n","  '63',\n","  '66',\n","  '67',\n","  '68',\n","  '70',\n","  '71',\n","  '73',\n","  '75',\n","  '76',\n","  '77',\n","  '78',\n","  '80',\n","  '81',\n","  '82',\n","  '83',\n","  '85',\n","  '86',\n","  '87',\n","  '88',\n","  '91',\n","  '93',\n","  '97'],\n"," [],\n"," ['4',\n","  '5',\n","  '6',\n","  '7',\n","  '8',\n","  '9',\n","  '10',\n","  '11',\n","  '12',\n","  '13',\n","  '14',\n","  '15',\n","  '16',\n","  '17',\n","  '18',\n","  '19',\n","  '20',\n","  '21',\n","  '22',\n","  '23',\n","  '24',\n","  '25',\n","  '26',\n","  '27',\n","  '28',\n","  '29',\n","  '30',\n","  '31',\n","  '32',\n","  '33',\n","  '34',\n","  '35',\n","  '36',\n","  '37',\n","  '38',\n","  '39',\n","  '40',\n","  '41',\n","  '42',\n","  '43',\n","  '44',\n","  '45',\n","  '46',\n","  '47',\n","  '48',\n","  '49',\n","  '50',\n","  '51',\n","  '52',\n","  '53',\n","  '54',\n","  '55',\n","  '56',\n","  '57',\n","  '58',\n","  '59',\n","  '60',\n","  '61',\n","  '62',\n","  '63',\n","  '64',\n","  '65',\n","  '66',\n","  '67',\n","  '68',\n","  '69',\n","  '70',\n","  '71',\n","  '72',\n","  '73',\n","  '74',\n","  '75',\n","  '76',\n","  '77',\n","  '78',\n","  '79',\n","  '80',\n","  '81',\n","  '82',\n","  '83',\n","  '84',\n","  '85',\n","  '86',\n","  '87',\n","  '88',\n","  '89',\n","  '90',\n","  '91',\n","  '92',\n","  '93',\n","  '94',\n","  '95',\n","  '96',\n","  '97',\n","  '98',\n","  '99',\n","  '100'],\n"," [],\n"," ['7', '44'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['67'],\n"," [],\n"," ['18', '26', '27', '28'],\n"," [],\n"," [],\n"," ['4',\n","  '5',\n","  '6',\n","  '7',\n","  '8',\n","  '9',\n","  '10',\n","  '11',\n","  '12',\n","  '13',\n","  '14',\n","  '15',\n","  '16',\n","  '17',\n","  '18',\n","  '19',\n","  '20',\n","  '21',\n","  '22',\n","  '23',\n","  '24',\n","  '25',\n","  '26',\n","  '27',\n","  '28',\n","  '30',\n","  '31',\n","  '32',\n","  '33',\n","  '34',\n","  '35',\n","  '36',\n","  '37',\n","  '38',\n","  '40',\n","  '41',\n","  '42',\n","  '43',\n","  '44',\n","  '45',\n","  '46',\n","  '47',\n","  '48',\n","  '49',\n","  '50',\n","  '51',\n","  '52',\n","  '53',\n","  '54',\n","  '55',\n","  '56',\n","  '57',\n","  '58',\n","  '60',\n","  '61',\n","  '62',\n","  '63',\n","  '64',\n","  '65',\n","  '66',\n","  '67',\n","  '68',\n","  '70',\n","  '71',\n","  '72',\n","  '73',\n","  '74',\n","  '75',\n","  '77',\n","  '78',\n","  '81',\n","  '82',\n","  '83',\n","  '84',\n","  '85',\n","  '86',\n","  '87',\n","  '88',\n","  '89',\n","  '90',\n","  '91',\n","  '92',\n","  '93',\n","  '94',\n","  '95',\n","  '96',\n","  '97',\n","  '98'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," ['97'],\n"," [],\n"," ['4', '5', '6', '7', '8', '33'],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," [],\n"," []]"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## all tokens"],"metadata":{"id":"mZnKX6YdthAM"}},{"cell_type":"code","source":["def get_copy_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_moved = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        for word in words:\n","        # for word in words[1:]:\n","            # word = words[-1]\n","            pred_tokens = [\n","                model.tokenizer.decode(token)\n","                for token in torch.topk(\n","                    logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","                ).indices\n","            ]\n","\n","            token_in_topK = 'no'\n","            if \" \" + prompt[word] in pred_tokens or prompt[word] in pred_tokens:\n","                n_right += 1\n","                words_moved.append(prompt[word])\n","                token_in_topK = 'yes'\n","            pred_tokens_dict[prompt[word]] = (pred_tokens, token_in_topK)\n","\n","    percent_right = (n_right / (dataset.N * len(words))) * 100\n","    # percent_right = (n_right / (dataset.N )) * 100\n","    if percent_right > 0:\n","        print(f\"Copy circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        return pred_tokens_dict\n","    else:\n","        # return words_moved\n","        return percent_right"],"metadata":{"id":"qs1yXNo8toUk","executionInfo":{"status":"ok","timestamp":1702706542861,"user_tz":300,"elapsed":314,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def get_next_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_moved = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        for word in words:\n","        # for word in words[1:]:\n","            # word = words[-1]\n","            pred_tokens = [\n","                model.tokenizer.decode(token)\n","                for token in torch.topk(\n","                    logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","                ).indices\n","            ]\n","\n","            # get next member after digit prompt[word]\n","            next_word = str(int(prompt[word]) + 1)\n","\n","            nextToken_in_topK = 'no'\n","            if \" \" + next_word in pred_tokens or next_word in pred_tokens:\n","                n_right += 1\n","                words_moved.append(prompt[word])\n","                nextToken_in_topK = 'yes'\n","            pred_tokens_dict[prompt[word]] = (pred_tokens, next_word, nextToken_in_topK)\n","\n","    percent_right = (n_right / (dataset.N * len(words))) * 100\n","    # percent_right = (n_right / (dataset.N )) * 100\n","    if percent_right > 0:\n","        print(f\"Next circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        return pred_tokens_dict\n","    else:\n","        # return words_moved\n","        return percent_right"],"metadata":{"id":"5hgbv7olttpi","executionInfo":{"status":"ok","timestamp":1702706543141,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["top_val = [(1,5), (4,4), (6,1), (7, 10), (7, 11), (8, 11), (9, 1)]\n","for index, (layer, head) in enumerate(top_val):\n","    print(index, get_copy_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706544357,"user_tz":300,"elapsed":1219,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ae8ee8df-4834-4f94-e365-cdf66341e9eb","id":"n8FFqALMti_u"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.0\n","1 0.0\n","Copy circuit for head 6.1 (sign=1) : Top 5 accuracy: 1.7500000000000002%\n","2 1.7500000000000002\n","Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 87.0%\n","3 87.0\n","Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 17.25%\n","4 17.25\n","Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 76.0%\n","5 76.0\n","Copy circuit for head 9.1 (sign=1) : Top 5 accuracy: 60.25%\n","6 60.25\n"]}]},{"cell_type":"code","source":["top_val = [(1,5), (4,4), (6,1), (7, 10), (7, 11), (8, 11), (9, 1)]\n","for index, (layer, head) in enumerate(top_val):\n","    print(index, get_next_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706545894,"user_tz":300,"elapsed":1539,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"751a87bb-fd17-4b87-d03d-a0c736867d2f","id":"XFo3KlOqti_2"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.0\n","1 0.0\n","Next circuit for head 6.1 (sign=1) : Top 5 accuracy: 14.499999999999998%\n","2 14.499999999999998\n","Next circuit for head 7.10 (sign=1) : Top 5 accuracy: 48.75%\n","3 48.75\n","Next circuit for head 7.11 (sign=1) : Top 5 accuracy: 9.0%\n","4 9.0\n","Next circuit for head 8.11 (sign=1) : Top 5 accuracy: 26.0%\n","5 26.0\n","Next circuit for head 9.1 (sign=1) : Top 5 accuracy: 87.25%\n","6 87.25\n"]}]},{"cell_type":"code","source":["top_val = [(0, 10), (0, 1), (5,5), (6,1), (7, 10), (8,8), (7,11), (8,11), (9,1), (9,5), (10,7)]\n","for index, (layer, head) in enumerate(top_val):\n","    print(index, get_next_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsbSH4Gdx6kB","executionInfo":{"status":"ok","timestamp":1702706548067,"user_tz":300,"elapsed":2174,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cb632415-6fd9-4c1c-a3e2-3663e77d0a87"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.0\n","1 0.0\n","Next circuit for head 5.5 (sign=1) : Top 5 accuracy: 0.25%\n","2 0.25\n","Next circuit for head 6.1 (sign=1) : Top 5 accuracy: 14.499999999999998%\n","3 14.499999999999998\n","Next circuit for head 7.10 (sign=1) : Top 5 accuracy: 48.75%\n","4 48.75\n","Next circuit for head 8.8 (sign=1) : Top 5 accuracy: 55.25%\n","5 55.25\n","Next circuit for head 7.11 (sign=1) : Top 5 accuracy: 9.0%\n","6 9.0\n","Next circuit for head 8.11 (sign=1) : Top 5 accuracy: 26.0%\n","7 26.0\n","Next circuit for head 9.1 (sign=1) : Top 5 accuracy: 87.25%\n","8 87.25\n","9 0.0\n","10 0.0\n"]}]},{"cell_type":"markdown","source":["Compare these scores to copy scores"],"metadata":{"id":"_oRBQ759--B8"}},{"cell_type":"code","source":["top_val = [(0, 10), (0, 1), (5,5), (6,1), (7, 10), (8,8), (7,11), (8,11), (9,1), (9,5), (10,7)]\n","for index, (layer, head) in enumerate(top_val):\n","    print(index, get_copy_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RC9ONjNG-9hp","executionInfo":{"status":"ok","timestamp":1702706550053,"user_tz":300,"elapsed":1988,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"430c7af6-ffc5-475b-ca6b-b5605e11aba0"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.0\n","1 0.0\n","Copy circuit for head 5.5 (sign=1) : Top 5 accuracy: 0.75%\n","2 0.75\n","Copy circuit for head 6.1 (sign=1) : Top 5 accuracy: 1.7500000000000002%\n","3 1.7500000000000002\n","Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 87.0%\n","4 87.0\n","Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 37.0%\n","5 37.0\n","Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 17.25%\n","6 17.25\n","Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 76.0%\n","7 76.0\n","Copy circuit for head 9.1 (sign=1) : Top 5 accuracy: 60.25%\n","8 60.25\n","9 0.0\n","10 0.0\n"]}]},{"cell_type":"code","source":["get_next_scores(model, 8, 8, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ApPes0FYtLdV","executionInfo":{"status":"ok","timestamp":1702706550400,"user_tz":300,"elapsed":350,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"afe5df16-221c-4e42-faf9-d1e18d11eab2"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Next circuit for head 8.8 (sign=1) : Top 5 accuracy: 55.25%\n"]},{"output_type":"execute_result","data":{"text/plain":["{'1': ([' multiplayer', ' Multiplayer', 'Thread', ' Instant', ' Realm'],\n","  '2',\n","  'no'),\n"," '2': ([' multiplayer', 'nerg', 'Thread', ' Instant', ' Multiplayer'],\n","  '3',\n","  'no'),\n"," '3': ([' multiplayer', 'Thread', 'nerg', 'Assembly', ' Multiplayer'],\n","  '4',\n","  'no'),\n"," '4': (['Thread', 'Assembly', ' multiplayer', 'nerg', 'ortal'], '5', 'no'),\n"," '5': (['Thread', 'Cooldown', '�', 'nerg', ' Realm'], '6', 'no'),\n"," '6': (['Thread', 'Cooldown', 'nerg', 'pload', '�'], '7', 'no'),\n"," '7': (['Thread', 'thread', ' multiplayer', '�', ' multit'], '8', 'no'),\n"," '8': (['Thread', 'Cooldown', 'ROM', 'RAM', '�'], '9', 'no'),\n"," '9': (['Thread', ' Realm', 'abyte', 'Cooldown', ' Realms'], '10', 'no'),\n"," '10': (['Cooldown', 'Thread', 'Timeout', '�', ' cooldown'], '11', 'no'),\n"," '11': (['Thread', ' MSI', '━', 'Mom', ' multiplayer'], '12', 'no'),\n"," '12': (['Cooldown', 'Thread', 'Timeout', 'ynchron', 'Mom'], '13', 'no'),\n"," '13': (['Cooldown', 'Thread', ' cooldown', ' binds', 'ynchron'], '14', 'no'),\n"," '14': (['Cooldown', 'Thread', 'Mom', 'mom', ' cooldown'], '15', 'no'),\n"," '15': (['Cooldown', 'Thread', ' cooldown', 'nerg', 'mom'], '16', 'no'),\n"," '16': (['Cooldown', 'Thread', 'nerg', 'Assembly', ' multiplayer'],\n","  '17',\n","  'no'),\n"," '17': (['Cooldown',\n","   'Thread',\n","   'Assembly',\n","   '--------------------------------------------------------',\n","   'mom'],\n","  '18',\n","  'no'),\n"," '18': (['Cooldown', 'Thread', 'Timeout', 'Mom', ' fathers'], '19', 'no'),\n"," '19': (['Thread', 'Mom', 'Cooldown', 'mom', ' Instant'], '20', 'no'),\n"," '20': (['Cooldown', 'Thread', 'Timeout', ' Instant', 'nerg'], '21', 'no'),\n"," '21': (['Thread', 'Cooldown', 'mom', 'Mom', 'thread'], '22', 'no'),\n"," '22': (['Cooldown', 'Timeout', 'Thread', 'Mom', 'mom'], '23', 'no'),\n"," '23': (['Cooldown', 'Thread', 'mom', 'ynchron', ' cooldown'], '24', 'no'),\n"," '24': (['Cooldown', 'Thread', 'ynchron', 'Timeout', 'Mom'], '25', 'no'),\n"," '25': (['Cooldown', ' cooldown', 'nerg', 'Thread', ' Instant'], '26', 'no'),\n"," '26': (['Cooldown', 'Thread', 'Timeout', 'ynchron', 'Mom'], '27', 'no'),\n"," '27': (['Cooldown', 'Thread', ' cooldown', 'ynchron', 'mom'], '28', 'no'),\n"," '28': (['Cooldown', 'ynchron', ' synchronization', 'Thread', 'Timeout'],\n","  '29',\n","  'no'),\n"," '29': (['Cooldown', 'mom', 'Mom', 'Thread', ' cooldown'], '30', 'no'),\n"," '30': (['Cooldown', 'Timeout', 'ynchron', ' cooldown', 'mom'], '31', 'no'),\n"," '31': (['Thread', 'ynchron', 'Cooldown', 'mom', ' Parent'], '32', 'no'),\n"," '32': (['Thread', 'Cooldown', 'Parent', ' fathers', ' sperm'], '33', 'no'),\n"," '33': (['Cooldown', 'nerg', 'mom', ' cooldown', 'Thread'], '34', 'no'),\n"," '34': (['Cooldown', ' fathers', ' pregn', ' sperm', 'Mom'], '35', 'no'),\n"," '35': (['Cooldown', 'nerg', ' cooldown', 'mom', 'Ireland'], '36', 'no'),\n"," '36': (['Thread', 'Cooldown', ' fathers', 'Mom', 'mom'], '37', 'no'),\n"," '37': (['mom', 'Thread', 'Ireland', '�', 'Mom'], '38', 'no'),\n"," '38': (['Cooldown', '��', ' Instant', 'mom', '�'], '39', 'no'),\n"," '39': (['mom', 'Thread', ' Immortal', ' Frag', 'Ireland'], '40', 'no'),\n"," '40': (['Cooldown', 'nerg', '��', '�', ' Instant'], '41', 'no'),\n"," '41': (['Thread', 'nerg', ' Frag', '�', 'Dom'], '42', 'no'),\n"," '42': (['nerg', '�', 'Thread', 'Cooldown', ' Instant'], '43', 'no'),\n"," '43': (['mom', 'Thread', '�', ' cooldown', 'nerg'], '44', 'no'),\n"," '44': (['mom', 'Mom', '�', 'Cooldown', 'nerg'], '45', 'no'),\n"," '45': (['Cooldown', 'mom', 'Thread', '�', ' cooldown'], '46', 'no'),\n"," '46': (['Thread', '�', 'nerg', 'Cooldown', '��'], '47', 'no'),\n"," '47': (['Thread', '�', 'mom', 'Script', ' Instant'], '48', 'no'),\n"," '48': (['Cooldown', '�', 'ynchron', '��', 'Thread'], '49', 'no'),\n"," '49': (['�', '��', 'Cooldown', 'mom', ' Instant'], '50', 'no'),\n"," '50': (['Cooldown', '�', 'nerg', ' cooldown', ' possessions'], '51', 'no'),\n"," '51': (['�', 'Thread', ' Realm', 'Dom', 'mom'], '52', 'no'),\n"," '52': (['Cooldown', '�', 'Thread', ' fathers', 'INST'], '53', 'no'),\n"," '53': (['Cooldown', '�', 'mom', 'Thread', ' cooldown'], '54', 'no'),\n"," '54': (['Cooldown', '��', '�', 'Thread', 'nerg'], '55', 'no'),\n"," '55': (['Cooldown', 'nerg', ' cooldown', '�', ' dependencies'], '56', 'no'),\n"," '56': (['�', 'Thread', '��', 'Cooldown', 'nerg'], '57', 'no'),\n"," '57': (['�', 'Thread', 'mom', '��', ' Realm'], '58', 'no'),\n"," '58': (['��', '�', 'Cooldown', 'Thread', 'nerg'], '59', 'no'),\n"," '59': (['�', '��', ' Realm', 'Thread', ' Immortal'], '60', 'no'),\n"," '60': (['Cooldown', 'Battery', '�', '��', 'nerg'], '61', 'no'),\n"," '61': (['Thread', 'DOM', '�', 'Dom', 'Chapter'], '62', 'no'),\n"," '62': (['Cooldown', '�', 'nerg', '��', 'Thread'], '63', 'no'),\n"," '63': (['Thread', 'Cooldown', ' dependencies', ' dependency', '�'],\n","  '64',\n","  'no'),\n"," '64': ([' ARM', '��', 'Thread', 'ortal', ' Unloaded'], '65', 'no'),\n"," '65': (['Cooldown', 'nerg', '�', ' dependencies', ' cooldown'], '66', 'no'),\n"," '66': (['nerg', '�', 'Cooldown', 'Thread', ' dependencies'], '67', 'no'),\n"," '67': (['�', 'Thread', ' dependencies', 'Cooldown', 'ROM'], '68', 'no'),\n"," '68': (['Cooldown', '�', '��', 'Timeout', 'Thread'], '69', 'no'),\n"," '69': (['�', 'Cooldown', ' Immortal', 'Ireland', 'ァ'], '70', 'no'),\n"," '70': (['Cooldown', '�', 'Battery', ' cooldown', ' Immortal'], '71', 'no'),\n"," '71': (['�', 'Thread', 'mom', 'ァ', ' dependencies'], '72', 'no'),\n"," '72': (['�', 'Cooldown', ' Instant', 'Thread', 'mom'], '73', 'no'),\n"," '73': (['�', 'Cooldown', 'mom', ' cooldown', ' dependencies'], '74', 'no'),\n"," '74': (['�', 'mom', 'Cooldown', 'nerg', 'Dom'], '75', 'no'),\n"," '75': (['Cooldown', '�', ' cooldown', 'nerg', 'VOL'], '76', 'no'),\n"," '76': (['�', ' abroad', 'Thread', 'Dom', 'Ireland'], '77', 'no'),\n"," '77': (['�', ' dependencies', 'Ireland', 'Dom', 'mom'], '78', 'no'),\n"," '78': (['�', 'Cooldown', 'Battery', 'nerg', 'VOL'], '79', 'no'),\n"," '79': (['�', 'Script', 'script', ' dependencies', 'ROM'], '80', 'no'),\n"," '80': (['�', 'Cooldown', 'Battery', 'nerg', 'ROM'], '81', 'no'),\n"," '81': (['�', 'Dom', ' dependencies', 'Thread', 'DOM'], '82', 'no'),\n"," '82': (['�', ' abroad', ' dependencies', ' MSI', 'INST'], '83', 'no'),\n"," '83': (['�', ' dependencies', 'Dom', ' dependency', 'INST'], '84', 'no'),\n"," '84': (['�', 'Dom', 'nerg', 'Battery', 'INST'], '85', 'no'),\n"," '85': (['�', 'Cooldown', 'Battery', 'nerg', ' abroad'], '86', 'no'),\n"," '86': (['ROM', ' dependencies', ' abroad', 'ァ', '��'], '87', 'no'),\n"," '87': (['�', 'Thread', ' MSI', ' emulation', 'nerg'], '88', 'no'),\n"," '88': (['�', 'ynchron', ' MSI', 'Cooldown', '��'], '89', 'no'),\n"," '89': (['�', '��', 'ROM', 'Script', 'Cooldown'], '90', 'no'),\n"," '90': (['�', 'Cooldown', 'Battery', 'nerg', ' possessions'], '91', 'no'),\n"," '91': (['�', 'Dom', 'Thread', 'DOM', 'mom'], '92', 'no'),\n"," '92': (['�', 'Cooldown', 'VOL', ' abroad', 'Dom'], '93', 'no'),\n"," '93': (['�', 'Cooldown', 'Dom', 'Thread', ' dependencies'], '94', 'no'),\n"," '94': (['�', 'Cooldown', 'nerg', 'Thread', 'Timeout'], '95', 'no'),\n"," '95': (['�', 'Cooldown', 'nerg', ' cooldown', ' Instant'], '96', 'no'),\n"," '96': (['�', '��', 'Cooldown', ' dependencies', ' abroad'], '97', 'no'),\n"," '97': (['�', 'Cooldown', ' MSI', 'Ireland', ' emulation'], '98', 'no'),\n"," '98': (['�', '��', 'Cooldown', ' MSI', 'nerg'], '99', 'no'),\n"," '99': (['abyte', ' dependencies', 'nerg', '�', 'Product'], '100', 'no'),\n"," '100': (['�', 'Cooldown', ' possessions', 'nerg', 'Dom'], '101', 'no'),\n"," '101': ([' 102', ' answers', ' specificity', ' 101', ' 103'], '102', 'yes'),\n"," '102': ([' 103', ' 137', ' 127', ' 133', ' 104'], '103', 'yes'),\n"," '103': ([' 104', ' 106', ' 102', ' 105', 'ْ'], '104', 'yes')}"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["# loop over all heads"],"metadata":{"id":"HBw-zaoQ1KxU"}},{"cell_type":"markdown","source":["only print out if there's a match"],"metadata":{"id":"lfRSmIz31LvS"}},{"cell_type":"code","source":["all_copy_scores = []\n","all_heads = [(layer, head) for layer in range(12) for head in range(12)]\n","for index, (layer, head) in enumerate(all_heads):\n","    all_copy_scores.append(get_copy_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZqG-leY32dn","executionInfo":{"status":"ok","timestamp":1702706637415,"user_tz":300,"elapsed":26681,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ba1f070a-f49e-43a6-dff4-ec8d2ac6e4ec"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Copy circuit for head 1.7 (sign=1) : Top 5 accuracy: 2.0618556701030926%\n","Copy circuit for head 2.1 (sign=1) : Top 5 accuracy: 1.5463917525773196%\n","Copy circuit for head 2.4 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 1.2886597938144329%\n","Copy circuit for head 4.8 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 11.34020618556701%\n","Copy circuit for head 5.1 (sign=1) : Top 5 accuracy: 48.71134020618557%\n","Copy circuit for head 5.5 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Copy circuit for head 6.1 (sign=1) : Top 5 accuracy: 1.804123711340206%\n","Copy circuit for head 6.4 (sign=1) : Top 5 accuracy: 0.5154639175257731%\n","Copy circuit for head 6.9 (sign=1) : Top 5 accuracy: 83.76288659793815%\n","Copy circuit for head 6.10 (sign=1) : Top 5 accuracy: 1.0309278350515463%\n","Copy circuit for head 7.1 (sign=1) : Top 5 accuracy: 1.0309278350515463%\n","Copy circuit for head 7.2 (sign=1) : Top 5 accuracy: 79.12371134020619%\n","Copy circuit for head 7.5 (sign=1) : Top 5 accuracy: 0.7731958762886598%\n","Copy circuit for head 7.7 (sign=1) : Top 5 accuracy: 2.3195876288659796%\n","Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 5.412371134020619%\n","Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 87.37113402061856%\n","Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 16.49484536082474%\n","Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 75.25773195876289%\n","Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Copy circuit for head 8.6 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 36.597938144329895%\n","Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 76.28865979381443%\n","Copy circuit for head 9.1 (sign=1) : Top 5 accuracy: 59.27835051546392%\n","Copy circuit for head 9.3 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 85.56701030927834%\n","Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 15.463917525773196%\n","Copy circuit for head 10.1 (sign=1) : Top 5 accuracy: 1.804123711340206%\n","Copy circuit for head 10.2 (sign=1) : Top 5 accuracy: 95.10309278350515%\n","Copy circuit for head 11.2 (sign=1) : Top 5 accuracy: 14.690721649484537%\n","Copy circuit for head 11.4 (sign=1) : Top 5 accuracy: 17.525773195876287%\n","Copy circuit for head 11.11 (sign=1) : Top 5 accuracy: 0.7731958762886598%\n"]}]},{"cell_type":"code","source":["sum(all_copy_scores)/len(all_copy_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTFw1DAx4C0W","executionInfo":{"status":"ok","timestamp":1702706637416,"user_tz":300,"elapsed":13,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1dce8e45-f230-4e3c-9788-f35133c5ce11"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.725587056128294"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["all_next_scores = []\n","all_heads = [(layer, head) for layer in range(12) for head in range(12)]\n","for index, (layer, head) in enumerate(all_heads):\n","    all_next_scores.append(get_next_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6OZZDpP3JZZ","executionInfo":{"status":"ok","timestamp":1702706664322,"user_tz":300,"elapsed":26918,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b6b710dd-3b3d-419e-85c1-34515216cca7"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Next circuit for head 1.7 (sign=1) : Top 5 accuracy: 0.7731958762886598%\n","Next circuit for head 2.1 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Next circuit for head 3.5 (sign=1) : Top 5 accuracy: 2.3195876288659796%\n","Next circuit for head 4.9 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Next circuit for head 5.0 (sign=1) : Top 5 accuracy: 7.731958762886598%\n","Next circuit for head 5.1 (sign=1) : Top 5 accuracy: 2.0618556701030926%\n","Next circuit for head 5.5 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Next circuit for head 6.1 (sign=1) : Top 5 accuracy: 14.948453608247423%\n","Next circuit for head 6.9 (sign=1) : Top 5 accuracy: 57.73195876288659%\n","Next circuit for head 7.2 (sign=1) : Top 5 accuracy: 31.185567010309278%\n","Next circuit for head 7.7 (sign=1) : Top 5 accuracy: 1.5463917525773196%\n","Next circuit for head 7.8 (sign=1) : Top 5 accuracy: 1.2886597938144329%\n","Next circuit for head 7.10 (sign=1) : Top 5 accuracy: 48.71134020618557%\n","Next circuit for head 7.11 (sign=1) : Top 5 accuracy: 9.278350515463918%\n","Next circuit for head 8.0 (sign=1) : Top 5 accuracy: 1.0309278350515463%\n","Next circuit for head 8.1 (sign=1) : Top 5 accuracy: 33.50515463917525%\n","Next circuit for head 8.6 (sign=1) : Top 5 accuracy: 0.25773195876288657%\n","Next circuit for head 8.8 (sign=1) : Top 5 accuracy: 55.412371134020624%\n","Next circuit for head 8.11 (sign=1) : Top 5 accuracy: 25.773195876288657%\n","Next circuit for head 9.1 (sign=1) : Top 5 accuracy: 87.37113402061856%\n","Next circuit for head 9.3 (sign=1) : Top 5 accuracy: 1.0309278350515463%\n","Next circuit for head 9.9 (sign=1) : Top 5 accuracy: 1.5463917525773196%\n","Next circuit for head 9.11 (sign=1) : Top 5 accuracy: 4.123711340206185%\n","Next circuit for head 10.2 (sign=1) : Top 5 accuracy: 77.83505154639175%\n","Next circuit for head 11.2 (sign=1) : Top 5 accuracy: 0.5154639175257731%\n","Next circuit for head 11.4 (sign=1) : Top 5 accuracy: 7.474226804123711%\n"]}]},{"cell_type":"code","source":["sum(all_next_scores)/len(all_next_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-shPbdk30Q4","executionInfo":{"status":"ok","timestamp":1702706664323,"user_tz":300,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4aa43380-abb1-4d94-84bc-76120b9333d0"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.2932416953035517"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["# months"],"metadata":{"id":"GtfDFskiOXq3"}},{"cell_type":"code","source":["def get_next_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_moved = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        for word in words:\n","        # for word in words[1:]:\n","            # word = words[-1]\n","            pred_tokens = [\n","                model.tokenizer.decode(token)\n","                for token in torch.topk(\n","                    logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","                ).indices\n","            ]\n","\n","            # get next member after digit prompt[word]\n","            next_word = str(months[months.index(prompt[word]) + 1])\n","\n","            nextToken_in_topK = 'no'\n","            if \" \" + next_word in pred_tokens or next_word in pred_tokens:\n","                print(prompt[word], next_word)\n","                n_right += 1\n","                words_moved.append(prompt[word])\n","                nextToken_in_topK = 'yes'\n","            pred_tokens_dict[prompt[word]] = (pred_tokens, next_word, nextToken_in_topK)\n","\n","    percent_right = (n_right / (dataset.N * len(words))) * 100\n","    # percent_right = (n_right / (dataset.N )) * 100\n","    if percent_right > 0:\n","        print(f\"Next circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        return pred_tokens_dict\n","    else:\n","        # return words_moved\n","        return percent_right"],"metadata":{"id":"U-MG567jPDvm","executionInfo":{"status":"ok","timestamp":1702707020279,"user_tz":300,"elapsed":282,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list():\n","    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n","    prompts_list = []\n","    for i in range(0, 8):\n","        prompt_dict = {\n","            'S1': months[i],\n","            'S2': months[i+1],\n","            'S3': months[i+2],\n","            'S4': months[i+3],\n","            'text': f\"{months[i]} {months[i+1]} {months[i+2]} {months[i+3]}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list()\n","print(prompts_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39xEr1lVOYkw","executionInfo":{"status":"ok","timestamp":1702706886151,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"24d6971f-3623-4972-d450-7b962375c33c"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'S1': 'January', 'S2': 'February', 'S3': 'March', 'S4': 'April', 'text': 'January February March April'}, {'S1': 'February', 'S2': 'March', 'S3': 'April', 'S4': 'May', 'text': 'February March April May'}, {'S1': 'March', 'S2': 'April', 'S3': 'May', 'S4': 'June', 'text': 'March April May June'}, {'S1': 'April', 'S2': 'May', 'S3': 'June', 'S4': 'July', 'text': 'April May June July'}, {'S1': 'May', 'S2': 'June', 'S3': 'July', 'S4': 'August', 'text': 'May June July August'}, {'S1': 'June', 'S2': 'July', 'S3': 'August', 'S4': 'September', 'text': 'June July August September'}, {'S1': 'July', 'S2': 'August', 'S3': 'September', 'S4': 'October', 'text': 'July August September October'}, {'S1': 'August', 'S2': 'September', 'S3': 'October', 'S4': 'November', 'text': 'August September October November'}]\n"]}]},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if key != 'text']:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702706886151,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"GUAI-uKbOvPI"},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702706886151,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"9xQjRSQJOvPP"},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["## loop over all heads"],"metadata":{"id":"q0wae9EXOdeq"}},{"cell_type":"markdown","source":["only print out if there's a match"],"metadata":{"id":"9L0G8OA5Odeq"}},{"cell_type":"code","source":["all_copy_scores = []\n","all_heads = [(layer, head) for layer in range(12) for head in range(12)]\n","for index, (layer, head) in enumerate(all_heads):\n","    all_copy_scores.append(get_copy_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706895817,"user_tz":300,"elapsed":9668,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"95c8b594-6c11-4b6a-fb72-f7f8eea4f107","id":"GTWjJ9DSOder"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 34.375%\n","Copy circuit for head 5.1 (sign=1) : Top 5 accuracy: 31.25%\n","Copy circuit for head 5.4 (sign=1) : Top 5 accuracy: 6.25%\n","Copy circuit for head 5.7 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 6.0 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 6.1 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 6.9 (sign=1) : Top 5 accuracy: 81.25%\n","Copy circuit for head 6.10 (sign=1) : Top 5 accuracy: 6.25%\n","Copy circuit for head 7.2 (sign=1) : Top 5 accuracy: 75.0%\n","Copy circuit for head 7.5 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 7.7 (sign=1) : Top 5 accuracy: 9.375%\n","Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 59.375%\n","Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 71.875%\n","Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 71.875%\n","Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 75.0%\n","Copy circuit for head 8.6 (sign=1) : Top 5 accuracy: 9.375%\n","Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 6.25%\n","Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 65.625%\n","Copy circuit for head 9.3 (sign=1) : Top 5 accuracy: 28.125%\n","Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.125%\n","Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 65.625%\n","Copy circuit for head 10.2 (sign=1) : Top 5 accuracy: 87.5%\n","Copy circuit for head 10.3 (sign=1) : Top 5 accuracy: 90.625%\n","Copy circuit for head 11.1 (sign=1) : Top 5 accuracy: 62.5%\n","Copy circuit for head 11.5 (sign=1) : Top 5 accuracy: 12.5%\n","Copy circuit for head 11.6 (sign=1) : Top 5 accuracy: 21.875%\n"]}]},{"cell_type":"code","source":["sum(all_copy_scores)/len(all_copy_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706895817,"user_tz":300,"elapsed":16,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9eecc923-cbf7-4e11-b364-654fa16adb90","id":"D7Bq8II8Oder"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.400173611111111"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["all_next_scores = []\n","all_heads = [(layer, head) for layer in range(12) for head in range(12)]\n","for index, (layer, head) in enumerate(all_heads):\n","    all_next_scores.append(get_next_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"72e8132e-f1f5-405c-e423-bd9499cc23f2","id":"QYd1bPRhOder","executionInfo":{"status":"ok","timestamp":1702707035797,"user_tz":300,"elapsed":10906,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["September October\n","Next circuit for head 3.5 (sign=1) : Top 5 accuracy: 3.125%\n","May June\n","May June\n","June July\n","June July\n","July August\n","July August\n","November December\n","Next circuit for head 5.0 (sign=1) : Top 5 accuracy: 21.875%\n","May June\n","June July\n","July August\n","August September\n","Next circuit for head 5.1 (sign=1) : Top 5 accuracy: 12.5%\n","July August\n","July August\n","Next circuit for head 5.4 (sign=1) : Top 5 accuracy: 6.25%\n","February March\n","May June\n","May June\n","May June\n","July August\n","July August\n","July August\n","August September\n","November December\n","Next circuit for head 6.1 (sign=1) : Top 5 accuracy: 28.125%\n","May June\n","May June\n","June July\n","May June\n","June July\n","July August\n","July August\n","November December\n","Next circuit for head 6.9 (sign=1) : Top 5 accuracy: 25.0%\n","June July\n","June July\n","Next circuit for head 6.10 (sign=1) : Top 5 accuracy: 6.25%\n","June July\n","June July\n","September October\n","September October\n","Next circuit for head 7.2 (sign=1) : Top 5 accuracy: 12.5%\n","September October\n","September October\n","October November\n","October November\n","November December\n","Next circuit for head 7.7 (sign=1) : Top 5 accuracy: 15.625%\n","February March\n","March April\n","March April\n","May June\n","July August\n","July August\n","July August\n","September October\n","September October\n","September October\n","Next circuit for head 7.8 (sign=1) : Top 5 accuracy: 31.25%\n","February March\n","March April\n","April May\n","April May\n","May June\n","May June\n","June July\n","April May\n","May June\n","June July\n","June July\n","August September\n","August September\n","August September\n","October November\n","August September\n","October November\n","Next circuit for head 7.11 (sign=1) : Top 5 accuracy: 53.125%\n","March April\n","July August\n","August September\n","August September\n","August September\n","Next circuit for head 8.1 (sign=1) : Top 5 accuracy: 15.625%\n","February March\n","August September\n","August September\n","Next circuit for head 8.6 (sign=1) : Top 5 accuracy: 9.375%\n","July August\n","July August\n","Next circuit for head 8.7 (sign=1) : Top 5 accuracy: 6.25%\n","February March\n","June July\n","June July\n","June July\n","August September\n","August September\n","August September\n","October November\n","October November\n","November December\n","Next circuit for head 8.8 (sign=1) : Top 5 accuracy: 31.25%\n","April May\n","April May\n","April May\n","July August\n","July August\n","July August\n","September October\n","September October\n","September October\n","Next circuit for head 9.3 (sign=1) : Top 5 accuracy: 28.125%\n","March April\n","March April\n","March April\n","August September\n","September October\n","September October\n","October November\n","August September\n","September October\n","October November\n","Next circuit for head 9.11 (sign=1) : Top 5 accuracy: 31.25%\n","March April\n","Next circuit for head 10.2 (sign=1) : Top 5 accuracy: 3.125%\n","February March\n","March April\n","March April\n","April May\n","June July\n","May June\n","June July\n","July August\n","June July\n","July August\n","August September\n","July August\n","August September\n","September October\n","July August\n","August September\n","September October\n","October November\n","August September\n","September October\n","October November\n","November December\n","Next circuit for head 10.3 (sign=1) : Top 5 accuracy: 68.75%\n","March April\n","May June\n","March April\n","May June\n","June July\n","May June\n","June July\n","July August\n","May June\n","June July\n","July August\n","June July\n","July August\n","October November\n","October November\n","November December\n","Next circuit for head 11.1 (sign=1) : Top 5 accuracy: 50.0%\n"]}]},{"cell_type":"code","source":["sum(all_next_scores)/len(all_next_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ji2ptL8mOder","executionInfo":{"status":"ok","timestamp":1702706907305,"user_tz":300,"elapsed":30,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"928a610c-4756-43c7-ea49-9349622397d9"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.1901041666666665"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["# months try months but check if NUMERAL IS IN IT"],"metadata":{"id":"ltvb9xhYQVm7"}},{"cell_type":"code","source":["def get_next_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_moved = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n","    ranks = ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth', 'eleventh', 'twelfth']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        for word in words:\n","        # for word in words[1:]:\n","            # word = words[-1]\n","            pred_tokens = [\n","                model.tokenizer.decode(token)\n","                for token in torch.topk(\n","                    logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","                ).indices\n","            ]\n","\n","            # get next member after digit prompt[word]\n","            next_word = str(ranks[months.index(prompt[word]) + 1])\n","\n","            nextToken_in_topK = 'no'\n","            if \" \" + next_word in pred_tokens or next_word in pred_tokens:\n","                # print(prompt[word], next_word)\n","                n_right += 1\n","                words_moved.append(prompt[word])\n","                nextToken_in_topK = 'yes'\n","            pred_tokens_dict[prompt[word]] = (pred_tokens, next_word, nextToken_in_topK)\n","        # if print_tokens == True:\n","        #     if \" \" + next_word in pred_tokens or next_word in pred_tokens:\n","        #         print(pred_tokens_dict)\n","\n","    percent_right = (n_right / (dataset.N * len(words))) * 100\n","    # percent_right = (n_right / (dataset.N )) * 100\n","    if percent_right > 0:\n","        print(f\"Next circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        # print(pred_tokens_dict)\n","        return pred_tokens_dict\n","    else:\n","        # return words_moved\n","        return percent_right"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702707614450,"user_tz":300,"elapsed":285,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"4LUHLLlJQVnC"},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list():\n","    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n","    prompts_list = []\n","    for i in range(0, 8):\n","        prompt_dict = {\n","            'S1': months[i],\n","            'S2': months[i+1],\n","            'S3': months[i+2],\n","            'S4': months[i+3],\n","            'text': f\"{months[i]} {months[i+1]} {months[i+2]} {months[i+3]}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list()\n","print(prompts_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706886151,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"24d6971f-3623-4972-d450-7b962375c33c","id":"IwVs9-OpQVnC"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'S1': 'January', 'S2': 'February', 'S3': 'March', 'S4': 'April', 'text': 'January February March April'}, {'S1': 'February', 'S2': 'March', 'S3': 'April', 'S4': 'May', 'text': 'February March April May'}, {'S1': 'March', 'S2': 'April', 'S3': 'May', 'S4': 'June', 'text': 'March April May June'}, {'S1': 'April', 'S2': 'May', 'S3': 'June', 'S4': 'July', 'text': 'April May June July'}, {'S1': 'May', 'S2': 'June', 'S3': 'July', 'S4': 'August', 'text': 'May June July August'}, {'S1': 'June', 'S2': 'July', 'S3': 'August', 'S4': 'September', 'text': 'June July August September'}, {'S1': 'July', 'S2': 'August', 'S3': 'September', 'S4': 'October', 'text': 'July August September October'}, {'S1': 'August', 'S2': 'September', 'S3': 'October', 'S4': 'November', 'text': 'August September October November'}]\n"]}]},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if key != 'text']:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"oVriGjHIQVnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"CUlZGWxFQVnD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## loop over all heads"],"metadata":{"id":"M_6BcEs9QVnD"}},{"cell_type":"markdown","source":["only print out if there's a match"],"metadata":{"id":"X0KHSC6qQVnD"}},{"cell_type":"code","source":["all_copy_scores = []\n","all_heads = [(layer, head) for layer in range(12) for head in range(12)]\n","for index, (layer, head) in enumerate(all_heads):\n","    all_copy_scores.append(get_copy_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706895817,"user_tz":300,"elapsed":9668,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"95c8b594-6c11-4b6a-fb72-f7f8eea4f107","id":"VkTF_1XUQVnD"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 34.375%\n","Copy circuit for head 5.1 (sign=1) : Top 5 accuracy: 31.25%\n","Copy circuit for head 5.4 (sign=1) : Top 5 accuracy: 6.25%\n","Copy circuit for head 5.7 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 6.0 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 6.1 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 6.9 (sign=1) : Top 5 accuracy: 81.25%\n","Copy circuit for head 6.10 (sign=1) : Top 5 accuracy: 6.25%\n","Copy circuit for head 7.2 (sign=1) : Top 5 accuracy: 75.0%\n","Copy circuit for head 7.5 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 7.7 (sign=1) : Top 5 accuracy: 9.375%\n","Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 59.375%\n","Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 71.875%\n","Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 71.875%\n","Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 75.0%\n","Copy circuit for head 8.6 (sign=1) : Top 5 accuracy: 9.375%\n","Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 3.125%\n","Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 6.25%\n","Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 65.625%\n","Copy circuit for head 9.3 (sign=1) : Top 5 accuracy: 28.125%\n","Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.125%\n","Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 65.625%\n","Copy circuit for head 10.2 (sign=1) : Top 5 accuracy: 87.5%\n","Copy circuit for head 10.3 (sign=1) : Top 5 accuracy: 90.625%\n","Copy circuit for head 11.1 (sign=1) : Top 5 accuracy: 62.5%\n","Copy circuit for head 11.5 (sign=1) : Top 5 accuracy: 12.5%\n","Copy circuit for head 11.6 (sign=1) : Top 5 accuracy: 21.875%\n"]}]},{"cell_type":"code","source":["sum(all_copy_scores)/len(all_copy_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702706895817,"user_tz":300,"elapsed":16,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9eecc923-cbf7-4e11-b364-654fa16adb90","id":"mdlB8z8iQVnD"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.400173611111111"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["all_next_scores = []\n","all_heads = [(layer, head) for layer in range(12) for head in range(12)]\n","for index, (layer, head) in enumerate(all_heads):\n","    all_next_scores.append(get_next_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f846e045-7252-4fa8-9d38-84a04c24ac70","executionInfo":{"status":"ok","timestamp":1702707753884,"user_tz":300,"elapsed":12797,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"8h-hFYb6QVnD"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Next circuit for head 9.1 (sign=1) : Top 5 accuracy: 31.25%\n"]}]},{"cell_type":"code","source":["sum(all_next_scores)/len(all_next_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702707753885,"user_tz":300,"elapsed":14,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4fd2b3fd-4238-489e-ec14-181813df2a0e","id":"7ZAY2Vs-QVnD"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2170138888888889"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["get_next_scores(model, 9, 1, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUnSdBSHRgyi","executionInfo":{"status":"ok","timestamp":1702707471482,"user_tz":300,"elapsed":298,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0dc8f7d8-49af-4061-be12-1cd97d8d1abd"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'January': ([' once', 'once', ' occasional', ' seventh', '296'],\n","  'February',\n","  'no'),\n"," 'February': ([' third', ' fourth', ' once', ' seventh', ' fifth'],\n","  'March',\n","  'no'),\n"," 'March': ([' fifth', ' seventh', ' fourth', ' sixth', ' third'],\n","  'April',\n","  'no'),\n"," 'April': ([' seventh', ' fifth', ' sixth', 'ISC', ' five'], 'May', 'no'),\n"," 'May': ([' seventh', ' sixth', 'ISC', ' once', ' 157'], 'June', 'no'),\n"," 'June': ([' seventh', ' third', ' seven', ' sixth', 'seven'], 'July', 'no'),\n"," 'July': ([' seventh', ' eighth', 'seven', ' once', ' seven'], 'August', 'no'),\n"," 'August': (['ighth', ' eighth', ' ninth', ' final', ' occasional'],\n","  'September',\n","  'no'),\n"," 'September': ([' 120', 'ure', 'Loading', '�', ' Nguyen'], 'October', 'no'),\n"," 'October': ([' Nur', 'undrum', '�', '�', ' 121'], 'November', 'no'),\n"," 'November': (['oor', 'ة', ' 122', 'Enlarge', ' Nur'], 'December', 'no')}"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["# nw"],"metadata":{"id":"JDu3sPfwTQQa"}},{"cell_type":"code","source":["def get_next_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_moved = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    # months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n","    months = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        for word in words:\n","        # for word in words[1:]:\n","            # word = words[-1]\n","            pred_tokens = [\n","                model.tokenizer.decode(token)\n","                for token in torch.topk(\n","                    logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","                ).indices\n","            ]\n","\n","            # get next member after digit prompt[word]\n","            next_word = str(months[months.index(prompt[word]) + 1])\n","\n","            nextToken_in_topK = 'no'\n","            if \" \" + next_word in pred_tokens or next_word in pred_tokens:\n","                # print(prompt[word], next_word)\n","                n_right += 1\n","                words_moved.append(prompt[word])\n","                nextToken_in_topK = 'yes'\n","            pred_tokens_dict[prompt[word]] = (pred_tokens, next_word, nextToken_in_topK)\n","\n","    percent_right = (n_right / (dataset.N * len(words))) * 100\n","    # percent_right = (n_right / (dataset.N )) * 100\n","    if percent_right > 0:\n","        print(f\"Next circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        return pred_tokens_dict\n","    else:\n","        # return words_moved\n","        return percent_right"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702708011965,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"X2DXzu5RTQQa"},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    words = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve']\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': words[i],\n","            'S2': words[i+1],\n","            'S3': words[i+2],\n","            'S4': words[i+3],\n","            'text': f\"{words[i]} {words[i+1]} {words[i+2]} {words[i+3]}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(0, 8)\n","prompts_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702707981020,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cf15ce8e-de85-4cb2-afa6-9e6d4bce9d23","id":"IyNr5ZJeTQQb"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'S1': 'one',\n","  'S2': 'two',\n","  'S3': 'three',\n","  'S4': 'four',\n","  'text': 'one two three four'},\n"," {'S1': 'two',\n","  'S2': 'three',\n","  'S3': 'four',\n","  'S4': 'five',\n","  'text': 'two three four five'},\n"," {'S1': 'three',\n","  'S2': 'four',\n","  'S3': 'five',\n","  'S4': 'six',\n","  'text': 'three four five six'},\n"," {'S1': 'four',\n","  'S2': 'five',\n","  'S3': 'six',\n","  'S4': 'seven',\n","  'text': 'four five six seven'},\n"," {'S1': 'five',\n","  'S2': 'six',\n","  'S3': 'seven',\n","  'S4': 'eight',\n","  'text': 'five six seven eight'},\n"," {'S1': 'six',\n","  'S2': 'seven',\n","  'S3': 'eight',\n","  'S4': 'nine',\n","  'text': 'six seven eight nine'},\n"," {'S1': 'seven',\n","  'S2': 'eight',\n","  'S3': 'nine',\n","  'S4': 'ten',\n","  'text': 'seven eight nine ten'},\n"," {'S1': 'eight',\n","  'S2': 'nine',\n","  'S3': 'ten',\n","  'S4': 'eleven',\n","  'text': 'eight nine ten eleven'}]"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if key != 'text']:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702707985670,"user_tz":300,"elapsed":729,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"9WHrmrCoTQQb"},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702707989106,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"Vcg7YfDITQQb"},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":["## loop over all heads"],"metadata":{"id":"NLRrvg8QTQQb"}},{"cell_type":"markdown","source":["only print out if there's a match"],"metadata":{"id":"IHhV_G_8TQQb"}},{"cell_type":"code","source":["all_next_scores = []\n","all_heads = [(layer, head) for layer in range(12) for head in range(12)]\n","for index, (layer, head) in enumerate(all_heads):\n","    all_next_scores.append(get_next_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5dfe886-2ae3-46b9-c2c6-ad9842e7c651","executionInfo":{"status":"ok","timestamp":1702708028253,"user_tz":300,"elapsed":13600,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"F4-MdsHHTQQc"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Next circuit for head 1.7 (sign=1) : Top 5 accuracy: 3.125%\n","Next circuit for head 2.1 (sign=1) : Top 5 accuracy: 3.125%\n","Next circuit for head 4.8 (sign=1) : Top 5 accuracy: 3.125%\n","Next circuit for head 5.0 (sign=1) : Top 5 accuracy: 9.375%\n","Next circuit for head 6.1 (sign=1) : Top 5 accuracy: 75.0%\n","Next circuit for head 7.2 (sign=1) : Top 5 accuracy: 21.875%\n","Next circuit for head 7.7 (sign=1) : Top 5 accuracy: 18.75%\n","Next circuit for head 7.11 (sign=1) : Top 5 accuracy: 6.25%\n","Next circuit for head 8.1 (sign=1) : Top 5 accuracy: 43.75%\n","Next circuit for head 8.8 (sign=1) : Top 5 accuracy: 56.25%\n","Next circuit for head 9.1 (sign=1) : Top 5 accuracy: 90.625%\n","Next circuit for head 9.7 (sign=1) : Top 5 accuracy: 9.375%\n","Next circuit for head 10.2 (sign=1) : Top 5 accuracy: 43.75%\n","Next circuit for head 11.4 (sign=1) : Top 5 accuracy: 43.75%\n"]}]},{"cell_type":"code","source":["sum(all_next_scores)/len(all_next_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702708034399,"user_tz":300,"elapsed":275,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0ceb3ef3-7823-42ad-cb8b-b8c90eac558d","id":"FYE4RLgJTQQc"},"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.9730902777777777"]},"metadata":{},"execution_count":83}]},{"cell_type":"markdown","source":["# TBC"],"metadata":{"id":"MdI-noObEAwb"}},{"cell_type":"markdown","source":["The cells below have not been updated yet for next scores so disregard them:\n","\n","---\n","\n"],"metadata":{"id":"ZuBrs1i5BFET"}},{"cell_type":"markdown","source":["# Writing direction results with scatterplot"],"metadata":{"id":"pc6wzXDl1G5E"}},{"cell_type":"code","source":["def scatter_attention_and_contribution(\n","    model,\n","    layer_no,\n","    head_no,\n","    dataset,\n","    S1_is_first=False,\n","    return_vals=False,\n","    return_fig=False,\n","):\n","    \"\"\"\n","    Plot a scatter plot\n","    for each input sequence with the attention paid to S\n","    and the amount that is written in the S directions\n","    \"\"\"\n","\n","    n_heads = model.cfg.n_heads\n","    n_layers = model.cfg.n_layers\n","    model_unembed = model.unembed.W_U.detach().cpu()\n","    df = []\n","    cache = {}\n","    model.cache_all(cache)\n","\n","    logits = model(dataset.toks.long())\n","\n","    for i, prompt in enumerate(dataset.prompts):\n","        s_toks = []\n","        s_positions = []\n","        s_dirs = []\n","\n","        targ_tokens = [key for key in dataset.prompts[0].keys() if key != 'text']\n","        for s_id in targ_tokens:\n","            if S1_is_first and s_id == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                s_tok = model.tokenizer(prompt[\"S1\"])[\"input_ids\"][0]\n","            else:\n","                s_tok = model.tokenizer(\" \" + prompt[s_id])[\"input_ids\"][0]\n","            s_toks.append(s_tok)\n","\n","            toks = model.tokenizer(prompt[\"text\"])[\"input_ids\"]\n","            try:\n","                s_pos = toks.index(s_tok)\n","            except ValueError:\n","                print(f\"{s_tok} is not present in {toks}. Skipping...\")\n","                continue\n","\n","            s_pos = toks.index(s_tok)\n","            s_positions.append(s_pos)\n","\n","            s_dir = model_unembed[:, s_tok].detach()\n","            s_dirs.append(s_dir)\n","\n","        for dire, posses, tok_type in zip(s_dirs, s_positions, targ_tokens):\n","            prob = sum(\n","                [\n","                    cache[f\"blocks.{layer_no}.attn.hook_attn\"][\n","                        i, head_no, dataset.word_idx[\"end\"][i], pos\n","                    ]\n","                    .detach()\n","                    .cpu()\n","                    for pos in [posses]\n","                ]\n","            )\n","            resid = (\n","                cache[f\"blocks.{layer_no}.attn.hook_result\"][\n","                    i, dataset.word_idx[\"end\"][i], head_no, :\n","                ]\n","                .detach()\n","                .cpu()\n","            )\n","            dot = torch.einsum(\"a,a->\", resid, dire)\n","            df.append([prob, dot, tok_type, prompt[\"text\"]])\n","\n","    viz_df = pd.DataFrame(\n","        df, columns=[f\"Attn Prob on Number\", f\"Dot w Number Embed\", \"Seq Position\", \"text\"]\n","    )\n","    fig = px.scatter(\n","        viz_df,\n","        x=f\"Attn Prob on Number\",\n","        y=f\"Dot w Number Embed\",\n","        color=\"Seq Position\",\n","        hover_data=[\"text\"],\n","        title=f\"How Strong {layer_no}.{head_no} Writes in the Number Embed Direction Relative to Attn Prob\",\n","    )\n","\n","    if return_vals:\n","        return viz_df\n","    if return_fig:\n","        return fig\n","    else:\n","        fig.show()"],"metadata":{"id":"Ah8DWLlUL_3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scatter_attention_and_contribution(\n","    model=model, layer_no=9, head_no=1, dataset=dataset, S1_is_first=False\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AwhyxrN3R8xL","executionInfo":{"status":"ok","timestamp":1694632979636,"user_tz":240,"elapsed":1177,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cc1d5211-5323-4818-8b10-a819b0ee9b6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"8fbbb245-1888-4018-990a-cfc0755633c8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8fbbb245-1888-4018-990a-cfc0755633c8\")) {                    Plotly.newPlot(                        \"8fbbb245-1888-4018-990a-cfc0755633c8\",                        [{\"customdata\":[[\"1 2 3 4\"],[\"2 3 4 5\"],[\"3 4 5 6\"],[\"4 5 6 7\"],[\"5 6 7 8\"],[\"6 7 8 9\"],[\"7 8 9 10\"],[\"8 9 10 11\"],[\"9 10 11 12\"],[\"10 11 12 13\"],[\"11 12 13 14\"],[\"12 13 14 15\"],[\"13 14 15 16\"],[\"14 15 16 17\"],[\"15 16 17 18\"],[\"16 17 18 19\"],[\"17 18 19 20\"],[\"18 19 20 21\"],[\"19 20 21 22\"],[\"20 21 22 23\"],[\"21 22 23 24\"],[\"22 23 24 25\"],[\"23 24 25 26\"],[\"24 25 26 27\"],[\"25 26 27 28\"],[\"26 27 28 29\"],[\"27 28 29 30\"],[\"28 29 30 31\"],[\"29 30 31 32\"],[\"30 31 32 33\"],[\"31 32 33 34\"],[\"32 33 34 35\"],[\"33 34 35 36\"],[\"34 35 36 37\"],[\"35 36 37 38\"],[\"36 37 38 39\"],[\"37 38 39 40\"],[\"38 39 40 41\"],[\"39 40 41 42\"],[\"40 41 42 43\"],[\"41 42 43 44\"],[\"42 43 44 45\"],[\"43 44 45 46\"],[\"44 45 46 47\"],[\"45 46 47 48\"],[\"46 47 48 49\"],[\"47 48 49 50\"],[\"48 49 50 51\"],[\"49 50 51 52\"],[\"50 51 52 53\"],[\"51 52 53 54\"],[\"52 53 54 55\"],[\"53 54 55 56\"],[\"54 55 56 57\"],[\"55 56 57 58\"],[\"56 57 58 59\"],[\"57 58 59 60\"],[\"58 59 60 61\"],[\"59 60 61 62\"],[\"60 61 62 63\"],[\"61 62 63 64\"],[\"62 63 64 65\"],[\"63 64 65 66\"],[\"64 65 66 67\"],[\"65 66 67 68\"],[\"66 67 68 69\"],[\"67 68 69 70\"],[\"68 69 70 71\"],[\"69 70 71 72\"],[\"70 71 72 73\"],[\"71 72 73 74\"],[\"72 73 74 75\"],[\"73 74 75 76\"],[\"74 75 76 77\"],[\"75 76 77 78\"],[\"76 77 78 79\"],[\"77 78 79 80\"],[\"78 79 80 81\"],[\"79 80 81 82\"],[\"80 81 82 83\"],[\"81 82 83 84\"],[\"82 83 84 85\"],[\"83 84 85 86\"],[\"84 85 86 87\"],[\"85 86 87 88\"],[\"86 87 88 89\"],[\"87 88 89 90\"],[\"88 89 90 91\"],[\"89 90 91 92\"],[\"90 91 92 93\"],[\"91 92 93 94\"],[\"92 93 94 95\"],[\"93 94 95 96\"],[\"94 95 96 97\"],[\"95 96 97 98\"],[\"96 97 98 99\"],[\"97 98 99 100\"]],\"hovertemplate\":\"Seq Position=S1\\u003cbr\\u003eAttn Prob on Number=%{x}\\u003cbr\\u003eDot w Number Embed=%{y}\\u003cbr\\u003etext=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"S1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"S1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.058369945734739304,0.04661092534661293,0.07315342873334885,0.058079153299331665,0.05081278085708618,0.11673405766487122,0.06410746276378632,0.0717870220541954,0.04567893594503403,0.039213333278894424,0.05854412168264389,0.08031466603279114,0.054261840879917145,0.03899860009551048,0.09240742027759552,0.10611087083816528,0.06065232679247856,0.09251639991998672,0.07898331433534622,0.06903322041034698,0.08081388473510742,0.03569205477833748,0.0320107564330101,0.052279721945524216,0.0491148978471756,0.10003545135259628,0.05480573698878288,0.037898071110248566,0.025862809270620346,0.02583528682589531,0.04233095794916153,0.04851434752345085,0.052419427782297134,0.043383657932281494,0.056453391909599304,0.10494264215230942,0.0704803317785263,0.07697003334760666,0.026571912690997124,0.04317902773618698,0.048181820660829544,0.04515625908970833,0.0502261258661747,0.05129455402493477,0.06540762633085251,0.043131373822689056,0.03432781994342804,0.04845472425222397,0.05043718218803406,0.09936491400003433,0.06495700776576996,0.0515628308057785,0.038121163845062256,0.05191275104880333,0.09533122181892395,0.18170194327831268,0.15029354393482208,0.07766516506671906,0.04286946728825569,0.07886725664138794,0.018000103533267975,0.03606640547513962,0.016660964116454124,0.03724221885204315,0.06365592032670975,0.09405805170536041,0.07084433734416962,0.039876729249954224,0.06363839656114578,0.0882931649684906,0.10085336863994598,0.0943865180015564,0.06805043667554855,0.07206793129444122,0.11166490614414215,0.14845938980579376,0.10762715339660645,0.06946232169866562,0.07483921200037003,0.08999312669038773,0.06697554886341095,0.08379931002855301,0.0921439528465271,0.07796071469783783,0.10680976510047913,0.15941160917282104,0.15456072986125946,0.05010952055454254,0.04620009660720825,0.07470546662807465,0.09162824600934982,0.046728648245334625,0.059796884655952454,0.049967095255851746,0.060601476579904556,0.08569534868001938,0.04282321408390999],\"xaxis\":\"x\",\"y\":[0.3768908977508545,3.3398380279541016,5.914272308349609,5.945875644683838,1.6907645463943481,4.424149990081787,1.477386713027954,0.6497106552124023,2.3197755813598633,6.406676292419434,5.48015022277832,6.003168106079102,0.720984935760498,1.2707651853561401,4.938894748687744,6.343235492706299,1.5373859405517578,4.801419734954834,4.203042030334473,5.588527202606201,3.682209014892578,1.9755555391311646,0.12904953956604004,5.508935451507568,7.615622520446777,8.455768585205078,3.4924588203430176,2.0864410400390625,-2.171081781387329,5.194705486297607,4.130370140075684,8.424798965454102,8.715641975402832,6.989168643951416,10.629358291625977,11.059053421020508,6.775578498840332,6.899392127990723,4.59255838394165,9.405509948730469,3.806697368621826,3.736084461212158,2.9992051124572754,9.411905288696289,8.190970420837402,8.266132354736328,4.445199489593506,9.726814270019531,2.243312358856201,6.002878665924072,4.194843769073486,5.120826721191406,2.9950122833251953,6.381253719329834,6.219289779663086,10.017535209655762,8.869142532348633,11.829793930053711,6.019057273864746,13.980338096618652,8.61738395690918,13.62224006652832,11.977130889892578,20.62066650390625,20.179189682006836,19.001258850097656,15.598492622375488,10.774189949035645,10.798739433288574,17.576465606689453,11.124285697937012,6.998898983001709,10.250484466552734,13.603531837463379,11.921567916870117,11.731103897094727,10.72551155090332,12.362720489501953,7.078793048858643,11.89607048034668,7.730164051055908,8.697235107421875,11.613883972167969,16.410633087158203,15.140596389770508,11.522521018981934,8.523770332336426,7.170890808105469,4.3135271072387695,12.49072265625,6.126021862030029,8.614516258239746,5.327075958251953,5.030712604522705,6.076260566711426,11.192577362060547,7.133969783782959],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"1 2 3 4\"],[\"2 3 4 5\"],[\"3 4 5 6\"],[\"4 5 6 7\"],[\"5 6 7 8\"],[\"6 7 8 9\"],[\"7 8 9 10\"],[\"8 9 10 11\"],[\"9 10 11 12\"],[\"10 11 12 13\"],[\"11 12 13 14\"],[\"12 13 14 15\"],[\"13 14 15 16\"],[\"14 15 16 17\"],[\"15 16 17 18\"],[\"16 17 18 19\"],[\"17 18 19 20\"],[\"18 19 20 21\"],[\"19 20 21 22\"],[\"20 21 22 23\"],[\"21 22 23 24\"],[\"22 23 24 25\"],[\"23 24 25 26\"],[\"24 25 26 27\"],[\"25 26 27 28\"],[\"26 27 28 29\"],[\"27 28 29 30\"],[\"28 29 30 31\"],[\"29 30 31 32\"],[\"30 31 32 33\"],[\"31 32 33 34\"],[\"32 33 34 35\"],[\"33 34 35 36\"],[\"34 35 36 37\"],[\"35 36 37 38\"],[\"36 37 38 39\"],[\"37 38 39 40\"],[\"38 39 40 41\"],[\"39 40 41 42\"],[\"40 41 42 43\"],[\"41 42 43 44\"],[\"42 43 44 45\"],[\"43 44 45 46\"],[\"44 45 46 47\"],[\"45 46 47 48\"],[\"46 47 48 49\"],[\"47 48 49 50\"],[\"48 49 50 51\"],[\"49 50 51 52\"],[\"50 51 52 53\"],[\"51 52 53 54\"],[\"52 53 54 55\"],[\"53 54 55 56\"],[\"54 55 56 57\"],[\"55 56 57 58\"],[\"56 57 58 59\"],[\"57 58 59 60\"],[\"58 59 60 61\"],[\"59 60 61 62\"],[\"60 61 62 63\"],[\"61 62 63 64\"],[\"62 63 64 65\"],[\"63 64 65 66\"],[\"64 65 66 67\"],[\"65 66 67 68\"],[\"66 67 68 69\"],[\"67 68 69 70\"],[\"68 69 70 71\"],[\"69 70 71 72\"],[\"70 71 72 73\"],[\"71 72 73 74\"],[\"72 73 74 75\"],[\"73 74 75 76\"],[\"74 75 76 77\"],[\"75 76 77 78\"],[\"76 77 78 79\"],[\"77 78 79 80\"],[\"78 79 80 81\"],[\"79 80 81 82\"],[\"80 81 82 83\"],[\"81 82 83 84\"],[\"82 83 84 85\"],[\"83 84 85 86\"],[\"84 85 86 87\"],[\"85 86 87 88\"],[\"86 87 88 89\"],[\"87 88 89 90\"],[\"88 89 90 91\"],[\"89 90 91 92\"],[\"90 91 92 93\"],[\"91 92 93 94\"],[\"92 93 94 95\"],[\"93 94 95 96\"],[\"94 95 96 97\"],[\"95 96 97 98\"],[\"96 97 98 99\"],[\"97 98 99 100\"]],\"hovertemplate\":\"Seq Position=S2\\u003cbr\\u003eAttn Prob on Number=%{x}\\u003cbr\\u003eDot w Number Embed=%{y}\\u003cbr\\u003etext=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"S2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"S2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.07873581349849701,0.10731079429388046,0.10352344065904617,0.10348695516586304,0.09009633958339691,0.17240649461746216,0.16490213572978973,0.18446506559848785,0.14336895942687988,0.24087612330913544,0.182461678981781,0.19920727610588074,0.09732425957918167,0.1466037929058075,0.10900336503982544,0.15884412825107574,0.10182013362646103,0.0984463021159172,0.14454971253871918,0.2178499698638916,0.10851317644119263,0.06955814361572266,0.09981309622526169,0.18695871531963348,0.15416507422924042,0.25141441822052,0.17032141983509064,0.23803485929965973,0.23469752073287964,0.19960874319076538,0.13889314234256744,0.18100903928279877,0.13108067214488983,0.1346719115972519,0.2294027954339981,0.2769585847854614,0.21540307998657227,0.1264331191778183,0.18459691107273102,0.2718696594238281,0.08831173181533813,0.12850745022296906,0.022050144150853157,0.23000450432300568,0.1336274892091751,0.19128450751304626,0.12012644112110138,0.160847008228302,0.24395577609539032,0.21175669133663177,0.08537479490041733,0.15026815235614777,0.06762507557868958,0.21020962297916412,0.15785855054855347,0.2238786667585373,0.17623955011367798,0.0939125269651413,0.24846507608890533,0.21350106596946716,0.12897664308547974,0.17042005062103271,0.09476252645254135,0.2975817918777466,0.18166472017765045,0.2168208509683609,0.2620604336261749,0.14649614691734314,0.2772209048271179,0.2603856325149536,0.06746089458465576,0.1280277967453003,0.06588388979434967,0.32126760482788086,0.19367793202400208,0.21325281262397766,0.2291259914636612,0.1752614974975586,0.2687947750091553,0.18869233131408691,0.06370814889669418,0.13363142311573029,0.10865548253059387,0.23996925354003906,0.11224327981472015,0.19627171754837036,0.11984694749116898,0.08880522102117538,0.17390993237495422,0.341315895318985,0.052027735859155655,0.19120226800441742,0.10694733262062073,0.29526934027671814,0.10370046645402908,0.2415165901184082,0.12343640625476837],\"xaxis\":\"x\",\"y\":[4.1631855964660645,6.472253799438477,10.069902420043945,9.48226547241211,4.522725582122803,9.023200988769531,5.206195831298828,6.425249099731445,10.334317207336426,10.548896789550781,9.432479858398438,9.347149848937988,-0.15148711204528809,7.17771053314209,10.721237182617188,12.275752067565918,6.181478977203369,6.658269882202148,7.191499710083008,7.280240058898926,6.802263259887695,0.8435254096984863,2.020536422729492,10.580190658569336,11.837533950805664,11.788631439208984,6.903816223144531,5.243185520172119,10.290245056152344,4.429510116577148,7.7107696533203125,10.592881202697754,9.325336456298828,13.958169937133789,15.456231117248535,15.39303207397461,8.862814903259277,9.275005340576172,12.556304931640625,12.248733520507812,5.328000068664551,5.854595184326172,5.520805358886719,13.168643951416016,12.006406784057617,9.444724082946777,11.69625473022461,9.949085235595703,11.072637557983398,8.609986305236816,6.194457054138184,6.449173927307129,5.27100944519043,9.831345558166504,12.451213836669922,16.25992774963379,13.84737777709961,8.685918807983398,19.16136360168457,13.913079261779785,10.883277893066406,15.875748634338379,17.705974578857422,24.480701446533203,23.895715713500977,21.258220672607422,19.54884910583496,13.221105575561523,21.57874870300293,17.378948211669922,11.94927978515625,9.35794448852539,9.462553977966309,18.19504165649414,18.095977783203125,17.566240310668945,15.30600643157959,11.817265510559082,18.014394760131836,12.620097160339355,9.518659591674805,11.118144035339355,14.920135498046875,18.530553817749023,18.200014114379883,15.725235939025879,11.857934951782227,6.286099433898926,13.536431312561035,14.199226379394531,7.505089282989502,8.802331924438477,5.904911994934082,11.108528137207031,12.944974899291992,13.797220230102539,12.843158721923828],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"1 2 3 4\"],[\"2 3 4 5\"],[\"3 4 5 6\"],[\"4 5 6 7\"],[\"5 6 7 8\"],[\"6 7 8 9\"],[\"7 8 9 10\"],[\"8 9 10 11\"],[\"9 10 11 12\"],[\"10 11 12 13\"],[\"11 12 13 14\"],[\"12 13 14 15\"],[\"13 14 15 16\"],[\"14 15 16 17\"],[\"15 16 17 18\"],[\"16 17 18 19\"],[\"17 18 19 20\"],[\"18 19 20 21\"],[\"19 20 21 22\"],[\"20 21 22 23\"],[\"21 22 23 24\"],[\"22 23 24 25\"],[\"23 24 25 26\"],[\"24 25 26 27\"],[\"25 26 27 28\"],[\"26 27 28 29\"],[\"27 28 29 30\"],[\"28 29 30 31\"],[\"29 30 31 32\"],[\"30 31 32 33\"],[\"31 32 33 34\"],[\"32 33 34 35\"],[\"33 34 35 36\"],[\"34 35 36 37\"],[\"35 36 37 38\"],[\"36 37 38 39\"],[\"37 38 39 40\"],[\"38 39 40 41\"],[\"39 40 41 42\"],[\"40 41 42 43\"],[\"41 42 43 44\"],[\"42 43 44 45\"],[\"43 44 45 46\"],[\"44 45 46 47\"],[\"45 46 47 48\"],[\"46 47 48 49\"],[\"47 48 49 50\"],[\"48 49 50 51\"],[\"49 50 51 52\"],[\"50 51 52 53\"],[\"51 52 53 54\"],[\"52 53 54 55\"],[\"53 54 55 56\"],[\"54 55 56 57\"],[\"55 56 57 58\"],[\"56 57 58 59\"],[\"57 58 59 60\"],[\"58 59 60 61\"],[\"59 60 61 62\"],[\"60 61 62 63\"],[\"61 62 63 64\"],[\"62 63 64 65\"],[\"63 64 65 66\"],[\"64 65 66 67\"],[\"65 66 67 68\"],[\"66 67 68 69\"],[\"67 68 69 70\"],[\"68 69 70 71\"],[\"69 70 71 72\"],[\"70 71 72 73\"],[\"71 72 73 74\"],[\"72 73 74 75\"],[\"73 74 75 76\"],[\"74 75 76 77\"],[\"75 76 77 78\"],[\"76 77 78 79\"],[\"77 78 79 80\"],[\"78 79 80 81\"],[\"79 80 81 82\"],[\"80 81 82 83\"],[\"81 82 83 84\"],[\"82 83 84 85\"],[\"83 84 85 86\"],[\"84 85 86 87\"],[\"85 86 87 88\"],[\"86 87 88 89\"],[\"87 88 89 90\"],[\"88 89 90 91\"],[\"89 90 91 92\"],[\"90 91 92 93\"],[\"91 92 93 94\"],[\"92 93 94 95\"],[\"93 94 95 96\"],[\"94 95 96 97\"],[\"95 96 97 98\"],[\"96 97 98 99\"],[\"97 98 99 100\"]],\"hovertemplate\":\"Seq Position=S3\\u003cbr\\u003eAttn Prob on Number=%{x}\\u003cbr\\u003eDot w Number Embed=%{y}\\u003cbr\\u003etext=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"S3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"S3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.38946297764778137,0.37382620573043823,0.5445653796195984,0.5873011350631714,0.6124491095542908,0.397522509098053,0.46057817339897156,0.5816602110862732,0.7233459949493408,0.561494767665863,0.5010247230529785,0.3783726096153259,0.6811946630477905,0.6343109011650085,0.4994603395462036,0.41700202226638794,0.4430747330188751,0.34226030111312866,0.4833616614341736,0.4016396999359131,0.45893749594688416,0.26862290501594543,0.7119778394699097,0.5942502021789551,0.7101374268531799,0.34007135033607483,0.5160886645317078,0.5420438051223755,0.622459888458252,0.5403756499290466,0.6339163184165955,0.4517670273780823,0.6667737364768982,0.5987311601638794,0.5561732649803162,0.32260245084762573,0.4124804437160492,0.45787790417671204,0.6902357935905457,0.4744815528392792,0.27137213945388794,0.12063872069120407,0.3179478943347931,0.3557927906513214,0.5589047074317932,0.37555402517318726,0.5430991649627686,0.5086029171943665,0.3838893175125122,0.20224280655384064,0.374148964881897,0.2173835188150406,0.6890817880630493,0.5074136257171631,0.49200162291526794,0.23679013550281525,0.3220005929470062,0.5996852517127991,0.5071170926094055,0.411350816488266,0.7441537380218506,0.6392216682434082,0.8503198027610779,0.5755881667137146,0.7143958210945129,0.532026469707489,0.5054248571395874,0.6927767395973206,0.5825222730636597,0.4524264931678772,0.4920650124549866,0.2463674247264862,0.7562913298606873,0.5156777501106262,0.5396316647529602,0.3013964891433716,0.4093829095363617,0.6560327410697937,0.5013412833213806,0.42048829793930054,0.4314861595630646,0.2643550634384155,0.569412887096405,0.5263566970825195,0.5945475697517395,0.34578022360801697,0.3729628920555115,0.5503026843070984,0.6892929077148438,0.2883768081665039,0.3511953353881836,0.36202675104141235,0.6100982427597046,0.43849602341651917,0.5833069682121277,0.34992823004722595,0.526486337184906],\"xaxis\":\"x\",\"y\":[12.447421073913574,13.613210678100586,17.158004760742188,16.135162353515625,16.329130172729492,17.162500381469727,16.630779266357422,17.891876220703125,20.57801628112793,23.519577026367188,17.507633209228516,16.295719146728516,16.704254150390625,22.568382263183594,23.67190170288086,20.760732650756836,14.956415176391602,12.795332908630371,15.640835762023926,15.359661102294922,12.076026916503906,7.631960391998291,13.745466232299805,22.7564754486084,18.740827560424805,20.53984260559082,16.082292556762695,18.486141204833984,18.364315032958984,18.26580810546875,16.654682159423828,16.70530891418457,25.042682647705078,22.529394149780273,23.14608383178711,21.81957244873047,19.180065155029297,19.255163192749023,21.96364974975586,22.984477996826172,9.772001266479492,8.532469749450684,9.987065315246582,21.58183479309082,18.209150314331055,19.79071807861328,21.804344177246094,21.090972900390625,17.67377471923828,14.527408599853516,10.519058227539062,9.946208953857422,15.60071086883545,21.280200958251953,21.524864196777344,23.242956161499023,16.262754440307617,24.29739761352539,24.445802688598633,22.74281120300293,19.158519744873047,24.693239212036133,28.34479522705078,33.100955963134766,30.637954711914062,30.770729064941406,27.62471580505371,27.055255889892578,28.002464294433594,25.078624725341797,16.45003318786621,12.271041870117188,19.986167907714844,28.967994689941406,27.74812126159668,22.945314407348633,20.96504020690918,25.568683624267578,22.646953582763672,20.60736656188965,14.405777931213379,14.783975601196289,20.904903411865234,27.401769638061523,22.7513427734375,22.822553634643555,16.972503662109375,16.097524642944336,20.77608871459961,21.83434295654297,11.071155548095703,13.375248908996582,15.888277053833008,23.22979736328125,18.275089263916016,20.7680721282959,19.848251342773438],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Attn Prob on Number\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Dot w Number Embed\"}},\"legend\":{\"title\":{\"text\":\"Seq Position\"},\"tracegroupgap\":0},\"title\":{\"text\":\"How Strong 9.1 Writes in the Number Embed Direction Relative to Attn Prob\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('8fbbb245-1888-4018-990a-cfc0755633c8');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Correlation vals"],"metadata":{"id":"U_srDb5pnjnp"}},{"cell_type":"code","source":["def get_prob_dot(  # same as scatterplot, but output x and y vals instead of plotting\n","    model,\n","    layer_no,\n","    head_no,\n","    dataset,\n","    S1_is_first=False,\n","    return_vals=False,\n","    return_fig=False,\n","):\n","    \"\"\"\n","    Plot a scatter plot\n","    for each input sequence with the attention paid to S\n","    and the amount that is written in the S directions\n","    \"\"\"\n","\n","    n_heads = model.cfg.n_heads\n","    n_layers = model.cfg.n_layers\n","    model_unembed = model.unembed.W_U.detach().cpu()\n","    # df = []\n","    all_prob = []\n","    all_dot = []\n","    cache = {}\n","    model.cache_all(cache)\n","\n","    logits = model(dataset.toks.long())\n","\n","    for i, prompt in enumerate(dataset.prompts):\n","        s_toks = []\n","        s_positions = []\n","        s_dirs = []\n","\n","        targ_tokens = [key for key in dataset.prompts[0].keys() if key != 'text']\n","        for s_id in targ_tokens:\n","            if S1_is_first and s_id == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                s_tok = model.tokenizer(prompt[\"S1\"])[\"input_ids\"][0]\n","            else:\n","                s_tok = model.tokenizer(\" \" + prompt[s_id])[\"input_ids\"][0]\n","            s_toks.append(s_tok)\n","\n","            toks = model.tokenizer(prompt[\"text\"])[\"input_ids\"]\n","            try:\n","                s_pos = toks.index(s_tok)\n","            except ValueError:\n","                print(f\"{s_tok} is not present in {toks}. Skipping...\")\n","                continue\n","\n","            s_pos = toks.index(s_tok)\n","            s_positions.append(s_pos)\n","\n","            s_dir = model_unembed[:, s_tok].detach()\n","            s_dirs.append(s_dir)\n","\n","        for dire, posses, tok_type in zip(s_dirs, s_positions, targ_tokens):\n","            prob = sum(\n","                [\n","                    cache[f\"blocks.{layer_no}.attn.hook_attn\"][\n","                        i, head_no, dataset.word_idx[\"end\"][i], pos\n","                    ]\n","                    .detach()\n","                    .cpu()\n","                    for pos in [posses]\n","                ]\n","            )\n","            resid = (\n","                cache[f\"blocks.{layer_no}.attn.hook_result\"][\n","                    i, dataset.word_idx[\"end\"][i], head_no, :\n","                ]\n","                .detach()\n","                .cpu()\n","            )\n","            dot = torch.einsum(\"a,a->\", resid, dire)\n","            #df.append([prob, dot, tok_type, prompt[\"text\"]])\n","            all_prob.append(prob)\n","            all_dot.append(dot)\n","\n","    return all_prob, all_dot\n"],"metadata":{"id":"Ma2Q7hVqQsEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=9, head_no=9, dataset=dataset, S1_is_first=False\n",")"],"metadata":{"id":"KA_YUND7xyFo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694632980363,"user_tz":240,"elapsed":741,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e5906611-ba10-4cb5-a8c2-93634cb6c5ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n"]}]},{"cell_type":"code","source":["import scipy.stats as stats\n","\n","# X and Y should be arrays, lists, or pandas Series\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0o3quHUwuCm","executionInfo":{"status":"ok","timestamp":1694632980740,"user_tz":240,"elapsed":382,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6ea3ad1b-f8f4-4a87-a6e9-c6d9b5ff25c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correlation: 0.8127540109290008\n","p-value: 8.99435128806603e-70\n"]}]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=9, head_no=1, dataset=dataset, S1_is_first=False\n",")\n","\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p867xJ4xz5cB","executionInfo":{"status":"ok","timestamp":1694632981120,"user_tz":240,"elapsed":384,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"43c36bd5-48fa-4a2c-82cf-2dbb3da7d888"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n","Correlation: 0.769961536938402\n","p-value: 2.4806569665462174e-58\n"]}]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=7, head_no=10, dataset=dataset, S1_is_first=False\n",")\n","\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELKt-ytFz8be","executionInfo":{"status":"ok","timestamp":1694632981580,"user_tz":240,"elapsed":466,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8ebd7c49-b5dc-4f29-e369-e6ff3b01b568"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n","Correlation: 0.8862134495731306\n","p-value: 1.4163996031691708e-98\n"]}]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=5, head_no=1, dataset=dataset, S1_is_first=False\n",")\n","\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RE4IdlYA_gQL","executionInfo":{"status":"ok","timestamp":1694632981910,"user_tz":240,"elapsed":338,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ede711f5-f06d-46ff-e166-9f223d7ce5cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n","Correlation: 0.6409386764009012\n","p-value: 4.6226971665520223e-35\n"]}]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=0, head_no=3, dataset=dataset, S1_is_first=False\n",")\n","\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"id":"f7NUqjvX_3-K","executionInfo":{"status":"ok","timestamp":1694633011204,"user_tz":240,"elapsed":724,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"93796647-3ad5-4b00-9194-c2ea061be0cf","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n","Correlation: 0.08677588286283226\n","p-value: 0.1397557257474165\n"]}]}]}