{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["6Fuq8XW770vX"],"toc_visible":true,"authorship_tag":"ABX9TyPVYBHefo4Es32pm0L445Go"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"abee47b7f1634e9ba3106fa13d216e12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2385bfc033104d7c9af555e7d4ba6078","IPY_MODEL_8729952fad7c4ebf9b4825af94a63291","IPY_MODEL_f547a8a52c894d0aab1b5eafc4979b4d"],"layout":"IPY_MODEL_ab3832469f324fbfb504f96d6b41d346"}},"2385bfc033104d7c9af555e7d4ba6078":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd52a98b649d43268a4a327fab3f03a1","placeholder":"​","style":"IPY_MODEL_f17f2651274e44938b5741b026d6fe75","value":"config.json: 100%"}},"8729952fad7c4ebf9b4825af94a63291":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df69f6e984e04e8f8200361984492bb2","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e22ee0a1163e4b8ba57e579d4c8a415f","value":665}},"f547a8a52c894d0aab1b5eafc4979b4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cef6a0487e44d6daaceee29b3bda60f","placeholder":"​","style":"IPY_MODEL_cf9233d71bfe4908a2400fff9182299a","value":" 665/665 [00:00&lt;00:00, 21.0kB/s]"}},"ab3832469f324fbfb504f96d6b41d346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd52a98b649d43268a4a327fab3f03a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f17f2651274e44938b5741b026d6fe75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df69f6e984e04e8f8200361984492bb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e22ee0a1163e4b8ba57e579d4c8a415f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4cef6a0487e44d6daaceee29b3bda60f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9233d71bfe4908a2400fff9182299a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be9eee319f004ce9a34385403f899e82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13b52c00c7a944908817390768208ec9","IPY_MODEL_9681e5e050ba41b7839377404e8725cc","IPY_MODEL_875961d4df0144219894d7f18b9d17cb"],"layout":"IPY_MODEL_75cbc47fd28b4a11908f7fc342358b15"}},"13b52c00c7a944908817390768208ec9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c5670878b964c69b26cd1fbf5341bb5","placeholder":"​","style":"IPY_MODEL_004e0e629fcd4188bbf294b5246850c1","value":"model.safetensors: 100%"}},"9681e5e050ba41b7839377404e8725cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51ab8ac3d4cf45ef813b077ca7486f1f","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f1a30dfb8514f10967c953d7dee5bb0","value":548105171}},"875961d4df0144219894d7f18b9d17cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d959d8947a6c4deb839b73300832cdc2","placeholder":"​","style":"IPY_MODEL_efc93bb86d124f5f8ef84388a155719c","value":" 548M/548M [00:06&lt;00:00, 85.5MB/s]"}},"75cbc47fd28b4a11908f7fc342358b15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c5670878b964c69b26cd1fbf5341bb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"004e0e629fcd4188bbf294b5246850c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51ab8ac3d4cf45ef813b077ca7486f1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f1a30dfb8514f10967c953d7dee5bb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d959d8947a6c4deb839b73300832cdc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efc93bb86d124f5f8ef84388a155719c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c7820f2dd1246e096f67d14e3cd6ed1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d336298125b4a469dbbff32f9c9ca5f","IPY_MODEL_b6535a37337a4c0692508d5d411cd011","IPY_MODEL_46633aa45b804473b9c67e45d5b08bef"],"layout":"IPY_MODEL_3ab3ea4fb31146b6be39acf66416d469"}},"8d336298125b4a469dbbff32f9c9ca5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e10f847a4d64748be1c805956071f19","placeholder":"​","style":"IPY_MODEL_4d6f5fd83e06414198394860d3887fe8","value":"generation_config.json: 100%"}},"b6535a37337a4c0692508d5d411cd011":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecd5b98d601140538d6d311b3dcf2bbb","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_162a5a7ad2a34309b05bbc0cc648a30b","value":124}},"46633aa45b804473b9c67e45d5b08bef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00f4f8011391489e87a2d3889a38ecb6","placeholder":"​","style":"IPY_MODEL_1fd39a7fc3b445b781177bf9896e6451","value":" 124/124 [00:00&lt;00:00, 3.24kB/s]"}},"3ab3ea4fb31146b6be39acf66416d469":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e10f847a4d64748be1c805956071f19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d6f5fd83e06414198394860d3887fe8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecd5b98d601140538d6d311b3dcf2bbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"162a5a7ad2a34309b05bbc0cc648a30b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"00f4f8011391489e87a2d3889a38ecb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fd39a7fc3b445b781177bf9896e6451":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec24819609f74db18e48117fca22e692":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd43803cd0fb4cadbf7763cc1faa640c","IPY_MODEL_d3672365e4234037b65c2a42979c87a2","IPY_MODEL_09b04d993cf841bc96941644a928dc0b"],"layout":"IPY_MODEL_85bb6c0a772a4761a6480d48fc0cc963"}},"bd43803cd0fb4cadbf7763cc1faa640c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_145d36e994694af8bf32f8665a2f6e74","placeholder":"​","style":"IPY_MODEL_7b08777fe2d04719b3916fc0ff22455f","value":"vocab.json: 100%"}},"d3672365e4234037b65c2a42979c87a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5f081eacff14583b0af56caa3a61431","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35c4c98bfc334cc794bf26f5057a4692","value":1042301}},"09b04d993cf841bc96941644a928dc0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be2fca1e1ff948a38f50514b4297d85e","placeholder":"​","style":"IPY_MODEL_cd924e35d7ba46c1b7ae4aa6dab789d4","value":" 1.04M/1.04M [00:00&lt;00:00, 3.03MB/s]"}},"85bb6c0a772a4761a6480d48fc0cc963":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"145d36e994694af8bf32f8665a2f6e74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b08777fe2d04719b3916fc0ff22455f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5f081eacff14583b0af56caa3a61431":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35c4c98bfc334cc794bf26f5057a4692":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be2fca1e1ff948a38f50514b4297d85e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd924e35d7ba46c1b7ae4aa6dab789d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f289cf96042a4b539717c7d81c0ae139":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f79c1120fe5148a887cbbd2d5103dc6f","IPY_MODEL_e2cbfe100dd24ea2ad358529c2cfed16","IPY_MODEL_b5647a8a1208425c8ff927beb4170b90"],"layout":"IPY_MODEL_99a97975d3a5472f8a7ad1524d25ed6c"}},"f79c1120fe5148a887cbbd2d5103dc6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87a9fb75d909411babe419fa11c64d3c","placeholder":"​","style":"IPY_MODEL_a38c47a2cbfe4be39cb2d2d9dbf37892","value":"merges.txt: 100%"}},"e2cbfe100dd24ea2ad358529c2cfed16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1737b138f40d4517abcd2a4d0cecc797","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5afcda57364d45e493373fb18d6b4da2","value":456318}},"b5647a8a1208425c8ff927beb4170b90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d01347a7e96408296ca6c8721a3b94f","placeholder":"​","style":"IPY_MODEL_d0298cff52324552a47c46ef9274a724","value":" 456k/456k [00:00&lt;00:00, 5.40MB/s]"}},"99a97975d3a5472f8a7ad1524d25ed6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87a9fb75d909411babe419fa11c64d3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a38c47a2cbfe4be39cb2d2d9dbf37892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1737b138f40d4517abcd2a4d0cecc797":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5afcda57364d45e493373fb18d6b4da2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d01347a7e96408296ca6c8721a3b94f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0298cff52324552a47c46ef9274a724":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90c64ba8334f4cb59b90960cc77350d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2894fa9320c9439c87360b5fee9ab432","IPY_MODEL_eef826587101445c922fe8eb304b0a6b","IPY_MODEL_03bd5b7da4394c468f94a52b0a13c7f6"],"layout":"IPY_MODEL_6eac70e9cd9c4fe0b304732fdba25f3b"}},"2894fa9320c9439c87360b5fee9ab432":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad429c80c06a499caf627f1eb6b77fcb","placeholder":"​","style":"IPY_MODEL_5ee704724fbd4b4596d49519ffc8c551","value":"tokenizer.json: 100%"}},"eef826587101445c922fe8eb304b0a6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d50d8d4e9d24128b8154a212e51663f","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2869cbc4abb84c94a57857a1f22a20e6","value":1355256}},"03bd5b7da4394c468f94a52b0a13c7f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_212a4b4d6c96476da412e6eb42d76164","placeholder":"​","style":"IPY_MODEL_76dc578941944b1da4106c8e01e75cf4","value":" 1.36M/1.36M [00:00&lt;00:00, 7.79MB/s]"}},"6eac70e9cd9c4fe0b304732fdba25f3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad429c80c06a499caf627f1eb6b77fcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ee704724fbd4b4596d49519ffc8c551":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d50d8d4e9d24128b8154a212e51663f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2869cbc4abb84c94a57857a1f22a20e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"212a4b4d6c96476da412e6eb42d76164":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76dc578941944b1da4106c8e01e75cf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup\n","(No need to change anything)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"rMcpSDdjIAiA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3c9826f9-9425-4968-cfa2-08651de9388f","executionInfo":{"status":"ok","timestamp":1702251356115,"user_tz":300,"elapsed":219661,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running as a Colab notebook\n","Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n","  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-27_dioce\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-27_dioce\n","  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit ce82675a8e89b6d5e6229a89620c843c794f3b04\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate>=0.23.0 (from transformer-lens==0.0.0)\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens==0.0.0)\n","  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n","  Downloading jaxtyping-0.2.24-py3-none-any.whl (38 kB)\n","Collecting numpy>=1.24 (from transformer-lens==0.0.0)\n","  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.7.0)\n","Collecting torch!=2.0,!=2.1.0,>=1.10 (from transformer-lens==0.0.0)\n","  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.35.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.5.0)\n","Collecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n","  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.4.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.4.1)\n","Collecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.9.1)\n","Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.11->transformer-lens==0.0.0)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.3.post1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.1.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.1.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.15.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: transformer-lens\n","  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=118964 sha256=a41697fddfb7a0f1f60d43d2eec2ff20c9aa08409a7870e3d636f9e7617edcf1\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-gjcg3ssy/wheels/8a/1e/37/ffb9c15454a1725b13a9d9f5e74fb91725048884ad734b8c1f\n","Successfully built transformer-lens\n","Installing collected packages: typeguard, smmap, setproctitle, sentry-sdk, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, gitdb, nvidia-cusolver-cu12, GitPython, wandb, torch, datasets, accelerate, transformer-lens\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu118\n","    Uninstalling torch-2.1.0+cu118:\n","      Successfully uninstalled torch-2.1.0+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n","torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n","torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.40 accelerate-0.25.0 beartype-0.14.1 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 einops-0.7.0 fancy-einsum-0.0.3 gitdb-4.0.11 jaxtyping-0.2.24 multiprocess-0.70.15 numpy-1.26.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pyarrow-hotfix-0.6 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 torch-2.1.1 transformer-lens-0.0.0 typeguard-2.13.3 wandb-0.16.1\n"]}],"source":["# Janky code to do different setup when run in a Colab notebook vs VSCode\n","DEBUG_MODE = False\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n","    # Install another version of node that makes PySvelte work way faster\n","    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")\n","    from IPython import get_ipython\n","\n","    ipython = get_ipython()\n","    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Z6b1n2tvIAiD","executionInfo":{"status":"ok","timestamp":1702251367471,"user_tz":300,"elapsed":11361,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zuhzYxbsIAiE","executionInfo":{"status":"ok","timestamp":1702251372147,"user_tz":300,"elapsed":4678,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"cFMTUcQiIAiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702251372148,"user_tz":300,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9583d8c5-0dec-422d-c0a4-e54c1658e76c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7c9a1bccf220>"]},"metadata":{},"execution_count":4}],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"OLkInsdjyHMx"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"xLwDyosvIAiJ","colab":{"base_uri":"https://localhost:8080/","height":226,"referenced_widgets":["abee47b7f1634e9ba3106fa13d216e12","2385bfc033104d7c9af555e7d4ba6078","8729952fad7c4ebf9b4825af94a63291","f547a8a52c894d0aab1b5eafc4979b4d","ab3832469f324fbfb504f96d6b41d346","dd52a98b649d43268a4a327fab3f03a1","f17f2651274e44938b5741b026d6fe75","df69f6e984e04e8f8200361984492bb2","e22ee0a1163e4b8ba57e579d4c8a415f","4cef6a0487e44d6daaceee29b3bda60f","cf9233d71bfe4908a2400fff9182299a","be9eee319f004ce9a34385403f899e82","13b52c00c7a944908817390768208ec9","9681e5e050ba41b7839377404e8725cc","875961d4df0144219894d7f18b9d17cb","75cbc47fd28b4a11908f7fc342358b15","3c5670878b964c69b26cd1fbf5341bb5","004e0e629fcd4188bbf294b5246850c1","51ab8ac3d4cf45ef813b077ca7486f1f","1f1a30dfb8514f10967c953d7dee5bb0","d959d8947a6c4deb839b73300832cdc2","efc93bb86d124f5f8ef84388a155719c","7c7820f2dd1246e096f67d14e3cd6ed1","8d336298125b4a469dbbff32f9c9ca5f","b6535a37337a4c0692508d5d411cd011","46633aa45b804473b9c67e45d5b08bef","3ab3ea4fb31146b6be39acf66416d469","7e10f847a4d64748be1c805956071f19","4d6f5fd83e06414198394860d3887fe8","ecd5b98d601140538d6d311b3dcf2bbb","162a5a7ad2a34309b05bbc0cc648a30b","00f4f8011391489e87a2d3889a38ecb6","1fd39a7fc3b445b781177bf9896e6451","ec24819609f74db18e48117fca22e692","bd43803cd0fb4cadbf7763cc1faa640c","d3672365e4234037b65c2a42979c87a2","09b04d993cf841bc96941644a928dc0b","85bb6c0a772a4761a6480d48fc0cc963","145d36e994694af8bf32f8665a2f6e74","7b08777fe2d04719b3916fc0ff22455f","c5f081eacff14583b0af56caa3a61431","35c4c98bfc334cc794bf26f5057a4692","be2fca1e1ff948a38f50514b4297d85e","cd924e35d7ba46c1b7ae4aa6dab789d4","f289cf96042a4b539717c7d81c0ae139","f79c1120fe5148a887cbbd2d5103dc6f","e2cbfe100dd24ea2ad358529c2cfed16","b5647a8a1208425c8ff927beb4170b90","99a97975d3a5472f8a7ad1524d25ed6c","87a9fb75d909411babe419fa11c64d3c","a38c47a2cbfe4be39cb2d2d9dbf37892","1737b138f40d4517abcd2a4d0cecc797","5afcda57364d45e493373fb18d6b4da2","6d01347a7e96408296ca6c8721a3b94f","d0298cff52324552a47c46ef9274a724","90c64ba8334f4cb59b90960cc77350d5","2894fa9320c9439c87360b5fee9ab432","eef826587101445c922fe8eb304b0a6b","03bd5b7da4394c468f94a52b0a13c7f6","6eac70e9cd9c4fe0b304732fdba25f3b","ad429c80c06a499caf627f1eb6b77fcb","5ee704724fbd4b4596d49519ffc8c551","9d50d8d4e9d24128b8154a212e51663f","2869cbc4abb84c94a57857a1f22a20e6","212a4b4d6c96476da412e6eb42d76164","76dc578941944b1da4106c8e01e75cf4"]},"executionInfo":{"status":"ok","timestamp":1702251393384,"user_tz":300,"elapsed":21239,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"550e74a1-d25b-4b0f-a71e-25cb60c1ea78"},"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abee47b7f1634e9ba3106fa13d216e12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be9eee319f004ce9a34385403f899e82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7820f2dd1246e096f67d14e3cd6ed1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec24819609f74db18e48117fca22e692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f289cf96042a4b539717c7d81c0ae139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c64ba8334f4cb59b90960cc77350d5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","source":["# Generate dataset with multiple prompts"],"metadata":{"id":"6Fuq8XW770vX"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        # self.corr_tokenIDs = [\n","        #     self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","        # ]\n","        # self.incorr_tokenIDs = [\n","        #     self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","        # ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"4wXBNWj5FwVn","executionInfo":{"status":"ok","timestamp":1702253949073,"user_tz":300,"elapsed":270,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["pos_dict = {\n","    'S1': 0,\n","    'S2': 1,\n","    'S3': 2,\n","    'S4': 3,\n","}"],"metadata":{"id":"kS_Tlrb_70vg","executionInfo":{"status":"ok","timestamp":1702253950044,"user_tz":300,"elapsed":972,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            # 'corr': str(i+4),\n","            # 'incorr': str(i+3),\n","            'text': f\"{i} {i+1} {i+2} {i+3}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 101)\n","dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"u0NPSKcZ1iDe","executionInfo":{"status":"ok","timestamp":1702253950044,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        r1 = random.randint(1, 100)\n","        r2 = random.randint(1, 100)\n","        while True:\n","            r3 = random.randint(1, 100)\n","            r4 = random.randint(1, 100)\n","            if r4 - 1 != r3:\n","                break\n","        prompt_dict = {\n","            'S1': str(r1),\n","            'S2': str(r2),\n","            'S3': str(r3),\n","            'S4': str(r4),\n","            'corr': str(i+4),\n","            'incorr': str(i+3),\n","            'text': f\"{r1} {r2} {r3} {r4}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 101)\n","# prompts_list_2 = generate_prompts_list_corr(1, 2)\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"dzzLlCqZS_wl","executionInfo":{"status":"ok","timestamp":1702253950044,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"msu6D4p_feW5","executionInfo":{"status":"ok","timestamp":1702253950044,"user_tz":300,"elapsed":1,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":["# tests"],"metadata":{"id":"cv_K7xG4DrDV"}},{"cell_type":"code","source":["dataset.toks.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gnL00CvkGMvw","executionInfo":{"status":"ok","timestamp":1702251610290,"user_tz":300,"elapsed":403,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5cc79295-94b5-40cb-bbff-927790d0fd4b"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 4])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["import torch as t\n","from torch import Tensor\n","from jaxtyping import Float, Int, Bool\n","from typing import List, Optional, Callable, Tuple, Dict, Literal, Set"],"metadata":{"id":"7DFv2xoUFedn","executionInfo":{"status":"ok","timestamp":1702251488868,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["results = t.zeros((2, model.cfg.n_layers, model.cfg.n_heads)) #, device=device\n","\n","# Define components from our model (for typechecking, and cleaner code)\n","embed = model.embed\n","mlp0 = model.blocks[0].mlp\n","ln0 = model.blocks[0].ln2\n","unembed = model.unembed\n","ln_final = model.ln_final\n","\n","# Get embeddings for the names in our list\n","# name_tokens: Int[Tensor, \"batch 1\"] = model.to_tokens(names, prepend_bos=False)\n","# name_embeddings: Int[Tensor, \"batch 1 d_model\"] = embed(name_tokens)\n","\n","embeddings = embed(dataset.toks)\n","embeddings.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7pjRnN1F5C6","executionInfo":{"status":"ok","timestamp":1702253952058,"user_tz":300,"elapsed":220,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f950c131-484b-4fe3-ac45-7fdb8702e8bd"},"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 4, 768])"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","source":["# Get residual stream after applying MLP\n","resid_after_mlp1 = embeddings + mlp0(ln0(embeddings))\n","resid_after_mlp1.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sc7G1RvMGojl","executionInfo":{"status":"ok","timestamp":1702253954381,"user_tz":300,"elapsed":170,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"25bafb16-8926-4079-b383-7c8789035a68"},"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 4, 768])"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["mlp9 = model.blocks[9].mlp\n","mlp9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwwQ79FbHAxE","executionInfo":{"status":"ok","timestamp":1702253958949,"user_tz":300,"elapsed":142,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5aa6b641-efff-4cf7-cabe-eb167f65f424"},"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLP(\n","  (hook_pre): HookPoint()\n","  (hook_post): HookPoint()\n",")"]},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["ln9 = model.blocks[9].ln2"],"metadata":{"id":"Dk4MwSmNHjVq","executionInfo":{"status":"ok","timestamp":1702253960299,"user_tz":300,"elapsed":174,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["resid_after_mlp9 = resid_after_mlp1 + mlp9(ln9(resid_after_mlp1))\n","resid_after_mlp9.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wLgTIaVFHIOJ","executionInfo":{"status":"ok","timestamp":1702251893837,"user_tz":300,"elapsed":1300,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1b1a3b29-c2f0-4ff4-ae10-37fcd97f9194"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 4, 768])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["logits = unembed(ln_final(resid_after_mlp9)).squeeze()\n","logits.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eserSHeHHs1U","executionInfo":{"status":"ok","timestamp":1702251941224,"user_tz":300,"elapsed":1345,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"498839ec-2f0e-44da-9917-9056f3ae86d0"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 4, 50257])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["k=5\n","topk_logits: Int[Tensor, \"batch k\"] = t.topk(logits, dim=-1, k=k).indices\n","topk_logits.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VA3Kpq0qH5E4","executionInfo":{"status":"ok","timestamp":1702251986838,"user_tz":300,"elapsed":543,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ace71f04-5126-41ef-b543-f1eaef944f79"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 4, 5])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WpUItiRrIiat","executionInfo":{"status":"ok","timestamp":1702252258189,"user_tz":300,"elapsed":790,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9c6b6866-5029-45a7-95b1-8955807618cf"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['S1', 'S2', 'S3', 'S4']"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzVRCWwgIhkJ","executionInfo":{"status":"ok","timestamp":1702252328303,"user_tz":300,"elapsed":259,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"aaa9c437-b460-49d0-cb97-f806beee717e"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["4 ['teenth', 'th', 'teen', 'WD', 'ND']\n","5 ['th', 'Thirty', '42', '41', '43']\n","6 ['teenth', 'th', 'teen', '03', '34']\n","7 ['th', '07', '49', '87', '46']\n","8 ['192', '98', '07', 'th', '90']\n","9 ['09', '07', '08', '999', '06']\n","10 [' minutes', ' percent', ' times', 'bp', ' years']\n","11 ['87', '10', '03', '11', '07']\n","12 ['03', '34', '02', '04', '92']\n","13 ['rd', '37', 'DD', '66', 'th']\n","14 ['teenth', 'th', '34', '37', '14']\n","15 ['th', ' minutes', '20', '15', ' years']\n","16 ['384', 'th', '16', 'burn', ' stitches']\n","17 ['th', 'rd', '76', '87', '37']\n","18 ['teenth', '37', '34', 'th', '94']\n","19 ['th', 'aldi', '61', ' months', 'teenth']\n","20 [' minutes', ' years', 'th', ' Years', 'nd']\n","21 ['st', 'nd', '50', 'rd', ' NCT']\n","22 ['nd', 'ND', ' NCT', ' sts', 'nces']\n","23 ['rd', 'RD', 'DD', 'nd', '00']\n","24 [' hrs', 'th', 'ND', ' hours', '34']\n","25 ['th', 'rd', 'ishing', '%-', 'agher']\n","26 ['th', 'nd', '26', '37', '31']\n","27 ['th', 'rd', '00', '37', '26']\n","28 ['th', '00', 'nd', 'nm', '80']\n","29 ['th', '29', 'ighth', '00', '79']\n","30 [' minutes', 'fps', ' seconds', ' Seconds', 'mins']\n","31 ['st', '803', 'rd', 'named', 'fish']\n","32 ['nd', '803', '384', '32', 'ND']\n","33 ['rd', '00', 'ield', '803', 'RD']\n","34 ['teenth', '00', '34', 'ename', '68']\n","35 ['00', 'th', ' years', ' secondly', ' seconds']\n","36 ['th', '36', '384', '00', '34']\n","37 ['th', '37', '00', '803', '46']\n","38 ['th', '37', ' bytes', '38', '32']\n","39 ['th', '29', '49', '39', '41']\n","40 ['%', ' percent', '40', '%-', 'nm']\n","41 ['%', 'icals', ' percent', ' NCT', 'Filename']\n","42 ['nd', 'ND', ' NCT', ' Ib', 'JJ']\n","43 ['rd', '00', 'ield', '73', 'RD']\n","44 ['teenth', '00', '73', '%-', '68']\n","45 ['oning', ' Minutes', '%-', ' minutes', 'th']\n","46 ['th', 'teenth', ' Minutes', '37', 'rpm']\n","47 ['eenth', '49', 'rd', 'th', '41']\n","48 ['eenth', 'een', '49', '48', '37']\n","49 ['49', 'ers', 'ERS', 'eenth', 'ield']\n","50 ['%-', ' cents', '%', '%\"', ' percent']\n","51 ['%;', '50', '%', 'ership', ' percent']\n","52 ['nd', '50', 'stown', '%', '%;']\n","53 ['rd', 'ield', '00', 'atively', 'OY']\n","54 ['64', 'teenth', '32', '00', '68']\n","55 ['%-', '%', ' Minutes', '_.', '32']\n","56 ['32', '256', ' NCT', 'eenth', '384']\n","57 ['%', '%-', '32', '66', '43']\n","58 [' Minutes', '58', '68', '32', '602']\n","59 ['%-', ' Minutes', '61', '%]', 'older']\n","60 [' Minutes', ' Seconds', 'fps', ' seconds', ' minutes']\n","61 [' Minutes', '61', '%', '50', '803']\n","62 ['803', ' Minutes', '502', '602', 'nd']\n","63 ['rd', '803', 'OY', '_.', ' stitches']\n","64 ['64', '128', '32', ' 64', '328']\n","65 [' ILCS', '328', 'izabeth', '_.', '64']\n","66 ['328', '50', '803', ' 66', ' cents']\n","67 ['66', '00', 'OY', '67', ' 67']\n","68 ['68', ' ILCS', '78', '70', '80']\n","69 ['61', 'older', '64', '69', '73']\n","70 [' ILCS', 'izabeth', '70', ' percent', '%-']\n","71 ['icals', '%;', 'mA', '803', '80']\n","72 ['80', 'nd', '803', 'ipop', ' hours']\n","73 ['rd', '803', '80', '70', '00']\n","74 ['64', '74', '69', '68', '80']\n","75 [' ILCS', '%-', '80', '%', '50']\n","76 ['ength', '80', '804', '808', '806']\n","77 ['97', '491', '751', '77', '70']\n","78 ['78', '80', '79', '803', 'rpm']\n","79 ['79', '803', '319', 'essor', ' 79']\n","80 ['80', '803', '50', 'idently', '70']\n","81 ['essor', '803', '80', 'ength', '50']\n","82 ['803', '80', '804', '102', 'nd']\n","83 ['rd', '803', '80', '502', 'ensive']\n","84 ['803', '80', '68', '64', '66']\n","85 ['%', '80', '%-', '803', '%:']\n","86 ['803', '80', ' Minutes', '802', '502']\n","87 ['803', '80', '387', 'eenth', '77']\n","88 ['80', '98', '68', '88', '77']\n","89 ['80', '79', '68', '089', '803']\n","90 ['80', '90', '%\"', 'ength', ' degree']\n","91 ['ese', '502', '80', 'icals', '803']\n","92 ['nd', '80', '502', '803', '804']\n","93 ['rd', '803', '%', '80', 'OY']\n","94 ['68', '66', '67', '803', ' 94']\n","95 ['%', '99', '95', '%-', ' percent']\n","96 ['80', '803', ' NCT', '808', '152']\n","97 ['97', ' 97', '152', ' NCT', '99']\n","98 ['98', '97', '90', ' 98', '80']\n","99 ['999', ' 99', '089', '%', 'IELD']\n","100 ['00000', '%\"', '%', ' NX', '%]']\n","101 [' 101', '101', '機', 'ERC', '102']\n","102 ['nd', '102', '502', '803', '203']\n","103 ['rd', '803', 'ERC', 'RD', ' Cosponsors']\n"]}]},{"cell_type":"code","source":["after_mlp1 = mlp0(ln0(embeddings))\n","after_mlp9 = mlp9(ln9(after_mlp1))\n","logits = unembed(ln_final(after_mlp9)).squeeze()\n","\n","for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_I1KLNUBJrIA","executionInfo":{"status":"ok","timestamp":1702252547579,"user_tz":300,"elapsed":4296,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e29c54c3-a58b-4dd9-fe55-f7ff61672f8b"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["4 ['.', ' to', '-', \"'\", ',']\n","5 [',', '-', ' to', '.', ' or']\n","6 [' to', '-', ',', 'z', '/']\n","7 [' to', ' (', ' or', ',', '.']\n","8 ['.', ',', ' or', '-', ' to']\n","9 [' to', ' T', 'to', ' L', ' M']\n","10 [' or', ',', '.', ' and', ' to']\n","11 [' to', ' (', '-', ',', '.']\n","12 ['-', ',', '.', ' (', '–']\n","13 ['-', ' (', ' which', ',', ' Image']\n","14 ['-', ' (', ',', ' M', 'to']\n","15 ['-', ',', ' and', '/', ' or']\n","16 ['-', ',', ' of', '/', ' stories']\n","17 ['-', 'to', '+', ' August', '%']\n","18 ['-', '+', ' M', ' T', ' B']\n","19 ['+', ' Jr', 'th', '-', ' of']\n","20 ['+', '%', '/', '.', 'yd']\n","21 ['st', ' st', 'y', '.', 'DE']\n","22 ['DE', 'D', 'AD', 'B', 'st']\n","23 ['DE', 'st', ' Way', ' GA', 'EMA']\n","24 ['-', '/', ',', '.', ' Just']\n","25 [' and', 'th', ' form', ' most', ',']\n","26 [' and', '-', ' recogn', '/', ' still']\n","27 [' to', ' recogn', 'al', ',', 'to']\n","28 [' &', ',', '-', 'is', ' which']\n","29 ['les', 'al', \"'s\", ' &', ' Ten']\n","30 ['-', ' or', ' and', ' to', '/']\n","31 ['-', '.', 'B', 'Image', '+']\n","32 ['-', ' un', '/', 'MA', '+']\n","33 ['-', ',', ' or', ' un', ' but']\n","34 ['-', ',', ' to', ' also', '/']\n","35 [' and', '-', ' or', ',', ' to']\n","36 ['-', ' to', ',', ' un', ' backs']\n","37 [' to', '-', ',', 'al', 'Ed']\n","38 [',', ' to', '.', ' S', ' un']\n","39 [' to', ' ill', ' un', '.', ' also']\n","40 [' and', ' or', ' to', ',', \"'s\"]\n","41 ['+', '%', ' to', ',', '-']\n","42 [' un', ' kind', '-', ' lead', ' back']\n","43 ['-', ' to', ' un', 'U', ' U']\n","44 [' to', '-', 'U', 'X', ' U']\n","45 ['3', '2', '-', ',', '1']\n","46 ['+', '-', ' of', '*', '3']\n","47 ['+', '/', ' of', '\\xa0', '%']\n","48 [',', '/', ' to', '-', '3']\n","49 [' to', ' (', ',', 'ing', 'atic']\n","50 [' and', ' or', ' to', ',', '.']\n","51 ['+', '/', '-', ' ab', '%']\n","52 ['-', ' ab', '/', ' and', '+']\n","53 [' ab', ' un', ' se', ' or', \"'s\"]\n","54 [' S', '-', '/', ' (', ' s']\n","55 [' and', ' or', ',', ' to', ' s']\n","56 [' un', ' s', ',', ' positions', ' to']\n","57 ['%', ' percent', 'ust', ' to', '%%']\n","58 [' S', ' se', ' un', '-', ' UN']\n","59 [' percent', '%', '+', \"'s\", '-']\n","60 ['-', ' to', ',', ' or', '%']\n","61 ['%', \"'s\", '+', '-', ' percent']\n","62 ['-', \"'s\", '%', '+', ' bas']\n","63 ['-', \"'s\", ' heads', ' S', '/']\n","64 ['-', ' S', '/', ' un', ' T']\n","65 [',', '-', ' or', ' to', 'ater']\n","66 ['-', '/', ' heads', '%', '+']\n","67 ['-', '/', '%', '+', ' 100']\n","68 ['-', '/', '+', ' to', ' or']\n","69 [' 20', ' 25', ' 50', '+', ' 47']\n","70 ['%', ' or', ' to', '+', \"'s\"]\n","71 ['+', '/', ' un', ' /', '-']\n","72 ['/', '-', ',', '+', ' or']\n","73 [' at', ' Or', ',', ' U', '/']\n","74 [' at', ' S', '/', ',', '-']\n","75 [' and', ',', \"'s\", ' or', '/']\n","76 ['ing', ',', ' heads', '/', ' on']\n","77 ['%', '+', 'al', '-', 'ad']\n","78 [' or', ' val', ' S', '+', ' vs']\n","79 ['ís', ' val', ' 50', ' 20', ' S']\n","80 ['%', ',', ' %', \"'s\", ' share']\n","81 ['/', ',', '+', '%', ']']\n","82 ['+', '/', ' un', ' bas', ' all']\n","83 [']', ' or', ' ad', '+', ',']\n","84 ['/', ',', ' 8', ' E', ' over']\n","85 [',', ' over', ' 2', ' 192', ' and']\n","86 ['/', '+', ',', '-', ' or']\n","87 ['+', '/', '%', '-', ' appreci']\n","88 ['-', '+', ',', 'y', '.']\n","89 ['/', '+', ' L', ' Fe', ' S']\n","90 ['%', '-', ' %', ',', '/']\n","91 ['-', '/', '.', '+', ' Bi']\n","92 ['-', ' in', '/', ' on', ',']\n","93 ['fe', '-', ' or', ' for', ' in']\n","94 [' E', '-', ' S', '/', 'b']\n","95 ['/', '-', '.', \"'s\", '+']\n","96 ['/', '-', '+', '.', ',']\n","97 [' 100', 'half', ' 50', ' 80', 'ind']\n","98 [' 100', '-', ' 200', ' 2', '/']\n","99 ['%', ',', ' 50', ' and', \"'s\"]\n","100 [' and', ',', '.', ' or', ' &']\n","101 [' un', ' take', ' hold', ' key', ' 100']\n","102 ['-', ' un', ' all', ' or', ',']\n","103 ['-', ' or', ',', ' un', ' in']\n"]}]},{"cell_type":"code","source":["after_mlp1 = mlp0(embeddings)\n","after_mlp9 = mlp9(after_mlp1)\n","logits = unembed(ln_final(after_mlp9)).squeeze()\n","\n","for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"turwoX8fKLG3","executionInfo":{"status":"ok","timestamp":1702252568965,"user_tz":300,"elapsed":1722,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f31ed23a-4522-49d1-bafa-9fd4d31c2f8f"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["4 ['th', ' days', 'g', ' of', 'ml']\n","5 ['cc', 'ici', 'ts', 'ml', 'earch']\n","6 [' coh', ' ms', ' feet', ' Hearts', ' AA']\n","7 [' coh', ' county', ' sqor', ' Investigator', 'da']\n","8 [' envelope', ' district', ' Borough', ' card', ' administrative']\n","9 [' Miner', 'id', ' magistrate', ' number', ' No']\n","10 [' coh', ' confir', 'ici', ' mm', ' nm']\n","11 ['th', ' AA', ' coh', ' day', 'd']\n","12 ['theless', 'th', 'ths', 'ui', ' gauge']\n","13 ['antz', 'th', ' coh', 'agara', 'ufact']\n","14 ['theless', 'th', 'bda', 'tm', ' coh']\n","15 ['thur', 'esville', ' elim', 'bek', 'bda']\n","16 ['abc', 'cm', 'db', 'bek', 'antz']\n","17 ['th', 'bek', 'db', 'd', 'px']\n","18 ['th', '000', 'mn', 'db', 'ths']\n","19 ['th', 'dn', 'db', 'nm', 'mn']\n","20 ['ths', 'thur', '%', 'th', 'mm']\n","21 ['ala', ' level', ' complex', ' very', '018']\n","22 ['th', 'b', 'ted', '45', 'D']\n","23 ['dp', '45', '00', 'bp', 'DS']\n","24 [' we', 'chy', 'ck', \"'s\", 'th']\n","25 ['cc', 'db', 'mm', 'hm', '000000']\n","26 ['abc', 'dn', '000', 'antz', 'a']\n","27 ['tnc', 'dn', ' obser', 'abc', '11']\n","28 ['th', '286', '000', '349', '455']\n","29 ['ds', 'ted', ' state', 'th', '454']\n","30 ['dn', 'acl', 'thur', 'ds', 'enf']\n","31 ['amaz', 'abc', ' tremend', 'db', 'dn']\n","32 ['chy', 'fs', 'cd', 'abc', 'BP']\n","33 ['37', '11', '27', '29', '279']\n","34 ['th', 'theless', ' intuitive', ' dilig', '29']\n","35 [' intuitive', 'thur', 'cc', 'cffff', 'ACP']\n","36 ['chy', '2', '1', '001', '002']\n","37 ['01', 'ACP', '03', 'th', '27']\n","38 [' dilig', 'ymm', '1', ' mosqu', 'ieth']\n","39 ['kef', ' metic', ' dilig', ' Wem', 'aimon']\n","40 ['ths', 'cffff', 'th', ' intuitive', 'ggles']\n","41 [' tremend', 'a', 'd', 'ald', 'th']\n","42 ['th', '01', '00', 'd', ' second']\n","43 ['00', '11', 'd', '01', 'ew']\n","44 ['00', 'd', 'th', 'theless', '01']\n","45 ['chy', 'id', '1', '2', '00']\n","46 ['chy', ' tremend', '00', 'id', 'ils']\n","47 ['id', 'eus', 'chy', ' intuitive', ' tremend']\n","48 ['id', 'chy', ' intuitive', ' dismant', ' Mond']\n","49 ['id', 'ers', 'ing', 'inas', 'esses']\n","50 ['jad', 'ッド', 'esc', 'ュ', 'ws']\n","51 ['ted', 'd', 'h', 'ently', 'hid']\n","52 ['ted', 'ties', 'th', 'd', '¢']\n","53 ['d', 'b', 'ff', 'dp', 'hist']\n","54 ['th', 'd', 'ties', 'ted', 'ct']\n","55 ['ct', 'cc', 'ths', 'd', 'th']\n","56 ['th', 'd', '00', 'ted', 'ide']\n","57 ['d', 'th', 'ted', 'b', 'g']\n","58 ['d', 'th', 'ted', 'ds', 'ths']\n","59 ['d', 'th', 'ds', 'da', 'b']\n","60 ['ths', '#$', 'cffff', 'asive', ' of']\n","61 ['d', 'cham', ' must', 'edient', ' tremend']\n","62 ['cham', 'nd', 'd', 'th', 'ew']\n","63 ['ff', 'd', ' coh', 'rd', 'ft']\n","64 ['agy', 'il', 'es', ' coh', 'chy']\n","65 ['mm', 'cc', 'ufact', 'cffff', '%']\n","66 [' tremend', 'ry', ' coh', 'ew', 'mm']\n","67 ['mm', '-', ' tremend', 'th', 'g']\n","68 ['th', ' coh', 'mm', '%', '-']\n","69 ['th', '-', '_', 'dd', ' very']\n","70 ['%', '-', 'acious', 'ft', 'cc']\n","71 ['-', 'd', 'ale', ' must', ' added']\n","72 [' and', ' figure', 'th', '[', '-']\n","73 ['th', 'mm', 'd', 'rd', '-']\n","74 ['th', 'ally', 'ted', '[', '-']\n","75 ['th', 'mb', '%', 'mm', 'ted']\n","76 ['1', '001', '25', '23', '000']\n","77 ['ted', 'th', 'd', 'ale', '01']\n","78 ['th', 'd', '22', '21', 'ted']\n","79 ['th', 'byter', 'ted', 'ally', 'Sel']\n","80 ['th', 'dn', 'ir', ' coh', 'schild']\n","81 ['ted', ' also', ' fast', ' tremend', ' immediately']\n","82 ['th', 'd', '%', 's', ' best']\n","83 ['th', '704', 'd', '68', '49']\n","84 ['th', '26', '25', '274', '21']\n","85 ['%', ' me', 'd', 'th', 'ae']\n","86 ['ry', ' first', '17', '1', 'id']\n","87 [' me', '89', 'th', 'id', 'd']\n","88 ['d', 'th', '22', '89', ' me']\n","89 ['d', 'th', '274', 'ted', 'id']\n","90 ['ents', 'fast', 'some', 'anes', 'fa']\n","91 ['bt', 'k', 'b', 'n', 'na']\n","92 ['th', 'd', '17', 'fa', 'nd']\n","93 ['d', 'th', 'rd', 'b', '274']\n","94 ['th', 'd', 'b', 'a', 'nd']\n","95 ['a', 'th', 'b', 'k', 'bd']\n","96 [' of', '29', '17', 'ations', 'b']\n","97 ['26', 'rd', '29', 'th', '22']\n","98 ['26', '25', '12', '11', '13']\n","99 ['cr', 'bis', 'rc', 'cest', 'r']\n","100 ['k', 'ws', 'ian', '%', 'atl']\n","101 ['k', '1', ' head', '0', '5']\n","102 ['b', 'd', 'nd', 'cells', 'th']\n","103 ['rd', 'b', 'd', 'th', '-']\n"]}]},{"cell_type":"code","source":["layer = 9\n","head = 1\n","\n","# Get W_OV matrix\n","W_OV = model.W_V[layer, head] @ model.W_O[layer, head]\n","\n","# Get residual stream after applying W_OV or -W_OV respectively\n","# (note, because of bias b_U, it matters that we do sign flip here, not later)\n","resid_after_OV_pos = resid_after_mlp1 @ W_OV\n","resid_after_OV_neg = resid_after_mlp1 @ -W_OV\n","\n","# Get logits from value of residual stream\n","logits_pos: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_OV_pos)).squeeze()\n","logits_neg: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_OV_neg)).squeeze()"],"metadata":{"id":"Yf_iuIRVG2VJ","executionInfo":{"status":"ok","timestamp":1702252630003,"user_tz":300,"elapsed":3632,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits_pos[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D7BQyV9OKeTV","executionInfo":{"status":"ok","timestamp":1702252645614,"user_tz":300,"elapsed":1470,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cba61bbc-1742-4ff2-d401-cdffbc9c5fd4"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["4 [' 5', ' 4', '5', ' 6', ' five']\n","5 [' 6', '6', ' 5', ' 7', ' six']\n","6 [' 7', ' 6', '7', ' 8', ' seven']\n","7 [' 8', ' 7', '8', ' 9', ' 808']\n","8 [' 9', '9', ' 8', ' 10', ' nine']\n","9 [' 10', '10', ' 9', ' 11', ' 12']\n","10 [' 11', ' 12', ' eleven', ' 10', '11']\n","11 [' 12', ' 13', '12', ' 14', ' 11']\n","12 [' 13', ' 14', '13', ' 12', ' thirteen']\n","13 [' 14', '14', ' 13', ' 15', ' 16']\n","14 [' 15', ' 16', ' 14', ' 18', ' 17']\n","15 [' 16', ' 17', ' 18', ' 15', '16']\n","16 [' 17', ' 18', '17', '18', ' 19']\n","17 [' 18', '18', ' 19', ' 17', ' 1889']\n","18 [' 19', ' 18', ' 1870', '19', ' 20']\n","19 [' 21', ' 20', ' 22', ' 19', ' 23']\n","20 [' 21', ' 22', ' 20', '21', ' 25']\n","21 [' 22', ' 23', ' 21', '22', ' 53']\n","22 [' 23', ' 22', ' 24', '23', ' 29']\n","23 [' 24', ' 23', ' 25', '24', '23']\n","24 [' 25', ' 26', ' 24', '25', ' 27']\n","25 [' 26', ' 25', ' 27', ' 30', ' 35']\n","26 [' 27', ' 28', ' 29', ' 37', ' 31']\n","27 [' 28', ' 29', ' 34', ' 27', ' 33']\n","28 [' 29', ' 30', ' 34', ' 39', ' 33']\n","29 [' 31', ' 30', ' 29', ' 34', ' 39']\n","30 [' 31', ' 40', ' 33', ' 30', ' 35']\n","31 [' 33', ' 32', ' 34', ' 37', ' 31']\n","32 [' 33', ' 34', ' 37', ' 43', ' 35']\n","33 [' 34', ' 35', ' 33', ' 37', ' 38']\n","34 [' 35', ' 36', ' 37', ' 34', ' 39']\n","35 [' 36', ' 37', ' 38', ' 40', ' 35']\n","36 [' 37', ' 38', ' 39', ' 36', ' 47']\n","37 [' 38', ' 39', ' 37', ' 47', ' 43']\n","38 [' 39', ' 40', ' 38', ' 41', ' 43']\n","39 [' 41', ' 40', ' 42', ' 39', ' 44']\n","40 [' 41', ' 50', ' 42', ' 40', ' 43']\n","41 [' 42', ' 43', ' 44', ' 52', ' 47']\n","42 [' 43', ' 44', ' 46', ' 53', ' 42']\n","43 [' 44', ' 46', ' 43', ' 45', ' 54']\n","44 [' 46', ' 45', ' 44', ' 47', ' 48']\n","45 [' 46', ' 50', ' 47', ' 48', ' 51']\n","46 [' 47', ' 48', ' 52', ' 46', ' 49']\n","47 [' 48', ' 49', ' 47', ' 53', ' 54']\n","48 [' 49', ' 51', ' 48', ' 50', ' 53']\n","49 [' 51', ' 50', ' 54', ' 52', ' 49']\n","50 [' 51', ' 50', ' 52', ' 60', ' 100']\n","51 [' 52', ' 53', ' 51', ' 62', ' 54']\n","52 [' 53', ' 54', ' 52', ' 63', ' 55']\n","53 [' 54', ' 55', ' 53', ' 56', ' 64']\n","54 [' 55', ' 56', ' 57', ' 54', '55']\n","55 [' 56', ' 57', ' 66', ' 61', ' 58']\n","56 [' 57', ' 58', ' 59', ' 61', ' 62']\n","57 [' 58', ' 59', ' 61', ' 62', ' 57']\n","58 [' 59', ' 61', ' 60', ' 62', ' 58']\n","59 [' 61', ' 60', ' 62', ' 59', ' 65']\n","60 [' 61', ' 62', ' 70', ' 71', ' 66']\n","61 [' 62', ' 63', ' 61', ' 67', ' 66']\n","62 [' 63', ' 62', ' 64', '63', ' 65']\n","63 [' 64', ' 65', ' 66', ' 63', ' 74']\n","64 [' 66', ' 65', ' 67', '65', ' 69']\n","65 [' 66', ' 67', ' 71', ' 70', ' 68']\n","66 [' 67', ' 68', ' 69', ' 71', ' 66']\n","67 [' 68', ' 69', ' 71', ' 67', ' 70']\n","68 [' 69', ' 71', ' 70', ' 68', ' 73']\n","69 [' 71', ' 70', ' 72', ' 69', ' 74']\n","70 [' 71', ' 72', ' 70', ' 80', ' 73']\n","71 [' 72', ' 73', ' 71', ' 74', ' 76']\n","72 [' 73', ' 72', ' 74', ' 77', ' 76']\n","73 [' 74', ' 75', ' 78', ' 73', ' 76']\n","74 [' 75', ' 76', ' 74', ' 77', ' 78']\n","75 [' 75', ' 76', ' 77', ' 80', ' 78']\n","76 [' 77', ' 78', ' 79', ' 82', ' 76']\n","77 [' 78', ' 79', ' 77', ' 80', ' 82']\n","78 [' 79', ' 80', ' 81', ' 78', ' 84']\n","79 [' 80', ' 81', ' 79', ' 82', ' 85']\n","80 [' 81', ' 80', ' 90', ' 82', ' 85']\n","81 [' 82', ' 83', ' 92', ' 81', ' 84']\n","82 [' 83', ' 84', ' 87', ' 85', ' 86']\n","83 [' 84', ' 85', ' 86', ' 88', ' 89']\n","84 [' 86', ' 85', ' 87', ' 89', ' 88']\n","85 [' 86', ' 87', ' 91', ' 85', ' 88']\n","86 [' 87', ' 88', ' 92', ' 89', ' 91']\n","87 [' 88', ' 89', ' 92', ' 93', ' 91']\n","88 [' 89', ' 91', ' 90', ' 94', ' 93']\n","89 [' 91', ' 94', ' 90', ' 92', ' 95']\n","90 [' 91', ' 92', ' 90', ' 100', ' 95']\n","91 [' 92', ' 93', ' 97', ' 94', ' 91']\n","92 [' 93', ' 94', ' 92', ' 95', ' 97']\n","93 [' 94', ' 95', ' 93', ' 96', ' 98']\n","94 [' 95', ' 96', ' 97', ' 94', ' 98']\n","95 [' 97', ' 96', ' 95', ' 98', ' 99']\n","96 [' 97', ' 98', ' 96', ' 99', ' 107']\n","97 [' 98', ' 99', ' 97', ' 103', ' 102']\n","98 [' 99', ' 100', ' 98', ' 102', ' 103']\n","99 [' 99', ' 100', ' 102', ' 101', '100']\n","100 [' 100', ' 101', ' 102', 'Loading', ' 110']\n","101 [' 102', '102', ' 103', ' 101', ' 112']\n","102 [' 103', ' 104', ' 102', '103', '104']\n","103 [' 104', '104', ' 103', ' 105', '105']\n"]}]},{"cell_type":"code","source":["for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits_neg[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEaFHST9KowF","executionInfo":{"status":"ok","timestamp":1702252683714,"user_tz":300,"elapsed":221,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"48ec2859-81c9-4b64-b2f6-0f5ea77fff42"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["4 [' both', 'amo', ' latter', ' second', ' Childhood']\n","5 [' both', 'unda', 'both', 'eral', ' antiv']\n","6 [' truly', ' Truly', ' really', ' Really', ' genuinely']\n","7 [' partly', ' secondly', ' truly', ' wholly', ' genuinely']\n","8 [' Clever', ' given', 'asse', ' Kerr', ' Shields']\n","9 [' latter', 'sworth', ' either', ' now', ' Either']\n","10 [' Either', ' needed', ' Both', 'helm', ' somehow']\n","11 [' second', '360', 'ilated', 'gyn', ' possible']\n","12 ['ills', ' though', ' ado', 'now', 'let']\n","13 ['now', ' therefore', 'omin', 'gging', ' secondly']\n","14 [' disreg', ' effortlessly', 'ered', ' unres', 'ering']\n","15 [' instead', ' needed', ' redistributed', 'anwhile', 'dayName']\n","16 [' demanded', ' instead', 'acs', ' urgently', ' required']\n","17 [' instead', ' Sind', ' required', ' demanded', 'instead']\n","18 [' somehow', ' though', ' somew', ' finally', ' agrees']\n","19 [' says', ' finally', ' insepar', ' say', 'bish']\n","20 [' says', ' warranted', './', ' unavoid', ' ultimately']\n","21 [' changed', ' continued', ' heartbeat', ' altered', ' intermediate']\n","22 ['ivo', 'nai', ' therefore', ' unable', 'most']\n","23 ['oj', 'emia', 'ordan', ' II', 'olson']\n","24 [' stuff', ' most', ' guys', ' unfortunately', ' gone']\n","25 [' today', ' particular', 'alde', ' needed', ' too']\n","26 [' however', ' presently', ' hereby', ' too', ' somew']\n","27 ['EngineDebug', 'Interstitial', 'cause', ' unch', ' intensified']\n","28 [' somehow', 'anything', ' though', ' anyone', ' especially']\n","29 [' even', 'even', 'SPONSORED', 'unks', ' except']\n","30 ['initely', ' retard', ' bru', 'gan', ' since']\n","31 ['ケ', ' wanted', 'asket', 'anwhile', ' retard']\n","32 ['ケ', ' Schro', 'イト', ' Braun', '�']\n","33 ['ヴ', '��', '�醒', 'ワン', 'sen']\n","34 ['�', 'urities', 'quished', ' stru', ' people']\n","35 [' �', 'urities', ' attempts', ' Kara', 'aden']\n","36 [' laun', 'urity', 'lee', 'iage', 'ond']\n","37 [' began', 'ond', ' begins', ' autos', ' became']\n","38 [' given', ' finally', ' autos', ' if', ' Finally']\n","39 ['enson', ' ultimately', ' autos', 'bec', ' order']\n","40 ['auer', ' respective', ' accordingly', ' 1929', ' never']\n","41 ['bec', ' streng', 'adal', ' various', 'ulative']\n","42 [' accordingly', ' however', ' various', 'ivas', 'enegger']\n","43 ['upon', ' both', ' endless', 'velt', 'nai']\n","44 [' however', 'velt', 'arted', ' though', ' amount']\n","45 [' anth', ' hither', 'IRT', ' also', ' sorts']\n","46 ['arted', 'urity', ' anth', 'art', 'fact']\n","47 [' began', ' rapidly', 'ilo', ' inquired', ' continued']\n","48 [' lately', '覚醒', 'urity', ' foremost', 'IRT']\n","49 ['BAT', 'enson', 'ilo', ' additional', ' Bat']\n","50 ['paralle', ' Stard', 'mentioned', '�', ' estab']\n","51 [' ramp', 'ilo', 'age', 'uala', ' began']\n","52 ['buff', 'itive', 'herty', 'inder', ' Dealer']\n","53 ['landish', 'edient', ' repeated', ' advanced', 'illion']\n","54 [' anytime', ' things', ' all', 'ּ', ' amount']\n","55 [' all', ' bru', 'landish', ' anytime', ' multiple']\n","56 [' laun', 'atha', 'bara', ' bru', ' amount']\n","57 [' amount', 'bre', 'atha', ' began', ' started']\n","58 [' amount', 'arna', 'vous', 'buff', ' amounts']\n","59 [' amount', ' yet', ' even', 'SPONSORED', 'paralle']\n","60 ['rich', 'sharp', ' respective', 'ary', 'separ']\n","61 [' yet', 'arted', ' potential', ' alleged', ' confir']\n","62 [' wherein', ' alleged', 'light', 'ills', 'begin']\n","63 ['jri', '��', 'ة', ' cr', 'itt']\n","64 [' Clayton', 'ish', 'イ', 'tip', 'owsky']\n","65 [' right', 'jri', 'ysc', 'atic', ' stressed']\n","66 [' Tart', ' sufficient', ' dotted', ' enough', 'arted']\n","67 [' todd', 'ortium', 'inki', ' dotted', ' enough']\n","68 ['dash', 'enough', 'again', 'itten', ' suffice']\n","69 ['ilies', ' Yi', 'fact', ' yet', 'uala']\n","70 [' firmly', 'landish', 'fact', 'inki', ' plainly']\n","71 ['fact', 'landish', 'int', 'esty', ' todd']\n","72 ['ueller', 'INT', ' before', 'learn', 'isen']\n","73 ['landish', 'ueless', 'yond', 'isen', 'enthal']\n","74 [' things', 'itional', 'things', 'ament', ' whether']\n","75 ['things', ' REG', ' --------------------', 'atron', ' Posted']\n","76 [' presently', 'lyak', 'atan', ' yet', ' known']\n","77 ['ph', 'leys', 'gress', 'berg', 'any']\n","78 [' finally', 'dash', 'anything', 'REDACTED', 'llah']\n","79 [' yet', ' total', 'REDACTED', ' continued', 'ings']\n","80 [' Clever', 'hedon', 'sm', 'REDACTED', ' reportedly']\n","81 [' Clever', ' possible', 'elligent', 'stad', ' perm']\n","82 [' given', ' certain', 'hedon', 'ascript', ' yes']\n","83 ['ueless', 'elligent', ' certain', 'ي', ' LIM']\n","84 [' certain', ' specified', ' Clever', 'things', 'REDACTED']\n","85 [' certain', ' clar', ' various', ' targ', ' Sheila']\n","86 [' unspecified', ' unnamed', ' further', ' fourth', ' theor']\n","87 [' second', ' unspecified', ' increasingly', ' selected', 'ios']\n","88 [' third', ' Dynamics', ' second', ' unspecified', 'how']\n","89 [' third', ' Finally', ' Dynamics', ' unspecified', ' HOW']\n","90 ['anwhile', ' meantime', ' Meanwhile', ' Might', 'esson']\n","91 [' alleged', ' unnamed', ' allegedly', 'NetMessage', ' unknown']\n","92 [' alleged', 'ivas', ' too', 'inder', 'nai']\n","93 [' unnecessary', ' alleged', 'lessly', 'iotic', ' too']\n","94 [' things', 'ayer', 'things', 'VEL', ' Things']\n","95 ['NetMessage', 'anwhile', 'aging', ' allegedly', ' GOODMAN']\n","96 [' meantime', ' Manufact', ' Peb', ' upcoming', ' Arkham']\n","97 [' Peb', ' CARD', 'NetMessage', 'anwhile', ' Arkham']\n","98 [' enough', 'enough', 'elight', ' things', ' Peb']\n","99 ['ages', 'anwhile', ' allegedly', ' claimed', ' alleged']\n","100 [' somehow', '7', '8', ' skirts', ' too']\n","101 [' incorrectly', ' somehow', 'Eventually', '*/(', ' Eventually']\n","102 [' somehow', 'once', 'ivia', ' once', ' too']\n","103 ['グ', 'once', ' even', 'imi', ' incorrectly']\n"]}]},{"cell_type":"code","source":["layer = 4\n","head = 4\n","\n","# Get W_OV matrix\n","W_OV = model.W_V[layer, head] @ model.W_O[layer, head]\n","\n","# Get residual stream after applying W_OV or -W_OV respectively\n","# (note, because of bias b_U, it matters that we do sign flip here, not later)\n","resid_after_OV_pos = resid_after_mlp1 @ W_OV\n","resid_after_OV_neg = resid_after_mlp1 @ -W_OV\n","\n","# Get logits from value of residual stream\n","logits_pos: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_OV_pos)).squeeze()\n","# logits_neg: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_OV_neg)).squeeze()\n","for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits_pos[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVvsttAqKvbU","executionInfo":{"status":"ok","timestamp":1702252764239,"user_tz":300,"elapsed":2133,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ba1998a1-b720-4e08-afe9-aa94369527b8"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["4 ['ogens', ' indo', 'transfer', 'wind', 'rich']\n","5 ['mone', 'ogens', ' Leban', 'embed', ' Gutenberg']\n","6 ['raine', ' needs', ' indo', 'embed', ' inher']\n","7 [' reinvest', 'raine', 'uph', 'oint', ' Constantin']\n","8 [' indo', ' spons', ' inher', ' reinvest', ' embell']\n","9 ['plates', 'ulet', ' embell', ' Helm', 'embed']\n","10 [' Leban', ' fingerprints', ' intrig', 'メ', ' Gutenberg']\n","11 [' indo', ' Leban', ' chances', ' prest', 'onz']\n","12 ['ール', ' indo', ' Leban', ' privatization', ' fundraising']\n","13 [' Leban', ' reuse', 'ール', ' prest', ' reusable']\n","14 ['itiz', 'ogens', ' vetting', 'FY', 'henko']\n","15 [' Leban', ' confir', 'ogens', 'itiz', 'vernment']\n","16 [' indo', 'ogens', ' spons', 'unity', ' urgently']\n","17 [' indo', ' spons', 'ogens', ' vetting', ' sten']\n","18 [' spons', 'ogens', ' indo', 'ebin', 'itiz']\n","19 [' Allaah', 'oyal', 'plet', 'wind', 'uph']\n","20 ['ebin', ' Allaah', 'mone', 'kay', ' guiActiveUn']\n","21 [' prints', 'prints', 'ettel', ' automate', 'ebin']\n","22 ['ール', 'apons', ' indo', 'soDeliveryDate', ' Allaah']\n","23 ['ール', ' Allaah', 'wind', 'wake', ' improve']\n","24 ['soDeliveryDate', ' indo', 'ゴン', ' Herz', 'ebin']\n","25 ['soDeliveryDate', 'ogens', 'ebin', ' Allaah', 'FY']\n","26 ['soDeliveryDate', ' indo', 'raine', ' needs', 'ogens']\n","27 [' indo', 'raine', ' Allaah', '────', ' TOD']\n","28 [' indo', ' TOD', 'ogens', ' needs', ' Allaah']\n","29 [' TOD', ' needs', 'wake', ' privatization', 'ettel']\n","30 ['issues', 'soDeliveryDate', 'ebin', ' intrig', 'mone']\n","31 [' chances', ' morale', ' costs', ' odds', ' quest']\n","32 [' proble', ' indo', 'soDeliveryDate', ' quest', ' privatization']\n","33 [' privatization', ' indo', ' quest', ' quiz', ' fundra']\n","34 ['ebin', ' fundraising', ' fundra', ' privatization', ' indo']\n","35 [' Allaah', 'soDeliveryDate', 'issues', ' Gutenberg', ' guiActiveUn']\n","36 [' indo', ' privatization', 'raine', ' ank', ' Gutenberg']\n","37 ['raine', ' indo', ' Reset', 'aire', ' Allaah']\n","38 [' indo', 'ogens', 'raine', ' relocation', ' Allaah']\n","39 ['raine', 'elin', ' Skype', 'ogens', 'imer']\n","40 ['ebin', 'ogens', 'mone', ' quest', ' Allaah']\n","41 [' indo', 'ogens', 'imer', 'raine', 'ebin']\n","42 ['ogens', ' indo', 'ール', 'inki', ' fundraising']\n","43 ['ogens', ' indo', ' reuse', ' Allaah', ' shorten']\n","44 ['ogens', ' indo', ' Allaah', 'raine', ' relocation']\n","45 ['ogens', ' indo', ' Allaah', ' Gutenberg', ' privatization']\n","46 [' indo', 'ogens', 'raine', 'inki', 'imer']\n","47 [' indo', 'ogens', ' replen', ' shorten', ' rewrite']\n","48 [' indo', 'ogens', ' Herz', ' Azerb', ' Gutenberg']\n","49 ['ogens', ' indo', 'inki', 'elin', 'urgy']\n","50 ['ogens', 'ebin', ' Gutenberg', ' Reborn', ' intrig']\n","51 [' indo', 'ogens', ' Gutenberg', ' travels', ' Zap']\n","52 ['ogens', ' indo', 'ール', 'iasis', ' expands']\n","53 ['ogens', ' indo', 'wake', ' flashbacks', ' jealous']\n","54 ['ogens', ' indo', ' Allaah', 'iasis', 'raine']\n","55 ['ogens', ' Allaah', 'raine', 'ilater', ' Gutenberg']\n","56 [' indo', 'ogens', 'inki', ' Allaah', ' quest']\n","57 [' indo', 'elin', 'ogens', 'raine', 'anamo']\n","58 ['ogens', ' indo', ' plans', ' needs', 'elin']\n","59 ['elin', 'inki', 'raine', 'iasis', ' Allaah']\n","60 ['ebin', 'SpaceEngineers', 'ogens', 'ilater', 'iasis']\n","61 [' indo', 'SpaceEngineers', 'iasis', ' quest', ' proble']\n","62 ['iasis', 'ogens', ' fundraising', ' proble', ' fundra']\n","63 [' Allaah', 'iasis', ' reuse', ' quest', 'ogens']\n","64 [' redes', ' proble', 'iasis', 'ogens', ' Allaah']\n","65 [' redes', 'ogens', 'dial', 'ebin', ' Allaah']\n","66 ['ogens', ' redes', ' fundraising', ' dialogue', ' quest']\n","67 [' Allaah', 'ogens', ' quest', ' dialogue', 'raine']\n","68 ['ogens', ' Allaah', 'raine', ' fundraiser', ' fundraising']\n","69 ['raine', 'dial', ' Allaah', 'ogens', 'elin']\n","70 ['mail', 'prints', ' quotation', ' Allaah', 'quote']\n","71 ['dial', 'raine', 'imer', 'ogens', 'translation']\n","72 [' Allaah', ' Herz', 'issues', 'Rew', 'catentry']\n","73 [' Allaah', 'elin', 'raine', 'prints', ' wik']\n","74 ['issues', 'raine', ' Allaah', '────', 'sheets']\n","75 [' LLC', 'prints', ' initials', 'issues', 'furt']\n","76 ['prints', 'oros', 'ogens', 'raine', ' Gutenberg']\n","77 ['elin', 'prints', ' sten', ' paperwork', 'raine']\n","78 [' Allaah', 'elin', 'ogens', 'oros', ' paperwork']\n","79 [' paperwork', 'elin', 'raine', ' ank', 'sheets']\n","80 ['ebin', ' ransom', ' quotation', ' ank', ' report']\n","81 [' paperwork', 'ebin', ' ank', ' ink', 'rone']\n","82 ['ebin', ' ank', ' paperwork', 'ogens', 'ール']\n","83 [' paperwork', ' ank', ' ink', 'raine', ' sten']\n","84 [' ank', ' paperwork', 'raine', ' sten', 'ogens']\n","85 [' ank', ' paperwork', ' initials', 'named', 'raine']\n","86 [' ank', ' indo', 'raine', 'arge', 'inki']\n","87 [' ank', ' indo', 'raine', 'elin', 'arge']\n","88 [' ank', ' indo', 'inki', 'ogens', ' repatri']\n","89 ['inki', 'elin', ' ank', 'raine', 'iman']\n","90 ['mail', ' ank', 'catentry', 'prints', 'mails']\n","91 [' ank', ' indo', 'inki', 'elin', 'raine']\n","92 ['ール', ' ank', 'elin', 'rone', ' indo']\n","93 ['elin', ' ank', ' initials', 'ール', 'inki']\n","94 ['inki', ' ank', 'elin', 'ール', 'ulet']\n","95 [' initials', ' ank', 'swer', ' arrows', 'elin']\n","96 [' ank', 'inki', ' indo', 'catentry', 'ulet']\n","97 [' initials', 'elin', 'ulet', ' arrows', ' ank']\n","98 [' ank', ' initials', 'inki', ' logos', ' arrows']\n","99 [' initials', ' Herald', ' logos', ' arrows', 'inki']\n","100 [' Herz', 'edi', 'oslav', ' intrig', 'ール']\n","101 ['artifacts', ' ank', ' fingerprints', ' reuse', ' arte']\n","102 ['ール', ' indo', 'rax', ' Zeal', ' Herz']\n","103 ['ール', 'elin', ' Allaah', ' Zeal', ' Leban']\n"]}]},{"cell_type":"code","source":["layer = 9\n","head = 1\n","\n","# Get W_OV matrix\n","W_OV = model.W_V[layer, head] @ model.W_O[layer, head]\n","\n","# Get residual stream after applying W_OV or -W_OV respectively\n","# (note, because of bias b_U, it matters that we do sign flip here, not later)\n","resid_after_OV_pos = resid_after_mlp1 @ W_OV\n","# resid_after_OV_neg = resid_after_mlp1 @ -W_OV\n","\n","resid_after_mlp9 = resid_after_OV_pos + mlp9(ln9(resid_after_OV_pos))\n","\n","logits_pos: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_mlp9)).squeeze()\n","\n","for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits_pos[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaRv_9gvK2r1","executionInfo":{"status":"ok","timestamp":1702252973987,"user_tz":300,"elapsed":2740,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"189fc48d-c9c2-4285-c197-37b820102d67"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["4 [' 5', ' 4', '5', ' 6', ' five']\n","5 [' 6', '6', ' 5', ' 7', ' six']\n","6 [' 7', '7', ' 6', ' 8', ' seven']\n","7 [' 8', ' 7', '8', ' 9', ' eight']\n","8 [' 9', '9', ' 8', ' 10', ' nine']\n","9 [' 10', '10', ' 9', ' 11', ' 12']\n","10 [' 11', ' 12', ' eleven', ' 10', '11']\n","11 [' 12', ' 13', '12', ' 14', '13']\n","12 [' 13', ' 14', '13', ' 12', ' thirteen']\n","13 [' 14', '14', ' 13', ' 15', ' 16']\n","14 [' 15', ' 16', '15', ' 18', ' 14']\n","15 [' 16', ' 17', ' 18', '16', ' 15']\n","16 [' 17', ' 18', '17', '18', ' 19']\n","17 [' 18', '18', ' 19', ' 17', ' 1889']\n","18 [' 19', ' 18', '19', ' 1870', ' 20']\n","19 [' 21', ' 20', ' 22', ' 19', '20']\n","20 [' 21', ' 22', ' 20', '21', ' 25']\n","21 [' 22', ' 23', ' 21', '22', ' 52']\n","22 [' 23', ' 22', '23', ' 24', ' 29']\n","23 [' 24', ' 23', ' 25', '24', ' 26']\n","24 [' 25', ' 26', '25', ' 24', ' 27']\n","25 [' 26', ' 25', ' 27', ' 30', ' 29']\n","26 [' 27', ' 28', ' 37', ' 29', ' 31']\n","27 [' 28', ' 29', ' 34', ' 27', ' 33']\n","28 [' 29', ' 30', ' 39', ' 34', ' 33']\n","29 [' 30', ' 31', ' 29', ' 34', ' 35']\n","30 [' 31', ' 40', ' 33', ' 30', ' 35']\n","31 [' 33', ' 32', ' 34', ' 37', ' 31']\n","32 [' 33', ' 34', ' 37', ' 43', ' 35']\n","33 [' 34', ' 35', ' 33', ' 37', ' 38']\n","34 [' 35', ' 36', ' 37', ' 34', ' 55']\n","35 [' 36', ' 37', ' 38', ' 40', ' 35']\n","36 [' 37', ' 38', ' 39', ' 36', ' 47']\n","37 [' 38', ' 39', ' 37', ' 40', ' 47']\n","38 [' 39', ' 40', ' 41', ' 38', ' 43']\n","39 [' 40', ' 41', ' 42', ' 39', ' 44']\n","40 [' 41', ' 50', ' 42', ' 40', ' 43']\n","41 [' 42', ' 43', ' 44', ' 52', ' 47']\n","42 [' 43', ' 44', ' 53', ' 46', ' 47']\n","43 [' 44', ' 46', ' 45', ' 43', ' 54']\n","44 [' 45', ' 46', ' 44', ' 47', ' 48']\n","45 [' 46', ' 50', ' 47', ' 48', ' 51']\n","46 [' 47', ' 48', ' 52', ' 49', ' 53']\n","47 [' 48', ' 49', ' 47', ' 53', ' 50']\n","48 [' 49', ' 51', ' 50', ' 53', ' 48']\n","49 [' 51', ' 50', ' 54', ' 52', ' 53']\n","50 [' 51', ' 50', ' 52', ' 60', ' 100']\n","51 [' 52', ' 53', ' 51', ' 62', ' 54']\n","52 [' 53', ' 54', ' 52', ' 63', ' 55']\n","53 [' 54', ' 55', ' 56', ' 53', ' 64']\n","54 [' 55', ' 56', ' 57', '55', ' 54']\n","55 [' 56', ' 57', ' 66', ' 61', ' 58']\n","56 [' 57', ' 58', ' 59', ' 61', ' 67']\n","57 [' 58', ' 59', ' 61', ' 62', ' 63']\n","58 [' 59', ' 61', ' 60', ' 63', ' 62']\n","59 [' 61', ' 60', ' 62', ' 65', ' 59']\n","60 [' 61', ' 62', ' 70', ' 71', ' 66']\n","61 [' 62', ' 63', ' 61', ' 66', ' 67']\n","62 [' 63', ' 64', ' 62', '63', ' 65']\n","63 [' 64', ' 65', ' 66', ' 63', ' 74']\n","64 [' 66', ' 65', ' 67', '65', ' 69']\n","65 [' 66', ' 67', ' 71', ' 70', ' 68']\n","66 [' 67', ' 68', ' 69', ' 71', ' 66']\n","67 [' 68', ' 69', ' 71', ' 67', ' 70']\n","68 [' 69', ' 71', ' 70', ' 73', ' 68']\n","69 [' 71', ' 70', ' 72', ' 74', ' 69']\n","70 [' 71', ' 72', ' 80', ' 70', ' 73']\n","71 [' 72', ' 73', ' 71', ' 74', ' 76']\n","72 [' 73', ' 74', ' 72', ' 77', ' 76']\n","73 [' 74', ' 75', ' 78', ' 76', ' 73']\n","74 [' 75', ' 76', ' 74', ' 77', ' 78']\n","75 [' 76', ' 75', ' 77', ' 80', ' 78']\n","76 [' 77', ' 78', ' 79', ' 82', ' 81']\n","77 [' 78', ' 79', ' 80', ' 77', ' 82']\n","78 [' 79', ' 80', ' 81', ' 78', ' 84']\n","79 [' 80', ' 81', ' 79', ' 82', ' 85']\n","80 [' 81', ' 90', ' 80', ' 82', ' 85']\n","81 [' 82', ' 83', ' 92', ' 84', ' 81']\n","82 [' 83', ' 84', ' 87', ' 85', ' 86']\n","83 [' 84', ' 85', ' 86', ' 88', ' 89']\n","84 [' 86', ' 85', ' 87', ' 89', '85']\n","85 [' 86', ' 87', ' 91', ' 85', ' 88']\n","86 [' 87', ' 88', ' 89', ' 92', ' 91']\n","87 [' 88', ' 89', ' 92', ' 93', ' 91']\n","88 [' 89', ' 91', ' 90', ' 93', ' 94']\n","89 [' 91', ' 90', ' 94', ' 92', ' 95']\n","90 [' 91', ' 92', ' 90', ' 100', ' 95']\n","91 [' 92', ' 93', ' 97', ' 94', ' 98']\n","92 [' 93', ' 94', ' 95', ' 92', ' 97']\n","93 [' 94', ' 95', ' 93', ' 96', ' 98']\n","94 [' 95', ' 96', ' 97', ' 94', ' 98']\n","95 [' 97', ' 96', ' 95', ' 98', ' 99']\n","96 [' 97', ' 98', ' 99', ' 96', ' 107']\n","97 [' 98', ' 99', ' 97', ' 103', ' 102']\n","98 [' 99', ' 100', ' 103', ' 98', ' 102']\n","99 [' 100', ' 99', ' 102', ' 101', '100']\n","100 [' 100', ' 101', ' 102', ' 110', 'Loading']\n","101 [' 102', '102', ' 103', ' 112', ' 101']\n","102 [' 103', '103', ' 104', ' 102', '104']\n","103 [' 104', '104', ' 103', ' 105', '105']\n"]}]},{"cell_type":"code","source":["layer = 4\n","head = 4\n","W_OV = model.W_V[layer, head] @ model.W_O[layer, head]\n","resid_after_OV_pos = resid_after_mlp1 @ W_OV\n","\n","layer = 9\n","head = 1\n","W_OV = model.W_V[layer, head] @ model.W_O[layer, head]\n","resid_after_OV_pos = resid_after_OV_pos @ W_OV\n","\n","resid_after_mlp9 = resid_after_OV_pos + mlp9(ln9(resid_after_OV_pos))\n","\n","logits_pos: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_mlp9)).squeeze()\n","\n","for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits_pos[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttvnKJvBL1Qt","executionInfo":{"status":"ok","timestamp":1702253267561,"user_tz":300,"elapsed":1687,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"09659539-c141-4601-a21a-7b40f8d94453"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["4 [' multiple', ' thousands', ' hundreds', ' millions', ' alike']\n","5 [' hundreds', ' thousands', ' multiple', ' countless', ' millions']\n","6 [' again', ' Called', ' believed', ' multiple', 'piring']\n","7 [' again', ' multiple', ' Again', 'again', ' thousands']\n","8 [' again', 'again', ' Again', ' three', 'Absolutely']\n","9 [' again', ' begun', ' hundreds', ' thousands', 'rawn']\n","10 [' hundreds', ' thousands', ' countless', ' millions', ' Thousands']\n","11 [' thousands', ' millions', ' hundreds', ' mistaken', ' two']\n","12 [' hundreds', ' millions', ' thousands', 'lished', ' enough']\n","13 [' enough', ' hundreds', ' multiple', 'lished', ' thousands']\n","14 [' needed', ' depended', ' required', ' multiple', ' believed']\n","15 [' thousands', ' millions', ' hundreds', ' needed', 'Reviewer']\n","16 [' believed', ' needed', ' meant', ' hundreds', ' enough']\n","17 [' multiple', ' required', ' thousands', ' needed', ' hundreds']\n","18 [' hundreds', ' thousands', ' beyond', ' Hundreds', ' Thousands']\n","19 [' thousands', ' hundreds', ' Thousands', ' millions', ' Hundreds']\n","20 [' Thousands', ' thousands', ' so', ' hundreds', ' millions']\n","21 [' still', ' thousands', ' then', ' hundreds', ' Zhou']\n","22 [' hundreds', ' thousands', ' unknown', ' countless', ' lots']\n","23 [' still', ' hundreds', ' thousands', ' lots', 'still']\n","24 [' unknown', ' unspecified', ' great', ' bunch', ' again']\n","25 [' hundreds', ' unknown', ' thousands', ' great', ' Thousands']\n","26 [' unknown', ' hundreds', ' great', ' bunch', 'utenberg']\n","27 [' because', ' due', 'utenberg', ' hundreds', ' great']\n","28 [' unbelievable', ' great', ' hundreds', ' tremendous', ' unknown']\n","29 [' unknown', ' unbelievable', 'utenberg', ' gotta', ' bunch']\n","30 [' hundreds', ' unbelievable', ' again', ' Hundreds', ' Tens']\n","31 [' bunch', ' great', ' gonna', ' four', ' three']\n","32 [' bunch', ' great', ' gonna', ' four', ' three']\n","33 [' great', ' hundreds', ' Will', ' Eaton', ' if']\n","34 [' great', ' if', ' Will', ' Eaton', ' gonna']\n","35 [' Will', ' again', ' great', ' Again', ' had']\n","36 [' great', ' Will', ' bunch', ' thanks', ' gonna']\n","37 [' thanks', ' great', ' three', 'URA', ' Will']\n","38 [' great', ' again', 'URA', ' bunch', ' Will']\n","39 [' Will', ' great', ' will', 'URA', 'Will']\n","40 [' great', ' again', ' thousands', ' wonderful', ' tremendous']\n","41 [' great', 'URA', ' Will', ' three', ' thanks']\n","42 [' great', ' thanks', ' many', ' going', ' thousands']\n","43 [' great', ' thanks', 'URA', ' wonderful', ' many']\n","44 [' great', ' if', ' thanks', ' due', ' many']\n","45 [' great', ' Will', ' will', ' going', '1']\n","46 [' great', 'URA', '000', ' thanks', ' will']\n","47 ['URA', ' great', ' all', ' thanks', '000']\n","48 [' great', 'URA', 'eter', ' bunch', ' all']\n","49 [' great', ' Will', ' will', 'eter', 'URA']\n","50 [' tremendous', ' great', ' wonderful', 'eter', ' thousands']\n","51 [' great', ' wonderful', ' tremendous', ' many', 'URA']\n","52 [' great', ' unbelievable', ' tremendous', ' huge', ' wonderful']\n","53 [' great', ' wonderful', ' unbelievable', ' tremendous', ' incredible']\n","54 [' great', ' tremendous', ' wonderful', ' unbelievable', ' incredible']\n","55 [' great', ' all', ' thousands', ' tremendous', ' wonderful']\n","56 [' great', ' all', ' three', '000', ' wonderful']\n","57 [' great', ' three', 'URA', '300', 'atha']\n","58 [' great', '000', '031', 'URA', ' tremendous']\n","59 [' great', 'atha', ' called', 'URA', '000']\n","60 [' again', ' Again', ' great', 'again', ' bunch']\n","61 [' great', 'URA', ' bunch', ' gonna', ' three']\n","62 [' great', ' bunch', ' gonna', ' told', ' called']\n","63 [' great', ' called', 'URA', 'atha', ' good']\n","64 [' great', 'URA', ' gonna', ' really', ' good']\n","65 [' great', ' before', ' again', ' Will', 'iety']\n","66 [' great', '031', ' good', ' told', ' called']\n","67 [' great', 'URA', '031', ' due', 'atha']\n","68 [' great', '000', '031', 'URA', ' again']\n","69 [' because', 'INAL', '000', ' due', ' great']\n","70 [' because', ' really', ' thousands', ' TOTAL', 'ertodd']\n","71 [' really', ' because', ' great', 'URA', ' due']\n","72 [' great', ' really', ' bunch', ' because', ' gonna']\n","73 [' great', ' because', ' due', 'ENTION', ' thanks']\n","74 [' great', 'ل', ' because', ' due', ' Will']\n","75 [' before', ' because', ' prior', ' certain', 'Required']\n","76 [' great', ' known', ' told', ' unknown', ' Skydragon']\n","77 ['031', ' certain', ' because', ' todd', 'URA']\n","78 ['000', '031', ' told', ' because', 'Required']\n","79 ['INAL', ' FINAL', ' final', 'Required', ' Will']\n","80 [' again', ' Again', ' great', 'again', ' tremendous']\n","81 [' great', 'URA', ' Will', 'INAL', ' Joined']\n","82 [' great', ' Skydragon', ' again', ' known', ' Tune']\n","83 [' great', 'URA', ' again', ' Will', ' thanks']\n","84 [' great', ' again', ' Will', 'ل', ' Tune']\n","85 [' again', ' Will', ' Again', ' great', ' will']\n","86 [' known', 'URA', ' Skydragon', ' Will', ' certain']\n","87 ['URA', ' new', ' thanks', ' all', ' again']\n","88 [' certain', '031', ' unspecified', ' given', 'URA']\n","89 [' certain', 'URA', ' unspecified', ' specified', ' new']\n","90 [' unspecified', ' unknown', ' known', ' Certain', ' certain']\n","91 [' unknown', ' great', ' bunch', ' unspecified', 'URA']\n","92 [' unknown', ' known', ' unspecified', ' undefined', ' bunch']\n","93 [' unspecified', ' unknown', ' certain', ' known', ' great']\n","94 [' unknown', ' known', ' certain', ' unspecified', ' great']\n","95 [' great', ' Will', ' will', 'Will', ' again']\n","96 [' great', ' known', ' unknown', ' certain', ' named']\n","97 [' certain', ' great', ' Will', '031', ' named']\n","98 [' certain', ' great', ' named', ' unspecified', ' specified']\n","99 [' certain', ' great', ' tremendous', ' exact', 'isen']\n","100 [' great', ' tremendous', ' thousands', ' Will', ' sorts']\n","101 [' Will', ' again', 'Will', ' Previously', 'ukong']\n","102 [' unknown', ' great', ' tremendous', ' unbelievable', ' countless']\n","103 [' great', ' Will', ' again', ' gonna', ' unknown']\n"]}]},{"cell_type":"code","source":["resid_after_OV_pos = resid_after_mlp1\n","for layer in range(12):\n","    for head in range(12):\n","        W_OV = model.W_V[layer, head] @ model.W_O[layer, head]\n","        resid_after_OV_pos = resid_after_OV_pos @ W_OV\n","\n","resid_after_mlp9 = resid_after_OV_pos + mlp9(ln9(resid_after_OV_pos))\n","\n","logits_pos: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_mlp9)).squeeze()\n","\n","for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits_pos[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDaVw9K1PRp8","executionInfo":{"status":"ok","timestamp":1702253987425,"user_tz":300,"elapsed":8330,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8af10552-ce6e-4d4b-e725-a2efa1cd9c47"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["1 ['bugs']\n","2 ['bugs']\n","3 ['bugs']\n","4 ['bugs']\n","5 ['bugs']\n","6 ['bugs']\n","7 ['bugs']\n","8 ['bugs']\n","9 ['bugs']\n","10 ['bugs']\n","11 ['bugs']\n","12 ['bugs']\n","13 ['bugs']\n","14 ['bugs']\n","15 ['bugs']\n","16 ['bugs']\n","17 ['bugs']\n","18 ['bugs']\n","19 ['bugs']\n","20 ['bugs']\n","21 ['bugs']\n","22 ['bugs']\n","23 ['bugs']\n","24 ['bugs']\n","25 ['bugs']\n","26 ['bugs']\n","27 ['bugs']\n","28 ['bugs']\n","29 ['bugs']\n","30 ['bugs']\n","31 ['bugs']\n","32 ['bugs']\n","33 ['bugs']\n","34 ['bugs']\n","35 ['bugs']\n","36 ['bugs']\n","37 ['bugs']\n","38 ['bugs']\n","39 ['bugs']\n","40 ['bugs']\n","41 ['bugs']\n","42 ['bugs']\n","43 ['bugs']\n","44 ['bugs']\n","45 ['bugs']\n","46 ['bugs']\n","47 ['bugs']\n","48 ['bugs']\n","49 ['bugs']\n","50 ['bugs']\n","51 ['bugs']\n","52 ['bugs']\n","53 ['bugs']\n","54 ['bugs']\n","55 ['bugs']\n","56 ['bugs']\n","57 ['bugs']\n","58 ['bugs']\n","59 ['bugs']\n","60 ['bugs']\n","61 ['bugs']\n","62 ['bugs']\n","63 ['bugs']\n","64 ['bugs']\n","65 ['bugs']\n","66 ['bugs']\n","67 ['bugs']\n","68 ['bugs']\n","69 ['bugs']\n","70 ['bugs']\n","71 ['bugs']\n","72 ['bugs']\n","73 ['bugs']\n","74 ['bugs']\n","75 ['bugs']\n","76 ['bugs']\n","77 ['bugs']\n","78 ['bugs']\n","79 ['bugs']\n","80 ['bugs']\n","81 ['bugs']\n","82 ['bugs']\n","83 ['bugs']\n","84 ['bugs']\n","85 ['bugs']\n","86 ['bugs']\n","87 ['bugs']\n","88 ['bugs']\n","89 ['bugs']\n","90 ['bugs']\n","91 ['bugs']\n","92 ['bugs']\n","93 ['bugs']\n","94 ['bugs']\n","95 ['bugs']\n","96 ['bugs']\n","97 ['bugs']\n","98 ['bugs']\n","99 ['bugs']\n","100 ['bugs']\n"]}]},{"cell_type":"markdown","source":["What if MLP 9 filters out all results except for 9.1?"],"metadata":{"id":"vR19NOwuMOzW"}},{"cell_type":"code","source":["layer = 4\n","head = 4\n","\n","# Get W_OV matrix\n","W_OV = model.W_V[layer, head] @ model.W_O[layer, head]\n","\n","# Get residual stream after applying W_OV or -W_OV respectively\n","# (note, because of bias b_U, it matters that we do sign flip here, not later)\n","resid_after_OV_pos = resid_after_mlp1 @ W_OV\n","# resid_after_OV_neg = resid_after_mlp1 @ -W_OV\n","\n","resid_after_mlp9 = resid_after_OV_pos + mlp9(ln9(resid_after_OV_pos))\n","\n","logits_pos: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_mlp9)).squeeze()\n","\n","for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            logits_pos[seq_idx, dataset.word_idx[word][seq_idx]], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"id":"QrP214WWMRk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_copying_scores(\n","    model: HookedTransformer,\n","    k: int = 5,\n","    names: list = NAMES\n",") -> Float[Tensor, \"2 layer-1 head\"]:\n","    '''\n","    Gets copying scores (both positive and negative) as described in page 6 of the IOI paper, for every (layer, head) pair in the model.\n","\n","    Returns these in a 3D tensor (the first dimension is for positive vs negative).\n","\n","    Omits the 0th layer, because this is before MLP0 (which we're claiming acts as an extended embedding).\n","    '''\n","    # SOLUTION\n","    results = t.zeros((2, model.cfg.n_layers, model.cfg.n_heads), device=device)\n","\n","    # Define components from our model (for typechecking, and cleaner code)\n","    embed: Embed = model.embed\n","    mlp0: MLP = model.blocks[0].mlp\n","    ln0: LayerNorm = model.blocks[0].ln2\n","    unembed: Unembed = model.unembed\n","    ln_final: LayerNorm = model.ln_final\n","\n","    # Get embeddings for the names in our list\n","    name_tokens: Int[Tensor, \"batch 1\"] = model.to_tokens(names, prepend_bos=False)\n","    name_embeddings: Int[Tensor, \"batch 1 d_model\"] = embed(name_tokens)\n","\n","    # Get residual stream after applying MLP\n","    resid_after_mlp1 = name_embeddings + mlp0(ln0(name_embeddings))\n","\n","    # Loop over all (layer, head) pairs\n","    for layer in range(1, model.cfg.n_layers):\n","        for head in range(model.cfg.n_heads):\n","\n","            # Get W_OV matrix\n","            W_OV = model.W_V[layer, head] @ model.W_O[layer, head]\n","\n","            # Get residual stream after applying W_OV or -W_OV respectively\n","            # (note, because of bias b_U, it matters that we do sign flip here, not later)\n","            resid_after_OV_pos = resid_after_mlp1 @ W_OV\n","            resid_after_OV_neg = resid_after_mlp1 @ -W_OV\n","\n","            # Get logits from value of residual stream\n","            logits_pos: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_OV_pos)).squeeze()\n","            logits_neg: Float[Tensor, \"batch d_vocab\"] = unembed(ln_final(resid_after_OV_neg)).squeeze()\n","\n","            # Check how many are in top k\n","            topk_logits: Int[Tensor, \"batch k\"] = t.topk(logits_pos, dim=-1, k=k).indices\n","            in_topk = (topk_logits == name_tokens).any(-1)\n","            # Check how many are in bottom k\n","            bottomk_logits: Int[Tensor, \"batch k\"] = t.topk(logits_neg, dim=-1, k=k).indices\n","            in_bottomk = (bottomk_logits == name_tokens).any(-1)\n","\n","            # Fill in results\n","            results[:, layer-1, head] = t.tensor([in_topk.float().mean(), in_bottomk.float().mean()])\n","\n","    return results"],"metadata":{"id":"uhqaQnFYDsXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate dataset 1 prompts"],"metadata":{"id":"vrCXPAurNL6c"}},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'text': f\"{i}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 101)\n","dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702253421503,"user_tz":300,"elapsed":162,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"W9O-rIrhNL6i"},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# results = t.zeros((2, model.cfg.n_layers, model.cfg.n_heads)) #, device=device\n","\n","# Define components from our model (for typechecking, and cleaner code)\n","embed = model.embed\n","mlp0 = model.blocks[0].mlp\n","ln0 = model.blocks[0].ln2\n","unembed = model.unembed\n","ln_final = model.ln_final\n","\n","# Get embeddings for the names in our list\n","# name_tokens: Int[Tensor, \"batch 1\"] = model.to_tokens(names, prepend_bos=False)\n","# name_embeddings: Int[Tensor, \"batch 1 d_model\"] = embed(name_tokens)\n","\n","embeddings = embed(dataset.toks)\n","embeddings.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702253422762,"user_tz":300,"elapsed":135,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0ba7528e-d5aa-4bdd-b37d-d7a6e5ef2097","id":"CSVjOlb8NZHN"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 1, 768])"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["# Get residual stream after applying MLP\n","resid_after_mlp1 = embeddings + mlp0(ln0(embeddings))\n","\n","mlp9 = model.blocks[9].mlp\n","ln9 = model.blocks[9].ln2\n","resid_after_mlp9 = resid_after_mlp1 + mlp9(ln9(resid_after_mlp1))\n","logits = unembed(ln_final(resid_after_mlp9)).squeeze()\n","logits.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702253452926,"user_tz":300,"elapsed":447,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"470e9213-9752-4453-978c-fb7d016fc45f","id":"Le3VvGZcNZHU"},"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 50257])"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["k=5\n","topk_logits: Int[Tensor, \"batch k\"] = t.topk(logits, dim=-1, k=k).indices\n","topk_logits.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702253467704,"user_tz":300,"elapsed":2092,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"97b153dd-1ab0-4ede-aa7a-e45048e5dc56","id":"kqOcuyZxNZHU"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 5])"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702253468735,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c0e3418e-7c88-4c52-f20c-ab2514af6cde","id":"0lFrkAvrNZHU"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['S1']"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["logits[0].size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ypj3TKpyN52t","executionInfo":{"status":"ok","timestamp":1702253565130,"user_tz":300,"elapsed":369,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b1967e4f-3761-4c50-f585-5b09128ad9dd"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50257])"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["k=1\n","for seq_idx, prompt in enumerate(dataset.prompts):\n","    # for word in words:\n","    word = words[-1]\n","    pred_tokens = [\n","        model.tokenizer.decode(token)\n","        for token in torch.topk(\n","            # logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","            logits[seq_idx], k\n","        ).indices\n","    ]\n","    print(prompt[word], pred_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702253602704,"user_tz":300,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0f0dfc18-3bdd-468d-e537-99e7774d09c0","id":"XVNOJEhrNZHV"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["1 ['128']\n","2 ['nd']\n","3 ['rd']\n","4 ['teenth']\n","5 ['th']\n","6 ['teenth']\n","7 ['46']\n","8 ['192']\n","9 ['999']\n","10 ['82']\n","11 ['87']\n","12 ['02']\n","13 ['66']\n","14 ['159']\n","15 ['50']\n","16 ['384']\n","17 ['76']\n","18 ['650']\n","19 ['37']\n","20 ['GW']\n","21 ['st']\n","22 ['nd']\n","23 ['rd']\n","24 ['89']\n","25 ['th']\n","26 ['th']\n","27 ['th']\n","28 ['th']\n","29 ['89']\n","30 ['30']\n","31 ['803']\n","32 ['nd']\n","33 ['rd']\n","34 ['68']\n","35 ['00']\n","36 ['36']\n","37 ['37']\n","38 ['608']\n","39 ['61']\n","40 ['40']\n","41 ['41']\n","42 ['nd']\n","43 ['rd']\n","44 ['00']\n","45 ['678']\n","46 ['46']\n","47 ['00']\n","48 ['576']\n","49 ['49']\n","50 ['50']\n","51 ['50']\n","52 ['50']\n","53 ['rd']\n","54 ['32']\n","55 ['55']\n","56 ['32']\n","57 ['LM']\n","58 ['427']\n","59 ['61']\n","60 [' Minutes']\n","61 ['61']\n","62 ['803']\n","63 ['rd']\n","64 ['64']\n","65 [' ILCS']\n","66 ['67']\n","67 ['89']\n","68 ['68']\n","69 ['69']\n","70 [' ILCS']\n","71 ['002']\n","72 ['80']\n","73 ['70']\n","74 ['69']\n","75 [' ILCS']\n","76 ['80']\n","77 ['77']\n","78 ['78']\n","79 ['79']\n","80 ['80']\n","81 ['803']\n","82 ['502']\n","83 ['rd']\n","84 ['80']\n","85 ['67']\n","86 ['32']\n","87 ['66']\n","88 ['889']\n","89 ['89']\n","90 ['90']\n","91 ['504']\n","92 ['502']\n","93 ['rd']\n","94 ['CE']\n","95 ['95']\n","96 ['152']\n","97 ['152']\n","98 ['98']\n","99 ['999']\n","100 ['00000']\n"]}]}]}