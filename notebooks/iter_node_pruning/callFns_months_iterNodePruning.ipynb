{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","collapsed_sections":["vKYgaZ9JjihZ","DcZG9rm2IAiA","6Fuq8XW770vX","w82u8B4EZdWi","zkx8xD8dwWOL","0NYZB-G19liQ","xbZkzn0nrrxt"],"authorship_tag":"ABX9TyN69OflKDWAcyYkAAUoOHVU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This will save files to your local machine if `save_files` is set to True."],"metadata":{"id":"6iIlUWijq7eJ"}},{"cell_type":"markdown","source":["# Change Inputs Here"],"metadata":{"id":"vKYgaZ9JjihZ"}},{"cell_type":"code","source":["task = \"numerals\"  # choose: numerals, numwords, months\n","prompt_types = ['done', 'lost', 'names']\n","num_samps_per_ptype = 512 #768 512\n","\n","model_name = \"gpt2-small\"\n","\n","save_files = True\n","run_on_other_tasks = True"],"metadata":{"id":"KSKP_OsTDki6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup"]},{"cell_type":"code","source":["%pip install git+https://github.com/neelnanda-io/TransformerLens.git"],"metadata":{"id":"F1wsEy0MqHU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6b1n2tvIAiD"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","# import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML\n","\n","import pickle\n","from google.colab import files\n","\n","import matplotlib.pyplot as plt\n","import statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuhzYxbsIAiE"},"outputs":[],"source":["import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFMTUcQiIAiF"},"outputs":[],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"OLkInsdjyHMx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLwDyosvIAiJ"},"outputs":[],"source":["model = HookedTransformer.from_pretrained(\n","    model_name,\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","source":["## Import functions from repo"],"metadata":{"id":"Z4iJEGh6b56v"}},{"cell_type":"code","source":["!git clone https://github.com/wlg1/seqcont_circ_expms.git"],"metadata":{"id":"F8TXMRL3CoPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/seqcont_circ_expms"],"metadata":{"id":"L41onbdlDQWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dataset import Dataset\n","from generate_data import *\n","from metrics import *\n","from head_ablation_fns import *\n","from mlp_ablation_fns import *\n","from node_ablation_fns import *\n","from loop_node_ablation_fns import *"],"metadata":{"id":"22TI4zjMDMfQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load datasets"],"metadata":{"id":"6Fuq8XW770vX"}},{"cell_type":"code","source":["prompts_list = []\n","\n","for i in prompt_types:\n","    file_name = f'/content/seqcont_circ_expms/data/{task}/{task}_prompts_{i}.pkl'\n","    with open(file_name, 'rb') as file:\n","        filelist = pickle.load(file)\n","\n","    print(filelist[0]['text'])\n","    prompts_list += filelist [:num_samps_per_ptype]\n","\n","len(prompts_list)"],"metadata":{"id":"CIe5yXuDhgEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list[0]['text']))):\n","    pos_dict['S'+str(i)] = i"],"metadata":{"id":"kS_Tlrb_70vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = Dataset(prompts_list, pos_dict, model.tokenizer)"],"metadata":{"id":"u0NPSKcZ1iDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_name = f'/content/seqcont_circ_expms/data/{task}/randDS_{task}.pkl'\n","with open(file_name, 'rb') as file:\n","    prompts_list_2 = pickle.load(file)"],"metadata":{"id":"-GJ_ZC48FB1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)"],"metadata":{"id":"msu6D4p_feW5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Get orig score"],"metadata":{"id":"BHHvz84w70vh"}},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)\n","ioi_logits_original = model(dataset.toks)\n","orig_score = get_logit_diff(ioi_logits_original, dataset)"],"metadata":{"id":"OI3FcmpMaNxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","\n","del(ioi_logits_original)\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"A-TjmW5PUwGC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find Impt Attention Heads from Full"],"metadata":{"id":"w82u8B4EZdWi"}},{"cell_type":"code","source":["circ = [(layer, head) for layer in range(12) for head in range(12)]\n","to_loop = [(layer, head) for layer in range(12) for head in range(12)]\n","\n","lh_scores = {}\n","for lh in to_loop:\n","    copy_circuit = circ.copy()\n","    copy_circuit.remove(lh)\n","    print(\"removed: \" + str(lh))\n","    new_score = ablate_head_from_full(copy_circuit, model, dataset, dataset_2, orig_score, print_output=True).item()\n","    lh_scores[lh] = new_score"],"metadata":{"id":"msckx6kcZgAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sort the dictionary by values in descending order\n","sorted_lh_scores = sorted(lh_scores.items(), key=lambda item: -item[1], reverse=True)\n","\n","# Iterate over the top 10 items and print them\n","for lh, score in sorted_lh_scores[:10]:\n","    modified_score = -round(100 - score, 2)\n","    print(lh, modified_score)"],"metadata":{"id":"D3E9dzLOZgAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lh_scores_drop = {key: min(0, val-100) for key, val in lh_scores.items()}\n","scores = list(lh_scores_drop.values())\n","plt.hist(scores, bins=10, edgecolor='black')\n","n, bins, patches = plt.hist(scores, bins=10, edgecolor='black')  # Adjust the number of bins as needed\n","\n","# Annotating the histogram with the number of values in each bin\n","for i in range(len(n)):\n","    plt.text(bins[i]+5, n[i], str(int(n[i])), va='bottom', ha='center')\n","\n","plt.xticks(range(-100, 0, 10))\n","plt.xlabel('Percentage Drop from Full Performance')\n","plt.ylabel('Number of Attention Heads')\n","# plt.title('Distribution of Attention Head Performance Drop Percentages')\n","\n","# plt.show()\n","\n","if save_files:\n","    pdf_filename = 'lh_scores_distribution.pdf'\n","    plt.savefig(pdf_filename)\n","    files.download(pdf_filename)"],"metadata":{"id":"yVAAmsZikKQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores = list(lh_scores_drop.values())\n","mean_score = statistics.mean(scores)\n","print(\"Mean of the scores:\", mean_score)\n","if save_files:\n","    with open('numerals_lh_scores.pkl', 'wb') as file:\n","        pickle.dump(lh_scores, file)\n","        files.download('numerals_lh_scores.pkl')"],"metadata":{"id":"A6E_Lbomm1js"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find Impt MLPs from Full"],"metadata":{"id":"zkx8xD8dwWOL"}},{"cell_type":"code","source":["for i in range(12):\n","    lst = [layer for layer in range(12) if layer != i]\n","    perc_of_orig = ablate_MLP_from_full(lst, model, dataset, dataset_2, orig_score, print_output=False).item()\n","    print(i, perc_of_orig)"],"metadata":{"id":"I9SR5ETh6BWw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Node Ablation Iteration"],"metadata":{"id":"0NYZB-G19liQ"}},{"cell_type":"code","source":["threshold = 20\n","curr_circ_heads = []\n","curr_circ_mlps = []\n","prev_score = 100\n","new_score = 0\n","iter = 1\n","all_comp_scores = []\n","while prev_score != new_score:\n","    print('\\nbackw prune, iter ', str(iter))\n","    old_circ_heads = curr_circ_heads.copy() # save old before finding new one\n","    old_circ_mlps = curr_circ_mlps.copy()\n","    curr_circ_heads, curr_circ_mlps, new_score, comp_scores = find_circuit_backw(model, dataset, dataset_2, curr_circ_heads, curr_circ_mlps, orig_score, threshold)\n","    if old_circ_heads == curr_circ_heads and old_circ_mlps == curr_circ_mlps:\n","        break\n","    all_comp_scores.append(comp_scores)\n","    print('\\nfwd prune, iter ', str(iter))\n","    # track changes in circuit as for some reason it doesn't work with scores\n","    old_circ_heads = curr_circ_heads.copy()\n","    old_circ_mlps = curr_circ_mlps.copy()\n","    curr_circ_heads, curr_circ_mlps, new_score, comp_scores = find_circuit_forw(model, dataset, dataset_2, curr_circ_heads, curr_circ_mlps, orig_score, threshold)\n","    if old_circ_heads == curr_circ_heads and old_circ_mlps == curr_circ_mlps:\n","        break\n","    all_comp_scores.append(comp_scores)\n","    iter += 1"],"metadata":{"id":"gUqLZl-QsahY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('numerals_bf_20_scores.pkl', 'wb') as file:\n","    pickle.dump(all_comp_scores, file)\n","files.download('numerals_bf_20_scores.pkl')"],"metadata":{"id":"V93FIJs2MiU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["curr_circ_heads"],"metadata":{"id":"oPlS7M_vyBSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["curr_circ_mlps"],"metadata":{"id":"zrSFmCgtyFwY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Find most impt heads from circ"],"metadata":{"id":"8At2Kqx69liS"}},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_ablation_hook_MLP_head(model, dataset_2, curr_circ_heads, curr_circ_mlps)\n","\n","new_logits = model(dataset.toks)\n","new_score = get_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"id":"ivoDzNKY9liS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lh_scores = {}\n","for lh in curr_circ_heads:\n","    copy_circuit = curr_circ_heads.copy()\n","    copy_circuit.remove(lh)\n","    print(\"removed: \" + str(lh))\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    model = add_ablation_hook_MLP_head(model, dataset_2, copy_circuit, curr_circ_mlps)\n","\n","    new_logits = model(dataset.toks)\n","    new_score = get_logit_diff(new_logits, dataset).item()\n","    new_perc = 100 * new_score / orig_score\n","    print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","    lh_scores[lh] = new_perc"],"metadata":{"id":"vsUtHR-y9liS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted_lh_scores = dict(sorted(lh_scores.items(), key=lambda item: item[1]))\n","for lh, score in sorted_lh_scores.items():\n","    print(lh, -round(circ_score-score.item(), 2))"],"metadata":{"id":"MNzdWLFj9liT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run on other task's circuits"],"metadata":{"id":"xbZkzn0nrrxt"}},{"cell_type":"code","source":["heads_not_ablate = [(0, 1), (1, 5), (4, 4), (4, 10), (5, 0), (6, 1), (6, 6), (6, 10), (7, 11), (8, 1), (8, 6), (8, 8), (8, 9), (9, 1)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11]\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = get_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"id":"i0GzjXOWFYkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# numwords\n","heads_not_ablate = [(0, 1), (1, 5), (4, 4), (4, 10), (5, 8), (6, 1), (6, 6), (6, 10), (7, 2), (7, 6), (7, 11), (8, 1), (8, 6), (8, 8), (8, 9), (8, 11), (9, 1), (9, 5), (9, 7)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = get_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"id":"XgtbBeiarrx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# months\n","heads_not_ablate = [(0, 1), (0, 5), (4, 4), (6, 1), (6, 6), (6, 10), (7, 6), (7, 9), (7, 10), (7, 11), (8, 8), (9, 1), (10, 7)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = get_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"id":"IYcsLUjIrrx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CIRCUIT = {\n","    \"name mover\": [(9, 9), (10, 0), (9, 6)],\n","    \"backup name mover\": [(10, 10), (10, 6), (10, 2), (10, 1), (11, 2), (9, 7), (9, 0), (11, 9)],\n","    \"negative name mover\": [(10, 7), (11, 10)],\n","    \"s2 inhibition\": [(7, 3), (7, 9), (8, 6), (8, 10)],\n","    \"induction\": [(5, 5), (5, 8), (5, 9), (6, 9)],\n","    \"duplicate token\": [(0, 1), (0, 10), (3, 0)],\n","    \"previous token\": [(2, 2), (4, 11)],\n","}\n","\n","import itertools\n","a = [val for val in CIRCUIT.values()]\n","IOI_heads = list(itertools.chain.from_iterable(a))\n","\n","mlps_not_ablate = list(range(12))\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_ablation_hook_MLP_head(model, dataset_2, IOI_heads, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = get_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"id":"e2_0-p-pn4xw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### months w/ MLP 11\n","\n","heads_not_ablate = [(0, 1), (4, 4), (4, 10), (6, 1), (6, 6), (6, 10), (7, 2), (7, 10), (7, 11), (8, 8), (9, 1)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11] # incl 5 makes it 66.1155%\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = get_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"id":"DYJ29X3Dn4-b"},"execution_count":null,"outputs":[]}]}