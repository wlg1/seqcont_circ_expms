{"cells":[{"cell_type":"markdown","metadata":{"id":"5DKgJ54Wl6mA"},"source":["# Setup Imports\n"]},{"cell_type":"markdown","metadata":{"id":"dGItip9frNK_"},"source":["Code from: https://arena-ch1-transformers.streamlit.app/[1.3]_Indirect_Object_Identification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"DC9tiWTycQxM","outputId":"b1a0f88e-6e0c-4805-8a44-b9ad5d15858e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n","Collecting jaxtyping\n","  Downloading jaxtyping-0.2.24-py3-none-any.whl (38 kB)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (1.23.5)\n","Collecting typeguard<3,>=2.13.3 (from jaxtyping)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (4.5.0)\n","Installing collected packages: typeguard, jaxtyping\n","Successfully installed jaxtyping-0.2.24 typeguard-2.13.3\n","Collecting transformer_lens\n","  Downloading transformer_lens-1.12.0-py3-none-any.whl (118 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate>=0.23.0 (from transformer_lens)\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n","  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.7.1 (from transformer_lens)\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.7.0)\n","Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.24)\n","Collecting numpy>=1.24 (from transformer_lens)\n","  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.5.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.0)\n","Collecting torch!=2.0,!=2.1.0,>=1.10 (from transformer_lens)\n","  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.1)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.35.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.5.0)\n","Collecting wandb>=0.13.5 (from transformer_lens)\n","  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n","Collecting pyarrow-hotfix (from datasets>=2.7.1->transformer_lens)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n","Collecting multiprocess (from datasets>=2.7.1->transformer_lens)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.9.1)\n","Requirement already satisfied: typeguard<3,>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.3.post1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (0.15.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n","  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fancy-einsum, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, gitdb, nvidia-cusolver-cu12, GitPython, wandb, torch, datasets, accelerate, transformer_lens\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu121\n","    Uninstalling torch-2.1.0+cu121:\n","      Successfully uninstalled torch-2.1.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.40 accelerate-0.25.0 beartype-0.14.1 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 fancy-einsum-0.0.3 gitdb-4.0.11 multiprocess-0.70.15 numpy-1.26.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pyarrow-hotfix-0.6 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 torch-2.1.2 transformer_lens-1.12.0 wandb-0.16.1\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 74.0M    0 74.0M    0     0  8059k      0 --:--:--  0:00:09 --:--:-- 9046k\n","Archive:  /content/main.zip\n","21ece660f4a10c3cee96388a0170fa5e36c76405\n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/\n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/\n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/.vscode/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/.vscode/settings.json  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/dataset.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/first_unique_char_model.pt  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/model.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/training.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/training_model.ipynb  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/eagx_berlin_challenge/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/eagx_berlin_challenge/dataset.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/eagx_berlin_challenge/model.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/eagx_berlin_challenge/palindrome_classifier.pt  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/eagx_berlin_challenge/palindrome_classifier_hard.pt  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/eagx_berlin_challenge/requirements.txt  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/dataset.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/model.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/palindrome_classifier.pt  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/training.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/training_model.ipynb  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/november23_cumsum/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/november23_cumsum/cumsum_model.pt  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/november23_cumsum/dataset.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/november23_cumsum/model.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/november23_cumsum/training.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/november23_cumsum/training_model.ipynb  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/october23_sorted_list/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/october23_sorted_list/dataset.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/october23_sorted_list/model.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/october23_sorted_list/sorted_list_model.pt  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/october23_sorted_list/training.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/october23_sorted_list/training_model.ipynb  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/dataset.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/model.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/sum_model.pt  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/training.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/training_model.ipynb  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/part1_transformer_from_scratch/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part1_transformer_from_scratch/solutions.py  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/part2_intro_to_mech_interp/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part2_intro_to_mech_interp/solutions.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part2_intro_to_mech_interp/tests.py  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/ioi_circuit_extraction.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/ioi_dataset.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/solutions.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/tests.py  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/brackets_data.json  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/brackets_datasets.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/brackets_model_state_dict.pt  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/solutions.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/tests.py  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/part5_grokking_and_modular_arithmetic/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part5_grokking_and_modular_arithmetic/my_utils.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part5_grokking_and_modular_arithmetic/solutions.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part5_grokking_and_modular_arithmetic/tests.py  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/part6_othellogpt/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part6_othellogpt/solutions.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part6_othellogpt/tests.py  \n","   creating: ARENA_2.0-main/chapter1_transformers/exercises/part7_toy_models_of_superposition/\n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part7_toy_models_of_superposition/solutions.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part7_toy_models_of_superposition/tests.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part7_toy_models_of_superposition/utils.py  \n","  inflating: ARENA_2.0-main/chapter1_transformers/exercises/plotly_utils.py  \n"]}],"source":["try:\n","    import google.colab # type: ignore\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","\n","import os, sys\n","\n","if IN_COLAB:\n","    # Install packages\n","    %pip install einops\n","    %pip install jaxtyping\n","    %pip install transformer_lens\n","    # %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n","\n","    # Code to download the necessary files (e.g. solutions, test funcs)\n","    import os, sys\n","    if not os.path.exists(\"chapter1_transformers\"):\n","        !curl -o /content/main.zip https://codeload.github.com/callummcdougall/ARENA_2.0/zip/refs/heads/main\n","        !unzip /content/main.zip 'ARENA_2.0-main/chapter1_transformers/exercises/*'\n","        sys.path.append(\"/content/ARENA_2.0-main/chapter1_transformers/exercises\")\n","        os.remove(\"/content/main.zip\")\n","        os.rename(\"ARENA_2.0-main/chapter1_transformers\", \"chapter1_transformers\")\n","        os.rmdir(\"ARENA_2.0-main\")\n","        os.chdir(\"chapter1_transformers/exercises\")\n","else:\n","    from IPython import get_ipython\n","    ipython = get_ipython()\n","    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n","    ipython.run_line_magic(\"autoreload\", \"2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Lx3WrMyUl6mB","outputId":"030c5d28-ef58-4d03-ee72-68105a8fdf89"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/accelerate/utils/imports.py:193: UserWarning: `ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\n","  warnings.warn(\n"]}],"source":["import os; os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n","import sys\n","from pathlib import Path\n","import torch as t\n","from torch import Tensor\n","import numpy as np\n","import einops\n","from tqdm.notebook import tqdm\n","import plotly.express as px\n","import webbrowser\n","import re\n","import itertools\n","from jaxtyping import Float, Int, Bool\n","from typing import List, Optional, Callable, Tuple, Dict, Literal, Set\n","from functools import partial\n","from IPython.display import display, HTML\n","from rich.table import Table, Column\n","from rich import print as rprint\n","# import circuitsvis as cv\n","from pathlib import Path\n","from transformer_lens.hook_points import HookPoint\n","from transformer_lens import utils, HookedTransformer, ActivationCache\n","from transformer_lens.components import Embed, Unembed, LayerNorm, MLP\n","\n","t.set_grad_enabled(False)\n","\n","# Make sure exercises are in the path\n","chapter = r\"chapter1_transformers\"\n","exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n","section_dir = (exercises_dir / \"part3_indirect_object_identification\").resolve()\n","if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n","\n","# from plotly_utils import imshow, line, scatter, bar\n","# import part3_indirect_object_identification.tests as tests\n","\n","device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n","\n","MAIN = __name__ == \"__main__\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MHJUVp2jl6mS"},"outputs":[],"source":["from part3_indirect_object_identification.ioi_dataset import NAMES, IOIDataset"]},{"cell_type":"markdown","metadata":{"id":"QZ0QiQPgqw4a"},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["588d580b7f954495912128fd792aeccd","c0f063a422f0479788cd118bf6df67dd","3505a06110e2495aa2892edce4266fd5","f6db91724a1f46fe8b9a1c947173fb61","267e4da320464ea5b333bb5ba699b0f5","b06409da576e4b8fbd4325e31895f1ee"]},"id":"FyK_Rd9Ol6mE","outputId":"d5c0e391-3bf4-4693-d9a5-63bcf8aa1283"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"588d580b7f954495912128fd792aeccd","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0f063a422f0479788cd118bf6df67dd","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3505a06110e2495aa2892edce4266fd5","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6db91724a1f46fe8b9a1c947173fb61","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"267e4da320464ea5b333bb5ba699b0f5","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b06409da576e4b8fbd4325e31895f1ee","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ql-mdgT_xyUR"},"source":["# Change Inputs Here"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BYlMNXH_x1rH"},"outputs":[],"source":["task = \"months\"\n","heads_not_ablate = [(0, 1), (0, 5), (4, 4), (6, 1), (6, 6), (6, 10), (7, 6), (7, 9), (7, 10), (7, 11), (8, 8), (9, 1), (10, 7)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]"]},{"cell_type":"markdown","metadata":{"id":"6Fuq8XW770vX"},"source":["# Generate dataset with multiple prompts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0Lk7-w67HrpZ"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4wXBNWj5FwVn"},"outputs":[],"source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.corr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.incorr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        # for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","        for targ in [key for key in pos_dict]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CIe5yXuDhgEK","outputId":"d8af4a52-e2e5-4466-ac96-5113fff82c08"},"outputs":[{"name":"stdout","output_type":"stream","text":["Van done in January. Hat done in February. Ring done in March. Desk done in April. Sun done in\n","Oil lost in January. Apple lost in February. Tree lost in March. Snow lost in April. Apple lost in\n","Marcus born in January. Victoria born in February. George born in March. Brandon born in April. Jamie born in\n"]},{"data":{"text/plain":["1536"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import pickle\n","\n","prompts_list = []\n","\n","temps = ['done', 'lost', 'names']\n","\n","for i in temps:\n","    file_name = f'/content/{task}_prompts_{i}.pkl'\n","    with open(file_name, 'rb') as file:\n","        filelist = pickle.load(file)\n","\n","    print(filelist[0]['text'])\n","    prompts_list += filelist [:512] #768 512\n","\n","len(prompts_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kS_Tlrb_70vg"},"outputs":[],"source":["# pos_dict = {\n","#     'S1': 4,\n","#     'S2': 10,\n","#     'S3': 16,\n","#     'S4': 22,\n","# }\n","\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","\n","# pos_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PXjX-6y3HBlG"},"outputs":[],"source":["import pickle\n","file_name = f'/content/randDS_{task}.pkl'\n","with open(file_name, 'rb') as file:\n","    prompts_list_2 = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ocj0L7ZvIM4O"},"outputs":[],"source":["# prompts_list = prompts_list[:500]\n","# prompts_list_2 = prompts_list_2[:500]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"u0NPSKcZ1iDe"},"outputs":[],"source":["dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iiLyuxfSH9cc"},"outputs":[],"source":["dataset_1 = dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"msu6D4p_feW5"},"outputs":[],"source":["dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"]},{"cell_type":"markdown","metadata":{"id":"OL0vNCqgD3m7"},"source":["# Path patching fns"]},{"cell_type":"markdown","metadata":{"id":"ePVYTBKY5QXh"},"source":["## Performance Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iSjSLuVxIRnE","outputId":"76bfe7d0-42cb-4575-af04-44c247fa9e98"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","\n","# del(ioi_cache)\n","# del(ioi_logits_original)\n","\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VPKTb0ril6mT"},"outputs":[],"source":["def logits_to_ave_logit_diff_2(logits: Float[Tensor, \"batch seq d_vocab\"], dataset_1: IOIDataset = dataset_1, per_prompt=False):\n","    '''\n","    Returns logit difference between the correct and incorrect answer.\n","\n","    If per_prompt=True, return the array of differences rather than the average.\n","    '''\n","\n","    # Only the final logits are relevant for the answer\n","    # Get the logits corresponding to the indirect object / subject tokens respectively\n","    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset_1.word_idx[\"end\"], dataset_1.corr_tokenIDs]\n","    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset_1.word_idx[\"end\"], dataset_1.incorr_tokenIDs]\n","    # Find logit difference\n","    answer_logit_diff = corr_logits - incorr_logits\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()\n","\n","model.reset_hooks(including_permanent=True)\n","\n","ioi_logits_original = model(dataset_1.toks)\n","abc_logits_original = model(dataset_2.toks)\n","\n","ioi_average_logit_diff = logits_to_ave_logit_diff_2(ioi_logits_original).item()\n","abc_average_logit_diff = logits_to_ave_logit_diff_2(abc_logits_original).item()\n","orig_score = ioi_average_logit_diff"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5GGyaa_u7jGC","outputId":"c25c1fb7-1988-42ba-b133-390edfa5aada"},"outputs":[{"name":"stdout","output_type":"stream","text":["IOI metric (IOI dataset): 1.0000\n","IOI metric (ABC dataset): -0.0081\n"]}],"source":["def ioi_metric_3(\n","    logits: Float[Tensor, \"batch seq d_vocab\"],\n","    clean_logit_diff: float = ioi_average_logit_diff,\n","    corrupted_logit_diff: float = abc_average_logit_diff,\n","    dataset_1: IOIDataset = dataset_1,\n",") -> float:\n","    patched_logit_diff = logits_to_ave_logit_diff_2(logits, dataset_1)\n","    return (patched_logit_diff / clean_logit_diff)\n","\n","print(f\"IOI metric (IOI dataset): {ioi_metric_3(ioi_logits_original):.4f}\")\n","print(f\"IOI metric (ABC dataset): {ioi_metric_3(abc_logits_original):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dL5-ITHuKQzx"},"outputs":[],"source":["del(ioi_logits_original)\n","del(abc_logits_original)"]},{"cell_type":"markdown","metadata":{"id":"IBxzrUmCgsfw"},"source":["## patching fns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yXu4jtOUg20q"},"outputs":[],"source":["def patch_or_freeze_head_vectors(\n","    orig_head_vector: Float[Tensor, \"batch pos head_index d_head\"],\n","    hook: HookPoint,\n","    new_cache: ActivationCache,\n","    orig_cache: ActivationCache,\n","    head_to_patch: Tuple[int, int],\n",") -> Float[Tensor, \"batch pos head_index d_head\"]:\n","    '''\n","    This helps implement step 2 of path patching. We freeze all head outputs (i.e. set them\n","    to their values in orig_cache), except for head_to_patch (if it's in this layer) which\n","    we patch with the value from new_cache.\n","\n","    head_to_patch: tuple of (layer, head)\n","        we can use hook.layer() to check if the head to patch is in this layer\n","    '''\n","    # Setting using ..., otherwise changing orig_head_vector will edit cache value too\n","    orig_head_vector[...] = orig_cache[hook.name][...]\n","    if head_to_patch[0] == hook.layer():\n","        orig_head_vector[:, :, head_to_patch[1]] = new_cache[hook.name][:, :, head_to_patch[1]]\n","    return orig_head_vector\n","\n","def patch_head_input(\n","    orig_activation: Float[Tensor, \"batch pos head_idx d_head\"],\n","    hook: HookPoint,\n","    patched_cache: ActivationCache,\n","    head_list: List[Tuple[int, int]],\n",") -> Float[Tensor, \"batch pos head_idx d_head\"]:\n","    '''\n","    Function which can patch any combination of heads in layers,\n","    according to the heads in head_list.\n","    '''\n","    heads_to_patch = [head for layer, head in head_list if layer == hook.layer()]\n","    orig_activation[:, :, heads_to_patch] = patched_cache[hook.name][:, :, heads_to_patch]\n","    return orig_activation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4DZ2SGhfl6mW"},"outputs":[],"source":["# def patch_or_freeze_head_vectors(\n","def patch_or_freeze_mlp_vectors(\n","    # orig_head_vector: Float[Tensor, \"batch pos head_index d_head\"],\n","    orig_MLP_vector: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    new_cache: ActivationCache,\n","    orig_cache: ActivationCache,\n","    # head_to_patch: Tuple[int, int],\n","    layer_to_patch: int,\n","# ) -> Float[Tensor, \"batch pos head_index d_head\"]:\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    '''\n","    This helps implement step 2 of path patching. We freeze all head outputs (i.e. set them\n","    to their values in orig_cache), except for head_to_patch (if it's in this layer) which\n","    we patch with the value from new_cache.\n","\n","    head_to_patch: tuple of (layer, head)\n","        we can use hook.layer() to check if the head to patch is in this layer\n","    '''\n","    # the layer is hook.layer(), and orig_head_vector is ALREADY an MLP at a layer, so we don't get it by layer\n","    # we just have to patch in each neuron of the MLP (d_model) that's why dims are \"batch pos d_model\"\n","\n","    # Setting using ..., otherwise changing orig_head_vector will edit cache value too\n","    # this keeps everything the same\n","    # we NEED this to prevent change by ref!\n","    # orig_head_vector[...] = orig_cache[hook.name][...]\n","    # orig_MLP_vector[...] = orig_cache[hook.name][...]\n","\n","    # this change the one MLP layer\n","    # if head_to_patch[0] == hook.layer():\n","    #     orig_head_vector[:, :, head_to_patch[1]] = new_cache[hook.name][:, :, head_to_patch[1]]\n","\n","    # set the entire sender head as new (corr) cache output actvs\n","    if layer_to_patch == hook.layer():\n","        orig_MLP_vector[:, :, :] = new_cache[hook.name][:, :, :]\n","    return orig_MLP_vector\n","\n","def patch_mlp_input(\n","# def patch_head_input(\n","    # orig_activation: Float[Tensor, \"batch pos head_idx d_head\"],\n","    orig_activation: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    patched_cache: ActivationCache,\n","    # head_list: List[Tuple[int, int]],\n","    layer_list: List[int],\n",") -> Float[Tensor, \"batch pos head_idx d_head\"]:\n","    '''\n","    Function which can patch any combination of heads in layers,\n","    according to the heads in head_list.\n","    '''\n","    # heads_to_patch = [head for layer, head in head_list if layer == hook.layer()]  # we dont need list, just layer int\n","    # orig_activation[:, :, heads_to_patch] = patched_cache[hook.name][:, :, heads_to_patch] # heads_to_patch should now be an int, layer?\n","\n","    # exact same thing as before, since we don't need to freeze other heads, so uinlike\n","    # w/ attn heads, this is the same fn. we can just re-use the prev one instead of this one\n","    # we also don't have a head list, but an MLP list\n","    # ACCTU this is diff; there's al ist of layer ints, and only if layer in list do we patch\n","\n","    # orig_activation[...] = orig_cache[hook.name][...]  # we dont need this? but by ref? postpone thinking this\n","\n","    if hook.layer() in layer_list:\n","        # pdb.set_trace()\n","        orig_activation[:, :, :] = patched_cache[hook.name][:, :, :]\n","    return orig_activation"]},{"cell_type":"markdown","metadata":{"id":"DlpH0Wib-v1j"},"source":["# MLP and Head ablation fns"]},{"cell_type":"markdown","metadata":{"id":"G9FQY3H3zkFV"},"source":["## MLP ablation fns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZOTRN8KnheFO"},"outputs":[],"source":["from torch import Tensor\n","from typing import Dict, Tuple, List\n","from jaxtyping import Float, Bool\n","import torch as t\n","\n","def logits_to_ave_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","    '''\n","    Returns logit difference between the correct and incorrect answer.\n","\n","    If per_prompt=True, return the array of differences rather than the average.\n","    '''\n","\n","    # Only the final logits are relevant for the answer\n","    # Get the logits corresponding to the indirect object / subject tokens respectively\n","    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n","    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n","    # Find logit difference\n","    answer_logit_diff = corr_logits - incorr_logits\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"McmRZoY7Wudl"},"outputs":[],"source":["def compute_means_by_template_MLP(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Returns the mean of each head's output over the means dataset. This mean is\n","    computed separately for each group of prompts with the same template (these\n","    are given by means_dataset.groups).\n","    '''\n","    # Cache the outputs of every head\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"mlp_out\"),\n","    )\n","    # Create tensor to store means\n","    n_layers, d_model = model.cfg.n_layers, model.cfg.d_model\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, d_model), device=model.cfg.device)\n","\n","    # Get set of different templates for this data\n","    for layer in range(n_layers):\n","        mlp_output_for_this_layer: Float[Tensor, \"batch seq d_model\"] = means_cache[utils.get_act_name(\"mlp_out\", layer)]\n","        for template_group in means_dataset.groups:  # here, we only have one group\n","            mlp_output_for_this_template = mlp_output_for_this_layer[template_group]\n","            # aggregate all batches\n","            mlp_output_means_for_this_template = einops.reduce(mlp_output_for_this_template, \"batch seq d_model -> seq d_model\", \"mean\")\n","            means[layer, template_group] = mlp_output_means_for_this_template\n","            # at layer, each batch ind is tempalte group (a tensor of size seq d_model)\n","            # is assigned the SAME mean, \"mlp_output_means_for_this_template\"\n","\n","    del(means_cache)\n","\n","    return means"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MH4KI_wCu7M-"},"outputs":[],"source":["def get_mlp_outputs_and_posns_to_keep(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[int]],  # Adjusted to hold list of layers instead of (layer, head) tuples\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq\"]]:  # Adjusted the return type to \"batch seq\"\n","    '''\n","    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n","    MLP output which *shouldn't* be mean-ablated.\n","\n","    The output of this function will be used for the hook function that does ablation.\n","    '''\n","    mlp_outputs_and_posns_to_keep = {}\n","    batch, seq = len(means_dataset), means_dataset.max_len\n","\n","    for layer in range(model.cfg.n_layers):\n","        mask = t.zeros(size=(batch, seq))\n","\n","        for (mlp_type, layer_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[mlp_type]\n","            indices = means_dataset.word_idx[seq_pos]\n","            if layer in layer_list:  # Check if the current layer is in the layer list for this mlp_type\n","                mask[:, indices] = 1\n","\n","        mlp_outputs_and_posns_to_keep[layer] = mask.bool()\n","\n","    return mlp_outputs_and_posns_to_keep"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fXWq7V0Mv0F4"},"outputs":[],"source":["def hook_fn_mask_mlp_out(\n","    mlp_out: Float[Tensor, \"batch seq d_mlp\"],\n","    hook: HookPoint,\n","    mlp_outputs_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq\"]],\n","    means: Float[Tensor, \"layer batch seq d_mlp\"],\n",") -> Float[Tensor, \"batch seq d_mlp\"]:\n","    '''\n","    Hook function which masks the MLP output of a transformer layer.\n","\n","    mlp_outputs_and_posns_to_keep\n","        Dict created with the get_mlp_outputs_and_posns_to_keep function. This tells\n","        us where to mask.\n","\n","    means\n","        Tensor of mean MLP output values of the means_dataset over each group of prompts\n","        with the same template. This tells us what values to mask with.\n","    '''\n","    # Get the mask for this layer, adapted for MLP output structure\n","    mask_for_this_layer = mlp_outputs_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(mlp_out.device)\n","\n","    # Set MLP output values to the mean where necessary\n","    mlp_out = t.where(mask_for_this_layer, mlp_out, means[hook.layer()])\n","\n","    return mlp_out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sJlawX18v-yD"},"outputs":[],"source":["CIRCUIT = {}\n","SEQ_POS_TO_KEEP = {}\n","def add_mean_ablation_hook_MLP(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n","    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Adds a permanent hook to the model, which ablates according to the circuit and\n","    seq_pos_to_keep dictionaries.\n","\n","    In other words, when the model is run on ioi_dataset, every head's output will\n","    be replaced with the mean over means_dataset for sequences with the same template,\n","    except for a subset of heads and sequence positions as specified by the circuit\n","    and seq_pos_to_keep dicts.\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template_MLP(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_mlp_out,\n","        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ko6itvH15NtO"},"outputs":[],"source":["def mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=True):\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = add_mean_ablation_hook_MLP(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    new_logits = model(dataset.toks)\n","\n","    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","    del(new_logits)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"]},{"cell_type":"markdown","metadata":{"id":"Zv5yGHXhpAK_"},"source":["## head fns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cE7xLtws_Pnt"},"outputs":[],"source":["def get_heads_and_posns_to_keep(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","    '''\n","    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n","    z output which *shouldn't* be mean-ablated.\n","\n","    The output of this function will be used for the hook function that does ablation.\n","    '''\n","    heads_and_posns_to_keep = {}\n","    batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","\n","    for layer in range(model.cfg.n_layers):\n","\n","        mask = t.zeros(size=(batch, seq, n_heads))\n","\n","        for (head_type, head_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[head_type]\n","            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","            for (layer_idx, head_idx) in head_list:\n","                if layer_idx == layer:\n","                    mask[:, indices, head_idx] = 1\n","\n","        heads_and_posns_to_keep[layer] = mask.bool()\n","\n","    return heads_and_posns_to_keep\n","\n","def hook_fn_mask_z(\n","    z: Float[Tensor, \"batch seq head d_head\"],\n","    hook: HookPoint,\n","    heads_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n","    means: Float[Tensor, \"layer batch seq head d_head\"],\n",") -> Float[Tensor, \"batch seq head d_head\"]:\n","    '''\n","    Hook function which masks the z output of a transformer head.\n","\n","    heads_and_posns_to_keep\n","        Dict created with the get_heads_and_posns_to_keep function. This tells\n","        us where to mask.\n","\n","    means\n","        Tensor of mean z values of the means_dataset over each group of prompts\n","        with the same template. This tells us what values to mask with.\n","    '''\n","    # Get the mask for this layer, and add d_head=1 dimension so it broadcasts correctly\n","    mask_for_this_layer = heads_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n","\n","    # Set z values to the mean\n","    z = t.where(mask_for_this_layer, z, means[hook.layer()])\n","\n","    return z\n","\n","def compute_means_by_template(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Returns the mean of each head's output over the means dataset. This mean is\n","    computed separately for each group of prompts with the same template (these\n","    are given by means_dataset.groups).\n","    '''\n","    # Cache the outputs of every head\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"z\"),\n","    )\n","    # Create tensor to store means\n","    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n","\n","    # Get set of different templates for this data\n","    for layer in range(model.cfg.n_layers):\n","        z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n","        for template_group in means_dataset.groups:\n","            z_for_this_template = z_for_this_layer[template_group]\n","            z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n","            means[layer, template_group] = z_means_for_this_template\n","\n","    del(means_cache)\n","\n","    return means\n","\n","def add_mean_ablation_hook(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n","    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Adds a permanent hook to the model, which ablates according to the circuit and\n","    seq_pos_to_keep dictionaries.\n","\n","    In other words, when the model is run on ioi_dataset, every head's output will\n","    be replaced with the mean over means_dataset for sequences with the same template,\n","    except for a subset of heads and sequence positions as specified by the circuit\n","    and seq_pos_to_keep dicts.\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_z,\n","        heads_and_posns_to_keep=heads_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"4zlcncHKo2h6"},"source":["## both"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mnGaDWe__CyA"},"outputs":[],"source":["def add_mean_ablation_hook_MLP_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    heads_lst, mlp_lst,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = heads_lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_z,\n","        heads_and_posns_to_keep=heads_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    ########################\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = mlp_lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template_MLP(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_mlp_out,\n","        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"GeskwYZbOY_z"},"source":["# only loop thru sender/mlp nodes of circuit"]},{"cell_type":"markdown","metadata":{"id":"8LJmWsjMT6IV"},"source":["## head to head"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GecpRVxKT6Id"},"outputs":[],"source":["def circ_path_patch_head_to_heads(\n","    circuit: List[Tuple[int, int]],\n","    receiver_heads: List[Tuple[int, int]],\n","    receiver_input: str,\n","    model: HookedTransformer,\n","    patching_metric: Callable,\n","    new_dataset: IOIDataset = dataset_2,\n","    orig_dataset: IOIDataset = dataset_1,\n","    new_cache: Optional[ActivationCache] = None,\n","    orig_cache: Optional[ActivationCache] = None,\n",") -> Float[Tensor, \"layer head\"]:\n","    '''\n","    Performs path patching (see algorithm in appendix B of IOI paper), with:\n","\n","        sender head = (each head, looped through, one at a time)\n","        receiver node = input to a later head (or set of heads)\n","\n","    The receiver node is specified by receiver_heads and receiver_input.\n","    Example (for S-inhibition path patching the queries):\n","        receiver_heads = [(8, 6), (8, 10), (7, 9), (7, 3)],\n","        receiver_input = \"v\"\n","\n","    Returns:\n","        tensor of metric values for every possible sender head\n","    '''\n","    # SOLUTION\n","    # model.reset_hooks()\n","\n","    assert receiver_input in (\"k\", \"q\", \"v\", \"z\")\n","    receiver_layers = set(next(zip(*receiver_heads)))  # a set of all layers of receiver heads\n","    receiver_hook_names = [utils.get_act_name(receiver_input, layer) for layer in receiver_layers]\n","    receiver_hook_names_filter = lambda name: name in receiver_hook_names\n","\n","    results = t.zeros(max(receiver_layers), model.cfg.n_heads, device=\"cuda\", dtype=t.float32)\n","\n","    # ========== Step 1 ==========\n","    # Gather activations on x_orig and x_new\n","\n","    # Note the use of names_filter for the run_with_cache function. Using it means we\n","    # only cache the things we need (in this case, just attn head outputs).\n","    z_name_filter = lambda name: name.endswith(\"z\")\n","    if new_cache is None:\n","        _, new_cache = model.run_with_cache(\n","            new_dataset.toks,\n","            names_filter=z_name_filter,\n","            return_type=None\n","        )\n","    if orig_cache is None:\n","        _, orig_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=z_name_filter,\n","            return_type=None\n","        )\n","\n","    # only consider circuit nodes that\n","    # however, as indices repr L H, results must still be as big as entire circuit\n","    # before the receiver head\n","    senders = [tup for tup in circuit if tup[0] < receiver_heads[0][0]]\n","\n","    for (sender_layer, sender_head) in tqdm(senders):\n","\n","    # Note, the sender layer will always be before the final receiver layer, otherwise there will\n","    # be no causal effect from sender -> receiver. So we only need to loop this far.\n","    # for (sender_layer, sender_head) in tqdm(list(itertools.product(\n","    #     range(max(receiver_layers)),\n","    #     range(model.cfg.n_heads)\n","    # ))):\n","\n","        # ========== Step 2 ==========\n","        # Run on x_orig, with sender head patched from x_new, every other head frozen\n","\n","        hook_fn = partial(\n","            patch_or_freeze_head_vectors,\n","            new_cache=new_cache,\n","            orig_cache=orig_cache,\n","            head_to_patch=(sender_layer, sender_head),\n","        )\n","        model.add_hook(z_name_filter, hook_fn, level=1)\n","\n","        _, patched_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=receiver_hook_names_filter,\n","            return_type=None\n","        )\n","        # model.reset_hooks(including_permanent=True)\n","        assert set(patched_cache.keys()) == set(receiver_hook_names)\n","\n","        # ========== Step 3 ==========\n","        # Run on x_orig, patching in the receiver node(s) from the previously cached value\n","\n","        hook_fn = partial(\n","            patch_head_input,\n","            patched_cache=patched_cache,\n","            head_list=receiver_heads,\n","        )\n","        patched_logits = model.run_with_hooks(\n","            orig_dataset.toks,\n","            fwd_hooks = [(receiver_hook_names_filter, hook_fn)],\n","            return_type=\"logits\"\n","        )\n","\n","        # Save the results\n","        results[sender_layer, sender_head] = patching_metric(patched_logits)\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"M_b5txIwj7Ql"},"source":["## mlp to mlp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7WqoY8OEj7Qm"},"outputs":[],"source":["def circ_path_patch_MLPs_to_MLPs(\n","    mlp_circuit: List[int],\n","    receiver_layers: List[int],\n","    # receiver_input: str,\n","    model: HookedTransformer,\n","    patching_metric: Callable,\n","    new_dataset: IOIDataset = dataset_2,\n","    orig_dataset: IOIDataset = dataset_1,\n","    new_cache: Optional[ActivationCache] = None,\n","    orig_cache: Optional[ActivationCache] = None,\n",") -> Float[Tensor, \"layer head\"]:\n","    '''\n","    Performs path patching (see algorithm in appendix B of IOI paper), with:\n","\n","        sender head = (each head, looped through, one at a time)\n","        receiver node = input to a later head (or set of heads)\n","\n","    The receiver node is specified by receiver_heads and receiver_input.\n","    Example (for S-inhibition path patching the queries):\n","        receiver_heads = [(8, 6), (8, 10), (7, 9), (7, 3)],\n","        receiver_input = \"v\"\n","\n","    Returns:\n","        tensor of metric values for every possible sender head\n","    '''\n","    # model.reset_hooks()\n","\n","    # assert receiver_input in (\"k\", \"q\", \"v\")  # we can run get_path_patch_head_to_heads() 3 times for k, q, v!\n","    # receiver_layers = set(next(zip(*receiver_heads)))\n","    # receiver_hook_names = [utils.get_act_name(receiver_input, layer) for layer in receiver_layers]\n","    receiver_hook_names = [utils.get_act_name('mlp_out', layer) for layer in receiver_layers]  # modify for mlp_out\n","    receiver_hook_names_filter = lambda name: name in receiver_hook_names\n","\n","    # results = t.zeros(max(receiver_layers), model.cfg.n_heads, device=\"cuda\", dtype=t.float32)\n","    results = t.zeros(max(receiver_layers), device=\"cuda\", dtype=t.float32)\n","\n","    # ========== Step 1 ==========\n","    # z_name_filter = lambda name: name.endswith(\"z\")\n","    z_name_filter = lambda name: name.endswith(\"mlp_out\")\n","\n","    if new_cache is None:\n","        _, new_cache = model.run_with_cache(\n","            new_dataset.toks,\n","            names_filter=z_name_filter,\n","            return_type=None\n","        )\n","    if orig_cache is None:\n","        _, orig_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=z_name_filter,\n","            return_type=None\n","        )\n","\n","    # Note, the sender layer will always be before the final receiver layer, otherwise there will\n","    # be no causal effect from sender -> receiver. So we only need to loop this far.\n","\n","    # for (sender_layer, sender_head) in tqdm(list(itertools.product(\n","    #     range(max(receiver_layers)),  # all the layers from 0 to highest receiver layer (in circuit)\n","    #     range(model.cfg.n_heads)  # all heads from 0 to 12\n","    # ))):\n","\n","    # for (sender_layer) in range(max(receiver_layers)):  # all the layers from 0 to highest receiver layer (in circuit)\n","    sender_mlp_list = [L for L in mlp_circuit if L < max(receiver_layers)]\n","    for (sender_layer) in sender_mlp_list:\n","        # ========== Step 2 ==========\n","        # Run on x_orig, with sender head patched from x_new, every other head frozen\n","\n","        hook_fn = partial(\n","            # patch_or_freeze_head_vectors,\n","            patch_or_freeze_mlp_vectors,\n","            new_cache=new_cache,\n","            orig_cache=orig_cache,\n","            # head_to_patch=(sender_layer, sender_head),\n","            layer_to_patch = sender_layer # an int\n","        )\n","\n","        model.add_hook(z_name_filter, hook_fn, level=1)\n","\n","        _, patched_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=receiver_hook_names_filter,\n","            return_type=None\n","        )\n","        # model.reset_hooks(including_permanent=True)\n","        # assert set(patched_cache.keys()) == set(receiver_hook_names)\n","\n","        # ========== Step 3 ==========\n","        # Run on x_orig, patching in the receiver node(s) from the previously cached value\n","\n","        hook_fn = partial(\n","            # patch_head_input,\n","            patch_mlp_input,\n","            patched_cache=patched_cache,\n","            # head_list=receiver_heads, # list of layer ints\n","            layer_list=receiver_layers,\n","        )\n","        patched_logits = model.run_with_hooks(\n","            orig_dataset.toks,\n","            fwd_hooks = [(receiver_hook_names_filter, hook_fn)],\n","            return_type=\"logits\"\n","        )\n","\n","        # Save the results\n","        # results[sender_layer, sender_head] = patching_metric(patched_logits)\n","        results[sender_layer] = patching_metric(patched_logits)\n","\n","    # the result is which sender layers affect ALL the inputted nodes. this is why we just\n","    # want to pass one node at a time- to see which layers affect just IT.\n","    # if we want a 'group of nodes under a common type', we'd pass a set of nodes\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"MAqhd68popR0"},"source":["## head to MLP"]},{"cell_type":"markdown","metadata":{"id":"IYWv3k4uopR1"},"source":["head senders to MLP receiver (circ nodes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RaTv9OYbopR1"},"outputs":[],"source":["def circ_path_patch_head_to_mlp(\n","    circuit: List[Tuple[int, int]],\n","    receiver_layers: List[int],\n","    model: HookedTransformer,\n","    patching_metric: Callable,\n","    new_dataset: IOIDataset = dataset_2,\n","    orig_dataset: IOIDataset = dataset_1,\n","    new_cache: Optional[ActivationCache] = None,\n","    orig_cache: Optional[ActivationCache] = None,\n",") -> Float[Tensor, \"layer head\"]:\n","    '''\n","    Performs path patching (see algorithm in appendix B of IOI paper), with:\n","\n","        sender head = (each head, looped through, one at a time)\n","        receiver node = input to a later head (or set of heads)\n","\n","    The receiver node is specified by receiver_heads and receiver_input.\n","    Example (for S-inhibition path patching the queries):\n","        receiver_heads = [(8, 6), (8, 10), (7, 9), (7, 3)],\n","        receiver_input = \"v\"\n","\n","    Returns:\n","        tensor of metric values for every possible sender head\n","    '''\n","    # model.reset_hooks()\n","\n","    # assert receiver_input in (\"k\", \"q\", \"v\")  # we can run get_path_patch_head_to_heads() 3 times for k, q, v!\n","    # receiver_layers = set(next(zip(*receiver_heads)))\n","    # receiver_hook_names = [utils.get_act_name(receiver_input, layer) for layer in receiver_layers]\n","    receiver_hook_names = [utils.get_act_name('mlp_out', layer) for layer in receiver_layers]  # modify for mlp_out\n","    receiver_hook_names_filter = lambda name: name in receiver_hook_names\n","\n","    results = t.zeros(max(receiver_layers), model.cfg.n_heads, device=\"cuda\", dtype=t.float32)\n","    # results = t.zeros(max(receiver_layers), device=\"cuda\", dtype=t.float32)\n","\n","    # ========== Step 1 ==========\n","    # z_name_filter = lambda name: name.endswith(\"z\")\n","    z_name_filter = lambda name: name.endswith((\"z\", \"mlpout\"))  # gets same value as just z\n","\n","    # z_name_filter = lambda name: name.endswith(\"mlp_out\")\n","\n","    if new_cache is None:\n","        _, new_cache = model.run_with_cache(\n","            new_dataset.toks,\n","            names_filter=z_name_filter,\n","            return_type=None\n","        )\n","    if orig_cache is None:\n","        _, orig_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=z_name_filter,\n","            return_type=None\n","        )\n","\n","    # Note, the sender layer will always be before the final receiver layer, otherwise there will\n","    # be no causal effect from sender -> receiver. So we only need to loop this far.\n","\n","    # for (sender_layer, sender_head) in tqdm(list(itertools.product(\n","    #     range(max(receiver_layers)),  # all the layers from 0 to highest receiver layer (in circuit)\n","    #     range(model.cfg.n_heads)  # all heads from 0 to 12\n","    # ))):\n","    senders = [tup for tup in circuit if tup[0] < receiver_layers[0]]\n","\n","    for (sender_layer, sender_head) in tqdm(senders):\n","\n","    # have a separate loop for both MLPs AND heads as senders\n","    # for (sender_layer) in range(max(receiver_layers)):  # all the layers from 0 to highest receiver layer (in circuit)\n","\n","        # ========== Step 2 ==========\n","        # Run on x_orig, with sender head patched from x_new, every other head frozen\n","\n","        hook_fn = partial(\n","            patch_or_freeze_head_vectors,\n","            # patch_or_freeze_mlp_vectors,\n","            new_cache=new_cache,\n","            orig_cache=orig_cache,\n","            head_to_patch=(sender_layer, sender_head),\n","            # layer_to_patch = sender_layer # an int\n","        )\n","\n","        model.add_hook(z_name_filter, hook_fn, level=1)\n","\n","        _, patched_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=receiver_hook_names_filter,\n","            return_type=None\n","        )\n","        # model.reset_hooks(including_permanent=True)\n","        # assert set(patched_cache.keys()) == set(receiver_hook_names)\n","\n","        # ========== Step 3 ==========\n","        # Run on x_orig, patching in the receiver node(s) from the previously cached value\n","\n","        hook_fn = partial(\n","            # patch_head_input,\n","            patch_mlp_input,\n","            patched_cache=patched_cache,\n","            # head_list=receiver_heads, # list of layer ints\n","            layer_list=receiver_layers,\n","        )\n","        patched_logits = model.run_with_hooks(\n","            orig_dataset.toks,\n","            fwd_hooks = [(receiver_hook_names_filter, hook_fn)],\n","            return_type=\"logits\"\n","        )\n","\n","        # Save the results\n","        results[sender_layer, sender_head] = patching_metric(patched_logits)\n","        # results[sender_layer] = patching_metric(patched_logits)\n","\n","    # the result is which sender layers affect ALL the inputted nodes. this is why we just\n","    # want to pass one node at a time- to see which layers affect just IT.\n","    # if we want a 'group of nodes under a common type', we'd pass a set of nodes\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"ok__ApdwYsSu"},"source":["## MLP to head"]},{"cell_type":"markdown","metadata":{"id":"tKFYikeHYsSv"},"source":["MLP senders to head receiver (circ nodes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uXVDECcvYsSv"},"outputs":[],"source":["def circ_path_patch_mlp_to_head(\n","    mlp_circuit: List[int],\n","    receiver_heads: List[Tuple[int, int]],\n","    # receiver_layers: List[int],\n","    receiver_input: str,\n","    model: HookedTransformer,\n","    patching_metric: Callable,\n","    new_dataset: IOIDataset = dataset_2,\n","    orig_dataset: IOIDataset = dataset_1,\n","    new_cache: Optional[ActivationCache] = None,\n","    orig_cache: Optional[ActivationCache] = None,\n",") -> Float[Tensor, \"layer head\"]:\n","    '''\n","    Performs path patching (see algorithm in appendix B of IOI paper), with:\n","\n","        sender head = (each head, looped through, one at a time)\n","        receiver node = input to a later head (or set of heads)\n","\n","    The receiver node is specified by receiver_heads and receiver_input.\n","    Example (for S-inhibition path patching the queries):\n","        receiver_heads = [(8, 6), (8, 10), (7, 9), (7, 3)],\n","        receiver_input = \"v\"\n","\n","    Returns:\n","        tensor of metric values for every possible sender head\n","    '''\n","    # model.reset_hooks() # doesn't make diff if comment out or not\n","\n","    assert receiver_input in (\"k\", \"q\", \"v\", \"z\")  # we can run get_path_patch_head_to_heads() 3 times for k, q, v!\n","    receiver_layers = set(next(zip(*receiver_heads)))\n","    receiver_hook_names = [utils.get_act_name(receiver_input, layer) for layer in receiver_layers]\n","    # receiver_hook_names = [utils.get_act_name('mlp_out', layer) for layer in receiver_layers]  # modify for mlp_out\n","    receiver_hook_names_filter = lambda name: name in receiver_hook_names\n","\n","    # results = t.zeros(max(receiver_layers), model.cfg.n_heads, device=\"cuda\", dtype=t.float32)\n","    results = t.zeros(max(receiver_layers), device=\"cuda\", dtype=t.float32)\n","\n","    # ========== Step 1 ==========\n","    # z_name_filter = lambda name: name.endswith(\"z\")  # this is for sender? actually no; orig cache uses it too\n","    # z_name_filter = lambda name: name.endswith(\"mlp_out\")\n","    z_name_filter = lambda name: name.endswith((\"z\", \"mlpout\"))  # gets same value as just mlp out\n","\n","    if new_cache is None:\n","        _, new_cache = model.run_with_cache(\n","            new_dataset.toks,\n","            names_filter=z_name_filter,\n","            return_type=None\n","        )\n","    if orig_cache is None:\n","        _, orig_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=z_name_filter,\n","            return_type=None\n","        )\n","\n","    # Note, the sender layer will always be before the final receiver layer, otherwise there will\n","    # be no causal effect from sender -> receiver. So we only need to loop this far.\n","\n","    # for (sender_layer, sender_head) in tqdm(list(itertools.product(\n","    #     range(max(receiver_layers)),  # all the layers from 0 to highest receiver layer (in circuit)\n","    #     range(model.cfg.n_heads)  # all heads from 0 to 12\n","    # ))):\n","\n","    # have a separate loop for both MLPs AND heads as senders\n","    # for (sender_layer) in range(max(receiver_layers)):  # all the layers from 0 to highest receiver layer (in circuit)\n","    sender_mlp_list = [L for L in mlp_circuit if L < receiver_heads[0][0]]\n","    for (sender_layer) in sender_mlp_list:\n","\n","        # ========== Step 2 ==========\n","        # Run on x_orig, with sender head patched from x_new, every other head frozen\n","\n","        hook_fn = partial(\n","            # patch_or_freeze_head_vectors,\n","            patch_or_freeze_mlp_vectors,\n","            new_cache=new_cache,\n","            orig_cache=orig_cache,\n","            # head_to_patch=(sender_layer, sender_head),\n","            layer_to_patch = sender_layer # an int\n","        )\n","\n","        model.add_hook(z_name_filter, hook_fn, level=1)\n","\n","        _, patched_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=receiver_hook_names_filter,\n","            return_type=None\n","        )\n","        # model.reset_hooks(including_permanent=True)\n","        assert set(patched_cache.keys()) == set(receiver_hook_names)\n","\n","        # ========== Step 3 ==========\n","        # Run on x_orig, patching in the receiver node(s) from the previously cached value\n","\n","        hook_fn = partial(\n","            patch_head_input,\n","            # patch_mlp_input,\n","            patched_cache=patched_cache,\n","            head_list=receiver_heads, # list of layer ints\n","            # layer_list=receiver_layers,\n","        )\n","        patched_logits = model.run_with_hooks(\n","            orig_dataset.toks,\n","            fwd_hooks = [(receiver_hook_names_filter, hook_fn)],\n","            return_type=\"logits\"\n","        )\n","\n","        # Save the results\n","        # results[sender_layer, sender_head] = patching_metric(patched_logits)\n","        results[sender_layer] = patching_metric(patched_logits)\n","\n","    # the result is which sender layers affect ALL the inputted nodes. this is why we just\n","    # want to pass one node at a time- to see which layers affect just IT.\n","    # if we want a 'group of nodes under a common type', we'd pass a set of nodes\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"c54cqFf2lT2F"},"source":["# loop backw"]},{"cell_type":"markdown","metadata":{"id":"no8gv3Hgka8Y"},"source":["## get circuit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vLVucYAnkdKK","outputId":"b0bf4291-d018-4210-e409-726c76ba9879"},"outputs":[{"name":"stdout","output_type":"stream","text":["(cand circuit / full) %: 80.2966\n"]}],"source":["orig_score = ioi_average_logit_diff\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","abl_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = logits_to_ave_logit_diff_2(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")\n","del(new_logits)"]},{"cell_type":"markdown","metadata":{"id":"HLA7GH89vZY2"},"source":["## head to head"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["42ad34b654bd4ed987893b4654dde5fc","2fb9867aa33948cbb52a30ab76d9c24d","d147037f057f402cace3b922ce87b5bd","c6dd912b36f04d7db5e52ca5a38db0b6","f615f793b25c43af957633792a6ed918","306320d57acd4fb287fd363f933a3b10","7229ca2a8fb142e9bf8d56b483049236","34209ff07daa4bdaad92f07d8442753a","37fb0eb374b04852a440ec2fbff15053","02f50f325dde4df19eabfe9dab741893","f4d79ea34bee4901b96237dc22ed8968","57504efb7f9f4eea80c6542cae6dad28","0b32f693c6f44f40a40e944686380d73","355e294bb1ed42b1aba6d523162e3122","5ddf2de1467d4d28ad32945bdd51d86b","f9dd4befaa6c42c5a86cdf0d3ad96ed9","7c8750dc623141bcbd596a31ce1d2bfa","db381d1aa3ea4226b444d7de1deb4051","f10d9e0f3cf342978916ef2bbe13a075","9b7b358163b149e2a199795bc1586749","5a2415ac5cb34e079c2745c5d820ce7a","006a81e75e494d259fac7314922cdc05","324576ed7a7441a4811d132af8f096c5","84440da0b84a416db912f75dd91583ed","8783dfb75e124e51a9098620c121b594","e2318bbaa5714a6baffc8f4fe24d2497","ad102e836ea945f180ac70f5bb84ac65","9767a1d430644eb9bcd251fd4e61e241","33724f655980462b9389d0202e3090d9","96b7f071a0f64215959978a0747bf233","be2cb81662494b0aa093adea7710ac5e","217d6a74b7524d34a6c90af5e3585546","db54cc6310854320b45e4caa1f6816e2","51dad4b805cf4bf4a4376ac2ab429b21","2b837e190abf4cf18c89bbad9656d581","95fe6e854f07424baf751aabdc69ae6d","ab6f25cff4584a509f3edc0d420473d9","40d2571eb25d4c78a58e5233ae51a7ea","b7a397db20904da79740b66c36dbde8f"]},"id":"LfncU3j7vWWC","outputId":"8f898c23-e01c-4bdd-8447-e35b44844288"},"outputs":[{"name":"stdout","output_type":"stream","text":["q (0, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42ad34b654bd4ed987893b4654dde5fc","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (0, 5)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2fb9867aa33948cbb52a30ab76d9c24d","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (4, 4)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d147037f057f402cace3b922ce87b5bd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (6, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6dd912b36f04d7db5e52ca5a38db0b6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (6, 6)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f615f793b25c43af957633792a6ed918","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (6, 10)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"306320d57acd4fb287fd363f933a3b10","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (7, 6)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7229ca2a8fb142e9bf8d56b483049236","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (7, 9)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34209ff07daa4bdaad92f07d8442753a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (7, 10)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37fb0eb374b04852a440ec2fbff15053","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (7, 11)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02f50f325dde4df19eabfe9dab741893","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (8, 8)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4d79ea34bee4901b96237dc22ed8968","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (9, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57504efb7f9f4eea80c6542cae6dad28","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/11 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["q (10, 7)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b32f693c6f44f40a40e944686380d73","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (0, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"355e294bb1ed42b1aba6d523162e3122","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (0, 5)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ddf2de1467d4d28ad32945bdd51d86b","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (4, 4)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9dd4befaa6c42c5a86cdf0d3ad96ed9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (6, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c8750dc623141bcbd596a31ce1d2bfa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (6, 6)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db381d1aa3ea4226b444d7de1deb4051","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (6, 10)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f10d9e0f3cf342978916ef2bbe13a075","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (7, 6)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b7b358163b149e2a199795bc1586749","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (7, 9)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a2415ac5cb34e079c2745c5d820ce7a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (7, 10)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"006a81e75e494d259fac7314922cdc05","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (7, 11)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"324576ed7a7441a4811d132af8f096c5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (8, 8)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84440da0b84a416db912f75dd91583ed","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (9, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8783dfb75e124e51a9098620c121b594","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/11 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["k (10, 7)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2318bbaa5714a6baffc8f4fe24d2497","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (0, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad102e836ea945f180ac70f5bb84ac65","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (0, 5)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9767a1d430644eb9bcd251fd4e61e241","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (4, 4)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33724f655980462b9389d0202e3090d9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (6, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96b7f071a0f64215959978a0747bf233","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (6, 6)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be2cb81662494b0aa093adea7710ac5e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (6, 10)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"217d6a74b7524d34a6c90af5e3585546","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (7, 6)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db54cc6310854320b45e4caa1f6816e2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (7, 9)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51dad4b805cf4bf4a4376ac2ab429b21","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (7, 10)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b837e190abf4cf18c89bbad9656d581","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (7, 11)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95fe6e854f07424baf751aabdc69ae6d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (8, 8)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab6f25cff4584a509f3edc0d420473d9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (9, 1)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40d2571eb25d4c78a58e5233ae51a7ea","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/11 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["v (10, 7)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7a397db20904da79740b66c36dbde8f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["qkv_to_HH = {} # qkv to dict\n","\n","for head_type in [\"q\", \"k\", \"v\"]:\n","    head_to_head_results = {}\n","    for head in heads_not_ablate:\n","        print(head_type, head)\n","        model.reset_hooks()\n","        model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","        result = circ_path_patch_head_to_heads(\n","            circuit = heads_not_ablate,\n","            receiver_heads = [head],\n","            receiver_input = head_type,\n","            model = model,\n","            patching_metric = ioi_metric_3\n","        )\n","        head_to_head_results[head] = result\n","    qkv_to_HH[head_type] = head_to_head_results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VE2IpLNpHgeb"},"outputs":[],"source":["head_to_head_adjList = {}\n","for head_type in [\"q\", \"k\", \"v\"]:\n","    for head in heads_not_ablate:\n","        result = qkv_to_HH[head_type][head]\n","        filtered_indices = (result < 0.8) & (result != 0.0)\n","        rows, cols = filtered_indices.nonzero(as_tuple=True)\n","        sender_nodes = list(zip(rows.tolist(), cols.tolist()))\n","        head_with_type = head + (head_type,)\n","        head_to_head_adjList[head_with_type] = sender_nodes"]},{"cell_type":"markdown","metadata":{"id":"ccROLT75vT7y"},"source":["## mlp to mlp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nD4bTlmHlVks","outputId":"b5b1212e-6ce7-4134-8dcb-a95a0a8dc7ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["10\n","9\n","8\n","7\n","6\n","4\n","3\n","2\n","1\n","0\n"]}],"source":["mlp_to_mlp_results = {}\n","\n","# for layer in range(11, 0, -1):\n","for layer in reversed(mlps_not_ablate):\n","    print(layer)\n","    model.reset_hooks()\n","    model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","    result = circ_path_patch_MLPs_to_MLPs(\n","        mlp_circuit = mlps_not_ablate,\n","        receiver_layers = [layer],\n","        model = model,\n","        patching_metric = ioi_metric_3\n","    )\n","    mlp_to_mlp_results[layer] = result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NN0Vd9eIrpL9"},"outputs":[],"source":["mlp_to_mlp_adjList = {}\n","for mlp in mlps_not_ablate:\n","    result = mlp_to_mlp_results[mlp]\n","    filtered_indices = (result < 0.80) & (result != 0.0)\n","    filtered_indices = filtered_indices.nonzero(as_tuple=True)[0]\n","    mlp_to_mlp_adjList[mlp] = filtered_indices.tolist()"]},{"cell_type":"markdown","metadata":{"id":"qStvFtFrBHTE"},"source":["## head to mlp"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511,"referenced_widgets":["ddf17b46ef08437780747c6e692542af","40d39c6909a7438f81e1b2acf60d99d8","b3713c4a5bf54d2ab281af92970c2435","76209a28af0640c5a705132a3e4ac2cc","33288baeb5eb40da8403e6cae5375a33","435679452a2a40769705eb2cf94ef7cb","7290fe2c394a48f0adafb7efee6cf3af","c6d93c6ed134484688caccecd67ec95f","a0fbd267bc6c40e68bbdd2abbc45c0bb","4fd2b83f5c654e418c433002da725bad","5f387b4074854b9784bc65a862f77c78","80d42110da264366be8b596e5665f78c","aab0b39b236d4f1cb180ccdf79b423a8","ea3cab216e1f4126994422b914e96bd3","394d3c9437f0405d9036b934b46311c6","49990d442ac14306b9efc85a71224b2d","6e8e9f11e65444a3bca3be5545a81863","c9b12698f3e64a9a8d5b21f0d9bc45f9","d24527e8670e467d8539279a1990c17c","1e2fdcee2be445a9953d7345e53f7bc1","7200e27d37d54bdcbb0e42f6b18deca4","bfd6d1d84e8f409c9e73426bf0e48719","c0c6c315f37f4b46a0c362d09b1cf413","b3cc564a6f3f4b7f825b5a265f3aa7c5","47a06b3ed4cd4a21b9045afa2c74dbb8","ce3803b14dff4010b4d1a1aea260b7b4","c43f1fc71a6d4416b426921eff09b138","982ba6a87fd64775b5d33b5bef529c5c","1c66c565aadb4d6984c547261540eb06","04705e78a2e54f42a5aebf8c66f299e5","676bf22a7b1f4929b202c954a4ca7e4c","80aa1f36153748dd8a231f0caf067972","a802bc5742ea44b1842f30f9334254eb","e71c955560064ebda02a3e582533bf92","74edca15317a4e538f223c09d9134f67","5195ed426e4045949b89122f28a94b30","13dfd24075f54d13a294ce55b294def6","89f7b1a191d042b286dc280b26867c08","855ac33055f8432a9be0507f63eba63d","57723a5ed31e4061bb62aaff84a2f958","a015da68eb164af28224bd8905d0243b","f8c3c68857704b14b4691086224563cd","d0f3d3d76ddb4a5b85dba5c11ce4faa2","21ecefd6fec4401ea61aecf3b7d5414d","4b4dd6be5b234f1b89477d43a1c64650","b5b247b66f6245dc808b992faf550c8e","5800b1f48a914b488eaf1bc60859b767","da54b990b17948b8b775b121cfdf5e4c","c6a426e789e84ca4ac9aa0b1e79ee17d","9818aff1a4514116b4d3776d2daed405","ab689fc27f3d4d58b3f4b258949a63e8","1922410d082a46d89a29d15f2cd79549","cc8cb09984d544cd85971644fbf54630","01861517d23e45b28222120a809210fc","906c3fe99bc347339e8b7a04eb75f255","bc0475f4cdb84edb8a71d69dd208ac18","e74a190c1f1a41e68e27625779815961","3445ab93f28447d8b240866612b4dedf","835baa878fa545dea5eb730e6e97eb87","38022751f3dc4683801441f87a8bf932","ed9a165b782a404d9e3077cda791b5c2","f5399421f7e74006870e091e81b7e094","0b084fc1cea940f18e2e032f304b4f17","6eb3dfc16b03425ea6a4c3c94d8085c7","80ce5abff90c4683bdd2d5c35ea33581","996610ccb764470fb90b439953384319","4e0b644b0cbc45f69135a88f3dd0d58a","5164bf6530c542a3a0ee41522d09843d","f5c4266043a0419bbdc31ad9cd98b59e","86a63c266dfd498fb69ac01230490c65","a089dcb94e944aa3b674d684e7c575a6","babe66d1770a45dc8704148f019465f9","ad8edce31ca74eeba414b216e1bd4bb4","0e75c286c6c447deb283995ffcebce09","35210c2981c94fc4be2a084dc7afda85","f8101ed895ce4014a1a657e00033ab86","d0c344551b074d409f34e3fa94a3b257","4798a6ef29294e62bbae65b644718ace","685390301460444e8715d3778d56a417","66ac26063aee439eb46d0b3669e39a9c","051d16cedb2b4665953e42115d1c6ff8","ef90002373974e5181939b1ca45a61ce","eba66b20bda944c09206177671f769ff","41230bcbdfca4480a348e83292a4b83c","baa45256d56a46bebca8968a434a8616","027afce0fbe54c809dbbba5189476f8e","ee17ca8154414cad99521cfe1b6df477","0f20adceff4d42cabe8a3b3218e63c54","53640e4614e2410992b159521eb774d4","2ea8af47110248fa89a35dd20ccdcf3e","a2368c2341634d7ba66e3fc8a615870a","c41225f87c884cbc85bfe9e59c11256e","8e675c2305f04760853ef857a50e0619","8036c52b89674149b1e15d133049d61a","b2772e54e8844a8db70d12a65f558cbd","239eff92e8944aa691f819ab7a431c33","550d98b276ea4600a046b2c198495aba","e1cf54276ccf4d91a4b1738530eec5e8","39f95c82337042f683af928b51219727","90b47409a516457ea3c633d2f18e93c2","099d3601e0724fbf85b987b0ec1666fb","97c3a43ed0fe4eda867d46c2e434f364","597bf770b6a0468bbaf55c1c334451a1","10ea699d58d24e71ae567782878c3efc","e29debc353e64a9586be738acea802d6","3d800229db944a2b85878930e80df654","18c529152b3f467d9441dfaaf203e295","7d35c5066016458588bccea11f67afbf","84e52143716d4af08e747912136c78af","a27de478ab2e4bb4ba44f26e2ef1a3a7"]},"id":"RkeGOxibBMIs","executionInfo":{"status":"ok","timestamp":1702608742096,"user_tz":300,"elapsed":130946,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"69238b2c-89dd-4ba2-9a8b-41ab1cc4fcc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf17b46ef08437780747c6e692542af"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["9\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/11 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d42110da264366be8b596e5665f78c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["8\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c6c315f37f4b46a0c362d09b1cf413"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["7\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71c955560064ebda02a3e582533bf92"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["6\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b4dd6be5b234f1b89477d43a1c64650"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["4\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0475f4cdb84edb8a71d69dd208ac18"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["3\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0b644b0cbc45f69135a88f3dd0d58a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4798a6ef29294e62bbae65b644718ace"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53640e4614e2410992b159521eb774d4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90b47409a516457ea3c633d2f18e93c2"}},"metadata":{}}],"source":["head_to_mlp_results = {}\n","\n","# for layer in range(11, 0, -1):\n","for layer in reversed(mlps_not_ablate):\n","    print(layer)\n","    model.reset_hooks()\n","    model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","    result = circ_path_patch_head_to_mlp(\n","        circuit = heads_not_ablate,\n","        receiver_layers = [layer],\n","        model = model,\n","        patching_metric = ioi_metric_3\n","    )\n","    head_to_mlp_results[layer] = result"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"9sSF5sgSCQfn","executionInfo":{"status":"ok","timestamp":1702608742097,"user_tz":300,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["head_to_mlp_adjList = {}\n","for layer in mlps_not_ablate:\n","    result = head_to_mlp_results[layer]\n","    filtered_indices = (result < 0.8) & (result != 0.0)\n","    rows, cols = filtered_indices.nonzero(as_tuple=True)\n","    sender_nodes = list(zip(rows.tolist(), cols.tolist()))\n","    head_to_mlp_adjList[layer] = sender_nodes"]},{"cell_type":"markdown","metadata":{"id":"y1DQqZXrEi4o"},"source":["## mlp to head"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4GdPAN1wEuhc","executionInfo":{"status":"ok","timestamp":1702609254965,"user_tz":300,"elapsed":512875,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"12b1a7b0-2a53-4b66-fe6f-224daa52fee6"},"outputs":[{"output_type":"stream","name":"stdout","text":["q (0, 1)\n","q (0, 5)\n","q (4, 4)\n","q (6, 1)\n","q (6, 6)\n","q (6, 10)\n","q (7, 6)\n","q (7, 9)\n","q (7, 10)\n","q (7, 11)\n","q (8, 8)\n","q (9, 1)\n","q (10, 7)\n","k (0, 1)\n","k (0, 5)\n","k (4, 4)\n","k (6, 1)\n","k (6, 6)\n","k (6, 10)\n","k (7, 6)\n","k (7, 9)\n","k (7, 10)\n","k (7, 11)\n","k (8, 8)\n","k (9, 1)\n","k (10, 7)\n","v (0, 1)\n","v (0, 5)\n","v (4, 4)\n","v (6, 1)\n","v (6, 6)\n","v (6, 10)\n","v (7, 6)\n","v (7, 9)\n","v (7, 10)\n","v (7, 11)\n","v (8, 8)\n","v (9, 1)\n","v (10, 7)\n"]}],"source":["qkv_mlp_to_HH = {} # qkv to dict\n","\n","for head_type in [\"q\", \"k\", \"v\"]:\n","    mlp_to_head_results = {}\n","    for head in heads_not_ablate:\n","        print(head_type, head)\n","        model.reset_hooks()\n","        model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","        result = circ_path_patch_mlp_to_head(\n","            mlp_circuit = mlps_not_ablate,\n","            receiver_heads = [head],\n","            receiver_input = head_type,\n","            model = model,\n","            patching_metric = ioi_metric_3\n","        )\n","        mlp_to_head_results[head] = result\n","    qkv_mlp_to_HH[head_type] = mlp_to_head_results"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"OKDLGm2KPlJ2","executionInfo":{"status":"ok","timestamp":1702609254965,"user_tz":300,"elapsed":16,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["mlp_to_head_adjList = {}\n","for head_type in [\"q\", \"k\", \"v\"]:\n","    for head in heads_not_ablate:\n","        result = qkv_mlp_to_HH[head_type][head]\n","        filtered_indices = (result < 0.8) & (result != 0.0)\n","        filtered_indices = filtered_indices.nonzero(as_tuple=True)[0]\n","        head_with_type = head + (head_type,)\n","        mlp_to_head_adjList[head_with_type] = filtered_indices.tolist()"]},{"cell_type":"markdown","metadata":{"id":"u1VIlinotlLf"},"source":["# save graph files to free up memory"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"0gCbZu7DtnwI","executionInfo":{"status":"ok","timestamp":1702609513043,"user_tz":300,"elapsed":384,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["import pickle\n","from google.colab import files"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"U8Fosqu6rt3_","executionInfo":{"status":"ok","timestamp":1702609513378,"user_tz":300,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"94af602e-3b47-46e0-a1d2-00f683a4c353"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_790e49d3-b32a-466e-8306-97dff710451e\", \"months_head_to_head_results.pkl\", 7692)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_035cfc69-ee92-4705-b59c-4a992762f3fc\", \"months_mlp_to_mlp_results.pkl\", 3222)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_27f398fb-4233-40ea-a94a-db942cda3a2c\", \"months_head_to_mlp_results.pkl\", 5462)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_739652b5-bf1b-4fd8-adc5-252642164a9b\", \"months_mlp_to_head_results.pkl\", 4252)"]},"metadata":{}}],"source":["with open(task + \"_head_to_head_results.pkl\", \"wb\") as file:\n","    pickle.dump(head_to_head_results, file)\n","files.download(task + \"_head_to_head_results.pkl\")\n","\n","with open(task + \"_mlp_to_mlp_results.pkl\", \"wb\") as file:\n","    pickle.dump(mlp_to_mlp_results, file)\n","files.download(task + \"_mlp_to_mlp_results.pkl\")\n","\n","with open(task + \"_head_to_mlp_results.pkl\", \"wb\") as file:\n","    pickle.dump(head_to_mlp_results, file)\n","files.download(task + \"_head_to_mlp_results.pkl\")\n","\n","with open(task + \"_mlp_to_head_results.pkl\", \"wb\") as file:\n","    pickle.dump(mlp_to_head_results, file)\n","files.download(task + \"_mlp_to_head_results.pkl\")\n","\n","del(head_to_head_results)\n","del(mlp_to_mlp_results)\n","del(head_to_mlp_results)\n","del(mlp_to_head_results)"]},{"cell_type":"markdown","metadata":{"id":"xyMeogkrRdVJ"},"source":["# resid post"]},{"cell_type":"markdown","metadata":{"id":"ZT1l8tATRna6"},"source":["## head to resid"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"DqvrtttURoeL","executionInfo":{"status":"ok","timestamp":1702609521756,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["def get_path_patch_head_to_final_resid_post(\n","    circuit: List[Tuple[int, int]],\n","    model: HookedTransformer,\n","    patching_metric: Callable,\n","    new_dataset: IOIDataset = dataset_2,\n","    orig_dataset: IOIDataset = dataset_1,\n",") -> Float[Tensor, \"layer head\"]:\n","    '''\n","    Performs path patching (see algorithm in appendix B of IOI paper), with:\n","\n","        sender head = (each head, looped through, one at a time)\n","        receiver node = final value of residual stream\n","\n","    Returns:\n","        tensor of metric values for every possible sender head\n","    '''\n","    # model.reset_hooks()\n","    results = t.zeros(model.cfg.n_layers, model.cfg.n_heads, device=\"cuda\", dtype=t.float32)\n","\n","    resid_post_hook_name = utils.get_act_name(\"resid_post\", model.cfg.n_layers - 1)\n","    resid_post_name_filter = lambda name: name == resid_post_hook_name\n","\n","\n","    # ========== Step 1 ==========\n","    # Gather activations on x_orig and x_new\n","\n","    # Note the use of names_filter for the run_with_cache function. Using it means we\n","    # only cache the things we need (in this case, just attn head outputs).\n","    z_name_filter = lambda name: name.endswith(\"z\")\n","\n","    _, new_cache = model.run_with_cache(\n","        new_dataset.toks,\n","        names_filter=z_name_filter,\n","        return_type=None\n","    )\n","\n","    _, orig_cache = model.run_with_cache(\n","        orig_dataset.toks,\n","        names_filter=z_name_filter,\n","        return_type=None\n","    )\n","\n","    # Looping over every possible sender head (the receiver is always the final resid_post)\n","    # Note use of itertools (gives us a smoother progress bar)\n","    # for (sender_layer, sender_head) in tqdm(list(itertools.product(range(model.cfg.n_layers), range(model.cfg.n_heads)))):\n","\n","    for (sender_layer, sender_head) in tqdm(circuit):\n","\n","        # ========== Step 2 ==========\n","        # Run on x_orig, with sender head patched from x_new, every other head frozen\n","\n","        hook_fn = partial(\n","            patch_or_freeze_head_vectors,\n","            new_cache=new_cache,\n","            orig_cache=orig_cache,\n","            head_to_patch=(sender_layer, sender_head),\n","        )\n","        model.add_hook(z_name_filter, hook_fn)\n","\n","        _, patched_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=resid_post_name_filter,\n","            return_type=None\n","        )\n","\n","        assert set(patched_cache.keys()) == {resid_post_hook_name}\n","\n","        # ========== Step 3 ==========\n","        # Unembed the final residual stream value, to get our patched logits\n","\n","        patched_logits = model.unembed(model.ln_final(patched_cache[resid_post_hook_name]))\n","\n","        # Save the results\n","        results[sender_layer, sender_head] = patching_metric(patched_logits)\n","\n","    return results"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c951132b65724628890b546190bd0f8d","2f8dc6c63d394de08e22ee453b95ee41","29feaadd8f734accb73f3d080713a90c","0770d56feccf4ea4b53752be64ebbe2e","373f5d9558594bef9e1123a74b7c4017","4d3ddeed2ebf44fe964c7df1b611969d","6409fbbcaca24cceb76f7c20b52c0ab6","aefe0a98d900480c8f08ecff179cd1bc","c26cbdcfc3584635b74d3dbf4537d35a","cc34d3046cec4f8cad03560e693bc93c","86b90b657c8f4d60a92c4aaedf5080e4"]},"id":"4m3Qf4FHTzwE","executionInfo":{"status":"ok","timestamp":1702609538160,"user_tz":300,"elapsed":16406,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d8517921-82ba-4719-f19f-1ec0190f0be6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/13 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c951132b65724628890b546190bd0f8d"}},"metadata":{}}],"source":["model.reset_hooks()\n","model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","path_patch_head_to_final_resid_post = get_path_patch_head_to_final_resid_post(heads_not_ablate, model, ioi_metric_3)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLruYArCUbjd","executionInfo":{"status":"ok","timestamp":1702609538160,"user_tz":300,"elapsed":15,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8c76be48-23bb-4c5f-aa3c-06dca3b05416"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([12, 12])"]},"metadata":{},"execution_count":51}],"source":["path_patch_head_to_final_resid_post.size()"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"81vDVv78Re-A","executionInfo":{"status":"ok","timestamp":1702609538160,"user_tz":300,"elapsed":13,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["heads_to_resid = {}\n","result = path_patch_head_to_final_resid_post\n","filtered_indices = (result < 0.8) & (result != 0.0)\n","rows, cols = filtered_indices.nonzero(as_tuple=True)\n","heads_to_resid['resid'] = list(zip(rows.tolist(), cols.tolist()))"]},{"cell_type":"markdown","metadata":{"id":"gdBVxmksWbO_"},"source":["## mlp to resid"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"tjz_J2HMWexY","executionInfo":{"status":"ok","timestamp":1702609538161,"user_tz":300,"elapsed":13,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["def get_path_patch_mlp_to_final_resid_post(\n","    mlp_circuit: List[int],\n","    model: HookedTransformer,\n","    patching_metric: Callable,\n","    new_dataset: IOIDataset = dataset_2,\n","    orig_dataset: IOIDataset = dataset_1,\n",") -> Float[Tensor, \"layer head\"]:\n","    '''\n","    Performs path patching (see algorithm in appendix B of IOI paper), with:\n","\n","        sender head = (each head, looped through, one at a time)\n","        receiver node = final value of residual stream\n","\n","    Returns:\n","        tensor of metric values for every possible sender head\n","    '''\n","    # model.reset_hooks()\n","    results = t.zeros(model.cfg.n_layers, device=\"cuda\", dtype=t.float32) #model.cfg.n_heads,\n","\n","    resid_post_hook_name = utils.get_act_name(\"resid_post\", model.cfg.n_layers - 1)\n","    resid_post_name_filter = lambda name: name == resid_post_hook_name\n","\n","\n","    # ========== Step 1 ==========\n","    # Gather activations on x_orig and x_new\n","\n","    # Note the use of names_filter for the run_with_cache function. Using it means we\n","    # only cache the things we need (in this case, just attn head outputs).\n","    z_name_filter = lambda name: name.endswith((\"z\", \"mlp_out\"))\n","\n","    _, new_cache = model.run_with_cache(\n","        new_dataset.toks,\n","        names_filter=z_name_filter,\n","        return_type=None\n","    )\n","\n","    _, orig_cache = model.run_with_cache(\n","        orig_dataset.toks,\n","        names_filter=z_name_filter,\n","        return_type=None\n","    )\n","\n","    # Looping over every possible sender head (the receiver is always the final resid_post)\n","    # Note use of itertools (gives us a smoother progress bar)\n","    # for (sender_layer, sender_head) in tqdm(list(itertools.product(range(model.cfg.n_layers), range(model.cfg.n_heads)))):\n","\n","    # for (sender_layer, sender_head) in tqdm(circuit):\n","    for sender_layer in mlp_circuit:\n","\n","        # ========== Step 2 ==========\n","        # Run on x_orig, with sender head patched from x_new, every other head frozen\n","\n","        hook_fn = partial(\n","            # patch_or_freeze_head_vectors,\n","            patch_or_freeze_mlp_vectors,\n","            new_cache=new_cache,\n","            orig_cache=orig_cache,\n","            # head_to_patch=(sender_layer, sender_head),\n","            layer_to_patch = sender_layer # an int\n","        )\n","        model.add_hook(z_name_filter, hook_fn)\n","\n","        _, patched_cache = model.run_with_cache(\n","            orig_dataset.toks,\n","            names_filter=resid_post_name_filter,\n","            return_type=None\n","        )\n","\n","        assert set(patched_cache.keys()) == {resid_post_hook_name}\n","\n","        # ========== Step 3 ==========\n","        # Unembed the final residual stream value, to get our patched logits\n","\n","        patched_logits = model.unembed(model.ln_final(patched_cache[resid_post_hook_name]))\n","\n","        # Save the results\n","        # results[sender_layer, sender_head] = patching_metric(patched_logits)\n","        results[sender_layer] = patching_metric(patched_logits)\n","\n","    return results"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"oSbfi1kPZqI6","executionInfo":{"status":"ok","timestamp":1702609552329,"user_tz":300,"elapsed":14181,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["model.reset_hooks()\n","model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","path_patch_mlp_to_final_resid_post = get_path_patch_mlp_to_final_resid_post(mlps_not_ablate, model, ioi_metric_3)"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENP8nod0ZqI7","executionInfo":{"status":"ok","timestamp":1702609552329,"user_tz":300,"elapsed":12,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"670cfe4f-d3a4-4a41-c889-718215cea943"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([12])"]},"metadata":{},"execution_count":55}],"source":["path_patch_mlp_to_final_resid_post.size()"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"idRsYRx2ZqI7","executionInfo":{"status":"ok","timestamp":1702609552329,"user_tz":300,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["mlps_to_resid = {}\n","result = path_patch_mlp_to_final_resid_post\n","filtered_indices = (result < 0.8) & (result != 0.0)\n","filtered_indices = filtered_indices.nonzero(as_tuple=True)[0]\n","mlps_to_resid['resid'] = filtered_indices.tolist()"]},{"cell_type":"markdown","metadata":{"id":"o8sGuo1wb10E"},"source":["# filter out nodes with no ingoing edges"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"L5BLSrZAb5sX","executionInfo":{"status":"ok","timestamp":1702609607147,"user_tz":300,"elapsed":347,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["head_to_head_adjList = {node: neighbors for node, neighbors in head_to_head_adjList.items() if neighbors}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXuZqTiFcL0i","executionInfo":{"status":"aborted","timestamp":1702608620959,"user_tz":300,"elapsed":0,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["mlp_to_head_adjList = {node: neighbors for node, neighbors in mlp_to_head_adjList.items() if neighbors}"]},{"cell_type":"code","source":["mlp_to_head_adjList = {node: neighbors for node, neighbors in mlp_to_head_adjList.items() if neighbors}"],"metadata":{"id":"stVWSCRRcPAq","executionInfo":{"status":"ok","timestamp":1702609615410,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Scg7MWoGmkFk"},"source":["# save graph files"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"NIJrXZOupjVx","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1702609618843,"user_tz":300,"elapsed":367,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"257d35d2-7089-4d30-bcfd-224656b376af"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_6f80960f-7e15-4ec4-b947-58229c4187c6\", \"months_head_to_head_adjList.pkl\", 560)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f91664c3-16c2-4a62-805d-bb1104cfec18\", \"months_mlp_to_mlp_adjList.pkl\", 161)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_4026da61-0943-4fbc-be18-ee9c547f44e0\", \"months_head_to_mlp_adjList.pkl\", 219)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f95bea3f-b2cd-4cc6-965e-6d15066e0b54\", \"months_mlp_to_head_adjList.pkl\", 354)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_1632af50-30fe-4487-97a1-de2f67ffbb1b\", \"months_heads_to_resid.pkl\", 75)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_32b720c1-b222-4e66-9fc2-b527eecb0401\", \"months_mlps_to_resid.pkl\", 47)"]},"metadata":{}}],"source":["# import pickle\n","\n","# task = \"numerals\"\n","\n","with open(task + \"_head_to_head_adjList.pkl\", \"wb\") as file:\n","    pickle.dump(head_to_head_adjList, file)\n","files.download(task + \"_head_to_head_adjList.pkl\")\n","\n","with open(task + \"_mlp_to_mlp_adjList.pkl\", \"wb\") as file:\n","    pickle.dump(mlp_to_mlp_adjList, file)\n","files.download(task + \"_mlp_to_mlp_adjList.pkl\")\n","\n","with open(task + \"_head_to_mlp_adjList.pkl\", \"wb\") as file:\n","    pickle.dump(head_to_mlp_adjList, file)\n","files.download(task + \"_head_to_mlp_adjList.pkl\")\n","\n","with open(task + \"_mlp_to_head_adjList.pkl\", \"wb\") as file:\n","    pickle.dump(mlp_to_head_adjList, file)\n","files.download(task + \"_mlp_to_head_adjList.pkl\")\n","\n","with open(task + \"_heads_to_resid.pkl\", \"wb\") as file:\n","    pickle.dump(heads_to_resid, file)\n","files.download(task + \"_heads_to_resid.pkl\")\n","\n","with open(task + \"_mlps_to_resid.pkl\", \"wb\") as file:\n","    pickle.dump(mlps_to_resid, file)\n","files.download(task + \"_mlps_to_resid.pkl\")"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"clO-UbgvpENB","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1702609627607,"user_tz":300,"elapsed":376,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d9a31a78-8834-4d87-b19e-45bf35a5646b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_b1d2a13c-275b-4f57-ad00-133657da4fb8\", \"months_heads_to_resid_results.pkl\", 980)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_1de9e4e0-1685-4f2c-9353-d1630fab5dd6\", \"months_mlps_to_resid_results.pkl\", 448)"]},"metadata":{}}],"source":["with open(task + \"_heads_to_resid_results.pkl\", \"wb\") as file:\n","    pickle.dump(path_patch_head_to_final_resid_post, file)\n","files.download(task + \"_heads_to_resid_results.pkl\")\n","\n","with open(task + \"_mlps_to_resid_results.pkl\", \"wb\") as file:\n","    pickle.dump(path_patch_mlp_to_final_resid_post, file)\n","files.download(task + \"_mlps_to_resid_results.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"qNC5dz11VcRG"},"source":["# graph plot"]},{"cell_type":"markdown","metadata":{"id":"ST-TgrqgVgqT"},"source":["## plot qkv"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"WNWW7NhPVgqT","executionInfo":{"status":"ok","timestamp":1702609880833,"user_tz":300,"elapsed":348,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["from graphviz import Digraph, Source\n","from IPython.display import display\n","from google.colab import files\n","\n","def plot_graph_adjacency_qkv(head_to_head_adjList, mlp_to_mlp_adjList, head_to_mlp_adjList,\n","                             mlp_to_head_adjList, heads_to_resid, mlps_to_resid,\n","                             filename=\"circuit_graph\", highlighted_nodes=None):\n","    dot = Digraph()\n","    dot.attr(ranksep='0.45', nodesep='0.11')  # vert height- ranksep, nodesep- w\n","\n","    dot.node('resid_post', color=\"#FDDC5C\", style='filled')\n","\n","    for node in mlp_to_mlp_adjList.keys():\n","        sender_name = \"MLP \" + str(node)\n","        dot.node(sender_name, color=\"#FDDC5C\", style='filled')\n","\n","    for node in head_to_head_adjList.keys():\n","        sender_name = f\"{node[0]} , {node[1]} {node[2]}\"\n","        dot.node(sender_name, color=\"#FDDC5C\", style='filled')\n","        sender_name = f\"{node[0]} , {node[1]}\"\n","        dot.node(sender_name, color=\"#FDDC5C\", style='filled')\n","\n","    edges_added = []\n","    # for every q k v node, plot an edge to output node\n","    for node in head_to_head_adjList.keys():\n","        sender_name = f\"{node[0]} , {node[1]} {node[2]}\"\n","        receiver_name = f\"{node[0]} , {node[1]}\"\n","        dot.edge(sender_name, receiver_name, color = '#D4AF37')\n","        edges_added.append((sender_name, receiver_name))\n","\n","    for node in mlp_to_head_adjList.keys():\n","        sender_name = f\"{node[0]} , {node[1]} {node[2]}\"\n","        dot.node(sender_name, color=\"#FDDC5C\", style='filled')\n","        sender_name = f\"{node[0]} , {node[1]}\"\n","        dot.node(sender_name, color=\"#FDDC5C\", style='filled')\n","\n","    # for every q k v node, plot an edge to output node\n","    for node in mlp_to_head_adjList.keys():\n","        sender_name = f\"{node[0]} , {node[1]} {node[2]}\"\n","        receiver_name = f\"{node[0]} , {node[1]}\"\n","        if (sender_name, receiver_name) not in edges_added:\n","            dot.edge(sender_name, receiver_name, color = '#D4AF37')\n","\n","    def loop_adjList(adjList):\n","        for end_node, start_nodes_list in adjList.items():\n","            if isinstance(end_node, int):\n","                receiver_name = \"MLP \" + str(end_node)\n","            elif isinstance(end_node, tuple):\n","                if len(end_node) == 3:\n","                    receiver_name = f\"{end_node[0]} , {end_node[1]} {end_node[2]}\"\n","                elif len(end_node) == 2:\n","                    receiver_name = f\"{end_node[0]} , {end_node[1]}\"\n","            else:\n","                receiver_name = 'resid_post'\n","            for start in start_nodes_list:\n","                if isinstance(start, int):\n","                    sender_name = \"MLP \" + str(start)\n","                elif isinstance(start, tuple):\n","                    if len(start) == 3:\n","                        sender_name = f\"{start[0]} , {start[1]} {start[2]}\"\n","                    elif len(start) == 2:\n","                        sender_name = f\"{start[0]} , {start[1]}\"\n","                dot.node(sender_name, color=\"#FDDC5C\", style='filled')\n","                dot.node(receiver_name, color=\"#FDDC5C\", style='filled')\n","                dot.edge(sender_name, receiver_name, color = '#D4AF37')\n","\n","    loop_adjList(head_to_head_adjList)\n","    loop_adjList(mlp_to_mlp_adjList)\n","    loop_adjList(head_to_mlp_adjList)\n","    loop_adjList(mlp_to_head_adjList)\n","    loop_adjList(heads_to_resid)\n","    loop_adjList(mlps_to_resid)\n","\n","    # Display the graph in Colab\n","    # display(Source(dot.source))\n","\n","    # Save the graph to a file\n","    dot.format = 'png'  # You can change this to 'pdf', 'png', etc. based on your needs\n","    dot.render(filename)\n","    files.download(filename + \".png\")"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"aS-TFDTOVgqU","executionInfo":{"status":"ok","timestamp":1702609881510,"user_tz":300,"elapsed":339,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"78168031-30c8-4edb-a7a8-3192abbbb8da"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_8386396b-f903-4159-86b0-bbb9c94d6fbc\", \"months_qkv_circ.png\", 1037743)"]},"metadata":{}}],"source":["plot_graph_adjacency_qkv(head_to_head_adjList, mlp_to_mlp_adjList, head_to_mlp_adjList,\n","                         mlp_to_head_adjList, heads_to_resid, mlps_to_resid, filename= task+\"_qkv_circ\")"]},{"cell_type":"markdown","metadata":{"id":"5zQfhV5QVgqU"},"source":["## rewrite no qkv fn"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"4Cg85tfNVgqU","executionInfo":{"status":"ok","timestamp":1702609822557,"user_tz":300,"elapsed":5,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["def plot_graph_adjacency(head_to_head_adjList, mlp_to_mlp_adjList, head_to_mlp_adjList,\n","                             mlp_to_head_adjList, heads_to_resid, mlps_to_resid,\n","                             filename=\"circuit_graph\", highlighted_nodes=None):\n","    dot = Digraph()\n","    dot.attr(ranksep='0.45', nodesep='0.11')  # vert height- ranksep, nodesep- w\n","\n","    edges_added = [] # do this bc when no qkv, multiple edges\n","    def loop_adjList(adjList):\n","        for end_node, start_nodes_list in adjList.items():\n","            if isinstance(end_node, int):\n","                receiver_name = \"MLP \" + str(end_node)\n","            elif isinstance(end_node, tuple):\n","                receiver_name = f\"{end_node[0]} , {end_node[1]}\"\n","            else:\n","                receiver_name = 'resid_post'\n","            for start in start_nodes_list:\n","                if isinstance(start, int):\n","                    sender_name = \"MLP \" + str(start)\n","                elif isinstance(start, tuple):\n","                    sender_name = f\"{start[0]} , {start[1]}\"\n","                dot.node(sender_name, color=\"#FDDC5C\", style='filled')\n","                dot.node(receiver_name, color=\"#FDDC5C\", style='filled')\n","                if (sender_name, receiver_name) not in edges_added:\n","                    dot.edge(sender_name, receiver_name, color = '#D4AF37')\n","                    edges_added.append((sender_name, receiver_name))\n","\n","    loop_adjList(head_to_head_adjList)\n","    loop_adjList(mlp_to_mlp_adjList)\n","    loop_adjList(head_to_mlp_adjList)\n","    loop_adjList(mlp_to_head_adjList)\n","    loop_adjList(heads_to_resid)\n","    loop_adjList(mlps_to_resid)\n","\n","    # Save the graph to a file\n","    dot.format = 'png'  # You can change this to 'pdf', 'png', etc. based on your needs\n","    dot.render(filename)\n","    files.download(filename + \".png\")"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"jlB5soNZVgqU","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1702609822558,"user_tz":300,"elapsed":6,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6e397604-ce18-418a-afb2-f02e865c0ad3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f1bb1e19-e5e4-48c0-a971-0819b4abe912\", \"months_no_qkv_circ.png\", 618351)"]},"metadata":{}}],"source":["plot_graph_adjacency(head_to_head_adjList, mlp_to_mlp_adjList, head_to_mlp_adjList,\n","                         mlp_to_head_adjList, heads_to_resid, mlps_to_resid, filename=task+\"_no_qkv_circ\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["5DKgJ54Wl6mA","ql-mdgT_xyUR","6Fuq8XW770vX","OL0vNCqgD3m7","DlpH0Wib-v1j","GeskwYZbOY_z","M_b5txIwj7Ql","no8gv3Hgka8Y"],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMUyM6rkVikGIOhpkMDxLVQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ddf17b46ef08437780747c6e692542af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40d39c6909a7438f81e1b2acf60d99d8","IPY_MODEL_b3713c4a5bf54d2ab281af92970c2435","IPY_MODEL_76209a28af0640c5a705132a3e4ac2cc"],"layout":"IPY_MODEL_33288baeb5eb40da8403e6cae5375a33"}},"40d39c6909a7438f81e1b2acf60d99d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_435679452a2a40769705eb2cf94ef7cb","placeholder":"​","style":"IPY_MODEL_7290fe2c394a48f0adafb7efee6cf3af","value":"100%"}},"b3713c4a5bf54d2ab281af92970c2435":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6d93c6ed134484688caccecd67ec95f","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0fbd267bc6c40e68bbdd2abbc45c0bb","value":12}},"76209a28af0640c5a705132a3e4ac2cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fd2b83f5c654e418c433002da725bad","placeholder":"​","style":"IPY_MODEL_5f387b4074854b9784bc65a862f77c78","value":" 12/12 [00:13&lt;00:00,  1.13s/it]"}},"33288baeb5eb40da8403e6cae5375a33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"435679452a2a40769705eb2cf94ef7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7290fe2c394a48f0adafb7efee6cf3af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6d93c6ed134484688caccecd67ec95f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0fbd267bc6c40e68bbdd2abbc45c0bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4fd2b83f5c654e418c433002da725bad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f387b4074854b9784bc65a862f77c78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80d42110da264366be8b596e5665f78c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aab0b39b236d4f1cb180ccdf79b423a8","IPY_MODEL_ea3cab216e1f4126994422b914e96bd3","IPY_MODEL_394d3c9437f0405d9036b934b46311c6"],"layout":"IPY_MODEL_49990d442ac14306b9efc85a71224b2d"}},"aab0b39b236d4f1cb180ccdf79b423a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e8e9f11e65444a3bca3be5545a81863","placeholder":"​","style":"IPY_MODEL_c9b12698f3e64a9a8d5b21f0d9bc45f9","value":"100%"}},"ea3cab216e1f4126994422b914e96bd3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d24527e8670e467d8539279a1990c17c","max":11,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e2fdcee2be445a9953d7345e53f7bc1","value":11}},"394d3c9437f0405d9036b934b46311c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7200e27d37d54bdcbb0e42f6b18deca4","placeholder":"​","style":"IPY_MODEL_bfd6d1d84e8f409c9e73426bf0e48719","value":" 11/11 [00:12&lt;00:00,  1.13s/it]"}},"49990d442ac14306b9efc85a71224b2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e8e9f11e65444a3bca3be5545a81863":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9b12698f3e64a9a8d5b21f0d9bc45f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d24527e8670e467d8539279a1990c17c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e2fdcee2be445a9953d7345e53f7bc1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7200e27d37d54bdcbb0e42f6b18deca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd6d1d84e8f409c9e73426bf0e48719":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0c6c315f37f4b46a0c362d09b1cf413":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3cc564a6f3f4b7f825b5a265f3aa7c5","IPY_MODEL_47a06b3ed4cd4a21b9045afa2c74dbb8","IPY_MODEL_ce3803b14dff4010b4d1a1aea260b7b4"],"layout":"IPY_MODEL_c43f1fc71a6d4416b426921eff09b138"}},"b3cc564a6f3f4b7f825b5a265f3aa7c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_982ba6a87fd64775b5d33b5bef529c5c","placeholder":"​","style":"IPY_MODEL_1c66c565aadb4d6984c547261540eb06","value":"100%"}},"47a06b3ed4cd4a21b9045afa2c74dbb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04705e78a2e54f42a5aebf8c66f299e5","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_676bf22a7b1f4929b202c954a4ca7e4c","value":10}},"ce3803b14dff4010b4d1a1aea260b7b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80aa1f36153748dd8a231f0caf067972","placeholder":"​","style":"IPY_MODEL_a802bc5742ea44b1842f30f9334254eb","value":" 10/10 [00:11&lt;00:00,  1.13s/it]"}},"c43f1fc71a6d4416b426921eff09b138":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"982ba6a87fd64775b5d33b5bef529c5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c66c565aadb4d6984c547261540eb06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04705e78a2e54f42a5aebf8c66f299e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"676bf22a7b1f4929b202c954a4ca7e4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80aa1f36153748dd8a231f0caf067972":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a802bc5742ea44b1842f30f9334254eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e71c955560064ebda02a3e582533bf92":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74edca15317a4e538f223c09d9134f67","IPY_MODEL_5195ed426e4045949b89122f28a94b30","IPY_MODEL_13dfd24075f54d13a294ce55b294def6"],"layout":"IPY_MODEL_89f7b1a191d042b286dc280b26867c08"}},"74edca15317a4e538f223c09d9134f67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_855ac33055f8432a9be0507f63eba63d","placeholder":"​","style":"IPY_MODEL_57723a5ed31e4061bb62aaff84a2f958","value":"100%"}},"5195ed426e4045949b89122f28a94b30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a015da68eb164af28224bd8905d0243b","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8c3c68857704b14b4691086224563cd","value":6}},"13dfd24075f54d13a294ce55b294def6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0f3d3d76ddb4a5b85dba5c11ce4faa2","placeholder":"​","style":"IPY_MODEL_21ecefd6fec4401ea61aecf3b7d5414d","value":" 6/6 [00:06&lt;00:00,  1.13s/it]"}},"89f7b1a191d042b286dc280b26867c08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855ac33055f8432a9be0507f63eba63d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57723a5ed31e4061bb62aaff84a2f958":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a015da68eb164af28224bd8905d0243b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8c3c68857704b14b4691086224563cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0f3d3d76ddb4a5b85dba5c11ce4faa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21ecefd6fec4401ea61aecf3b7d5414d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b4dd6be5b234f1b89477d43a1c64650":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5b247b66f6245dc808b992faf550c8e","IPY_MODEL_5800b1f48a914b488eaf1bc60859b767","IPY_MODEL_da54b990b17948b8b775b121cfdf5e4c"],"layout":"IPY_MODEL_c6a426e789e84ca4ac9aa0b1e79ee17d"}},"b5b247b66f6245dc808b992faf550c8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9818aff1a4514116b4d3776d2daed405","placeholder":"​","style":"IPY_MODEL_ab689fc27f3d4d58b3f4b258949a63e8","value":"100%"}},"5800b1f48a914b488eaf1bc60859b767":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1922410d082a46d89a29d15f2cd79549","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc8cb09984d544cd85971644fbf54630","value":3}},"da54b990b17948b8b775b121cfdf5e4c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01861517d23e45b28222120a809210fc","placeholder":"​","style":"IPY_MODEL_906c3fe99bc347339e8b7a04eb75f255","value":" 3/3 [00:03&lt;00:00,  1.14s/it]"}},"c6a426e789e84ca4ac9aa0b1e79ee17d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9818aff1a4514116b4d3776d2daed405":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab689fc27f3d4d58b3f4b258949a63e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1922410d082a46d89a29d15f2cd79549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc8cb09984d544cd85971644fbf54630":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01861517d23e45b28222120a809210fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"906c3fe99bc347339e8b7a04eb75f255":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc0475f4cdb84edb8a71d69dd208ac18":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e74a190c1f1a41e68e27625779815961","IPY_MODEL_3445ab93f28447d8b240866612b4dedf","IPY_MODEL_835baa878fa545dea5eb730e6e97eb87"],"layout":"IPY_MODEL_38022751f3dc4683801441f87a8bf932"}},"e74a190c1f1a41e68e27625779815961":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed9a165b782a404d9e3077cda791b5c2","placeholder":"​","style":"IPY_MODEL_f5399421f7e74006870e091e81b7e094","value":"100%"}},"3445ab93f28447d8b240866612b4dedf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b084fc1cea940f18e2e032f304b4f17","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6eb3dfc16b03425ea6a4c3c94d8085c7","value":2}},"835baa878fa545dea5eb730e6e97eb87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80ce5abff90c4683bdd2d5c35ea33581","placeholder":"​","style":"IPY_MODEL_996610ccb764470fb90b439953384319","value":" 2/2 [00:02&lt;00:00,  1.14s/it]"}},"38022751f3dc4683801441f87a8bf932":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed9a165b782a404d9e3077cda791b5c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5399421f7e74006870e091e81b7e094":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b084fc1cea940f18e2e032f304b4f17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6eb3dfc16b03425ea6a4c3c94d8085c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80ce5abff90c4683bdd2d5c35ea33581":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"996610ccb764470fb90b439953384319":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e0b644b0cbc45f69135a88f3dd0d58a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5164bf6530c542a3a0ee41522d09843d","IPY_MODEL_f5c4266043a0419bbdc31ad9cd98b59e","IPY_MODEL_86a63c266dfd498fb69ac01230490c65"],"layout":"IPY_MODEL_a089dcb94e944aa3b674d684e7c575a6"}},"5164bf6530c542a3a0ee41522d09843d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_babe66d1770a45dc8704148f019465f9","placeholder":"​","style":"IPY_MODEL_ad8edce31ca74eeba414b216e1bd4bb4","value":"100%"}},"f5c4266043a0419bbdc31ad9cd98b59e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e75c286c6c447deb283995ffcebce09","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35210c2981c94fc4be2a084dc7afda85","value":2}},"86a63c266dfd498fb69ac01230490c65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8101ed895ce4014a1a657e00033ab86","placeholder":"​","style":"IPY_MODEL_d0c344551b074d409f34e3fa94a3b257","value":" 2/2 [00:02&lt;00:00,  1.14s/it]"}},"a089dcb94e944aa3b674d684e7c575a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"babe66d1770a45dc8704148f019465f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad8edce31ca74eeba414b216e1bd4bb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e75c286c6c447deb283995ffcebce09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35210c2981c94fc4be2a084dc7afda85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8101ed895ce4014a1a657e00033ab86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0c344551b074d409f34e3fa94a3b257":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4798a6ef29294e62bbae65b644718ace":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_685390301460444e8715d3778d56a417","IPY_MODEL_66ac26063aee439eb46d0b3669e39a9c","IPY_MODEL_051d16cedb2b4665953e42115d1c6ff8"],"layout":"IPY_MODEL_ef90002373974e5181939b1ca45a61ce"}},"685390301460444e8715d3778d56a417":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eba66b20bda944c09206177671f769ff","placeholder":"​","style":"IPY_MODEL_41230bcbdfca4480a348e83292a4b83c","value":"100%"}},"66ac26063aee439eb46d0b3669e39a9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_baa45256d56a46bebca8968a434a8616","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_027afce0fbe54c809dbbba5189476f8e","value":2}},"051d16cedb2b4665953e42115d1c6ff8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee17ca8154414cad99521cfe1b6df477","placeholder":"​","style":"IPY_MODEL_0f20adceff4d42cabe8a3b3218e63c54","value":" 2/2 [00:02&lt;00:00,  1.14s/it]"}},"ef90002373974e5181939b1ca45a61ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eba66b20bda944c09206177671f769ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41230bcbdfca4480a348e83292a4b83c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"baa45256d56a46bebca8968a434a8616":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"027afce0fbe54c809dbbba5189476f8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee17ca8154414cad99521cfe1b6df477":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f20adceff4d42cabe8a3b3218e63c54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53640e4614e2410992b159521eb774d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ea8af47110248fa89a35dd20ccdcf3e","IPY_MODEL_a2368c2341634d7ba66e3fc8a615870a","IPY_MODEL_c41225f87c884cbc85bfe9e59c11256e"],"layout":"IPY_MODEL_8e675c2305f04760853ef857a50e0619"}},"2ea8af47110248fa89a35dd20ccdcf3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8036c52b89674149b1e15d133049d61a","placeholder":"​","style":"IPY_MODEL_b2772e54e8844a8db70d12a65f558cbd","value":"100%"}},"a2368c2341634d7ba66e3fc8a615870a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_239eff92e8944aa691f819ab7a431c33","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_550d98b276ea4600a046b2c198495aba","value":2}},"c41225f87c884cbc85bfe9e59c11256e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1cf54276ccf4d91a4b1738530eec5e8","placeholder":"​","style":"IPY_MODEL_39f95c82337042f683af928b51219727","value":" 2/2 [00:02&lt;00:00,  1.14s/it]"}},"8e675c2305f04760853ef857a50e0619":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8036c52b89674149b1e15d133049d61a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2772e54e8844a8db70d12a65f558cbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"239eff92e8944aa691f819ab7a431c33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"550d98b276ea4600a046b2c198495aba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1cf54276ccf4d91a4b1738530eec5e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f95c82337042f683af928b51219727":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90b47409a516457ea3c633d2f18e93c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_099d3601e0724fbf85b987b0ec1666fb","IPY_MODEL_97c3a43ed0fe4eda867d46c2e434f364","IPY_MODEL_597bf770b6a0468bbaf55c1c334451a1"],"layout":"IPY_MODEL_10ea699d58d24e71ae567782878c3efc"}},"099d3601e0724fbf85b987b0ec1666fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e29debc353e64a9586be738acea802d6","placeholder":"​","style":"IPY_MODEL_3d800229db944a2b85878930e80df654","value":""}},"97c3a43ed0fe4eda867d46c2e434f364":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18c529152b3f467d9441dfaaf203e295","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d35c5066016458588bccea11f67afbf","value":0}},"597bf770b6a0468bbaf55c1c334451a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84e52143716d4af08e747912136c78af","placeholder":"​","style":"IPY_MODEL_a27de478ab2e4bb4ba44f26e2ef1a3a7","value":" 0/0 [00:00&lt;?, ?it/s]"}},"10ea699d58d24e71ae567782878c3efc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e29debc353e64a9586be738acea802d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d800229db944a2b85878930e80df654":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18c529152b3f467d9441dfaaf203e295":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7d35c5066016458588bccea11f67afbf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84e52143716d4af08e747912136c78af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a27de478ab2e4bb4ba44f26e2ef1a3a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c951132b65724628890b546190bd0f8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f8dc6c63d394de08e22ee453b95ee41","IPY_MODEL_29feaadd8f734accb73f3d080713a90c","IPY_MODEL_0770d56feccf4ea4b53752be64ebbe2e"],"layout":"IPY_MODEL_373f5d9558594bef9e1123a74b7c4017"}},"2f8dc6c63d394de08e22ee453b95ee41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d3ddeed2ebf44fe964c7df1b611969d","placeholder":"​","style":"IPY_MODEL_6409fbbcaca24cceb76f7c20b52c0ab6","value":"100%"}},"29feaadd8f734accb73f3d080713a90c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aefe0a98d900480c8f08ecff179cd1bc","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c26cbdcfc3584635b74d3dbf4537d35a","value":13}},"0770d56feccf4ea4b53752be64ebbe2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc34d3046cec4f8cad03560e693bc93c","placeholder":"​","style":"IPY_MODEL_86b90b657c8f4d60a92c4aaedf5080e4","value":" 13/13 [00:08&lt;00:00,  1.50it/s]"}},"373f5d9558594bef9e1123a74b7c4017":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d3ddeed2ebf44fe964c7df1b611969d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6409fbbcaca24cceb76f7c20b52c0ab6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aefe0a98d900480c8f08ecff179cd1bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c26cbdcfc3584635b74d3dbf4537d35a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc34d3046cec4f8cad03560e693bc93c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86b90b657c8f4d60a92c4aaedf5080e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}