{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","collapsed_sections":["b3Chees1fkO1","G9FQY3H3zkFV","zkx8xD8dwWOL"],"machine_shape":"hm","toc_visible":true,"authorship_tag":"ABX9TyNrcqbzeidBIhqHMdZ+3+lS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f7a6c278dc1441c589a5482d6fc74a3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc51cd1a05d34af9bf8a58775234fab5","IPY_MODEL_4c6bf9e79e3c4362941fa2cdaeaa85c6","IPY_MODEL_4209780c7fce405b9d686054c4539dbe"],"layout":"IPY_MODEL_9d83e4528bcd4e6a9b7ca42e7234e615"}},"dc51cd1a05d34af9bf8a58775234fab5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_252de5a71f2546ef9738409a4f9efa35","placeholder":"​","style":"IPY_MODEL_ccd47ff45f464f81920ff93e38989b0b","value":"config.json: 100%"}},"4c6bf9e79e3c4362941fa2cdaeaa85c6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62baa868423849efad10d12b431484a8","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_793eb01506354363bd837375c82892ab","value":665}},"4209780c7fce405b9d686054c4539dbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cfbc51204304585be0bfe413e0ccc22","placeholder":"​","style":"IPY_MODEL_7ecafb6603b5483aafc6620a1caf453f","value":" 665/665 [00:00&lt;00:00, 52.3kB/s]"}},"9d83e4528bcd4e6a9b7ca42e7234e615":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"252de5a71f2546ef9738409a4f9efa35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccd47ff45f464f81920ff93e38989b0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62baa868423849efad10d12b431484a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"793eb01506354363bd837375c82892ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cfbc51204304585be0bfe413e0ccc22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ecafb6603b5483aafc6620a1caf453f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f143bdda96e04ba99311b82389e135e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e27fb9157254514aa6b5e57645d61dd","IPY_MODEL_4c107b3278864d60b775ab9e8b9e667c","IPY_MODEL_14fa5e3dae204f128f8c6d39a1f768a2"],"layout":"IPY_MODEL_132b6a4f29cb4d86ac2b15236c101ab3"}},"2e27fb9157254514aa6b5e57645d61dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8a76d0230e4420baf00c785e0ae386d","placeholder":"​","style":"IPY_MODEL_2f0855dd35464a179daf1678c2b4b996","value":"model.safetensors: 100%"}},"4c107b3278864d60b775ab9e8b9e667c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32871329e51e4a0483cb44f49cf4339c","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02bee95828c64d6f97509d84ed1283a8","value":548105171}},"14fa5e3dae204f128f8c6d39a1f768a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7dd2247b3f3471186590a7fcf16e2cb","placeholder":"​","style":"IPY_MODEL_e0c897a7d9994a61abe545ddae505c57","value":" 548M/548M [00:02&lt;00:00, 279MB/s]"}},"132b6a4f29cb4d86ac2b15236c101ab3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8a76d0230e4420baf00c785e0ae386d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f0855dd35464a179daf1678c2b4b996":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32871329e51e4a0483cb44f49cf4339c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02bee95828c64d6f97509d84ed1283a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7dd2247b3f3471186590a7fcf16e2cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0c897a7d9994a61abe545ddae505c57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"719f86ca01074faf8e8ee98dbc1810c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a8604663e9c4bc9b5203c66e04409e6","IPY_MODEL_43995f5311244baf9d2ac7f52c0e93a3","IPY_MODEL_4ebcbcfdd9604b8692b3876e17264f12"],"layout":"IPY_MODEL_719a89531a314392bc61932dd7e4ec38"}},"8a8604663e9c4bc9b5203c66e04409e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10723e849fb94122af4162e8ba8ac1f4","placeholder":"​","style":"IPY_MODEL_b180447b02dc4b9292bab0869a633205","value":"generation_config.json: 100%"}},"43995f5311244baf9d2ac7f52c0e93a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ddd58a9d6984c099bfc84b1d34ca38c","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b9de209a282474f921dac95d0fa7b59","value":124}},"4ebcbcfdd9604b8692b3876e17264f12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59b3c7c74138472394af3550cfcaaef2","placeholder":"​","style":"IPY_MODEL_debf465377f741bcb5352382c7775024","value":" 124/124 [00:00&lt;00:00, 11.8kB/s]"}},"719a89531a314392bc61932dd7e4ec38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10723e849fb94122af4162e8ba8ac1f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b180447b02dc4b9292bab0869a633205":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ddd58a9d6984c099bfc84b1d34ca38c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b9de209a282474f921dac95d0fa7b59":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59b3c7c74138472394af3550cfcaaef2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"debf465377f741bcb5352382c7775024":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3696b0b5208d4f21a08729a865b470f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34503c3282b847ae8d1a5f04ae9189e0","IPY_MODEL_336b8b2ccb4d4648968dd9fe0e97e9e9","IPY_MODEL_13f593f399b64a9f82314e993d7bee78"],"layout":"IPY_MODEL_0dad6e9b66224881b4f203512f8765d3"}},"34503c3282b847ae8d1a5f04ae9189e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03a1dafc304a4542a22fcdff5329c280","placeholder":"​","style":"IPY_MODEL_2393e76612764ca5880d6d5ce5e47abf","value":"vocab.json: 100%"}},"336b8b2ccb4d4648968dd9fe0e97e9e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f869409532ae42629b4849a952730471","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a907176b935a4f3d9c9cb0983b8442f1","value":1042301}},"13f593f399b64a9f82314e993d7bee78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a81d8bf941484c81b3acd174306bacb1","placeholder":"​","style":"IPY_MODEL_368afc15b7a54aff8fc452549edb4cc4","value":" 1.04M/1.04M [00:00&lt;00:00, 1.09MB/s]"}},"0dad6e9b66224881b4f203512f8765d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03a1dafc304a4542a22fcdff5329c280":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2393e76612764ca5880d6d5ce5e47abf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f869409532ae42629b4849a952730471":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a907176b935a4f3d9c9cb0983b8442f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a81d8bf941484c81b3acd174306bacb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"368afc15b7a54aff8fc452549edb4cc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffe41c4d165d44628507110574d5bf5f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e5a95ea9a4f41cda8c22de62b875d66","IPY_MODEL_c3db5a2768684d3e808fdf8ae1ce1558","IPY_MODEL_713df4a14e084d8f9845ed43b7d015cc"],"layout":"IPY_MODEL_3966676e70e14696ba55bdf22736edc7"}},"3e5a95ea9a4f41cda8c22de62b875d66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81743b0f338d447fbb2fe10e89a0f3e9","placeholder":"​","style":"IPY_MODEL_04be0f727d54431fabc8b6e29f2b9574","value":"merges.txt: 100%"}},"c3db5a2768684d3e808fdf8ae1ce1558":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_016d46b1029747e590504987919546b8","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20c5168a05094cadb7a81ba30edd4b0d","value":456318}},"713df4a14e084d8f9845ed43b7d015cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0d56593698544028e2cd578a4e220cb","placeholder":"​","style":"IPY_MODEL_395ab1b5d9ea48aa8e841bdf18889ed8","value":" 456k/456k [00:00&lt;00:00, 636kB/s]"}},"3966676e70e14696ba55bdf22736edc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81743b0f338d447fbb2fe10e89a0f3e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04be0f727d54431fabc8b6e29f2b9574":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"016d46b1029747e590504987919546b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20c5168a05094cadb7a81ba30edd4b0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0d56593698544028e2cd578a4e220cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"395ab1b5d9ea48aa8e841bdf18889ed8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41a2b65a10fd496faecafa7de81349db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae555d2d23ef48c29f22c5cd1438c645","IPY_MODEL_f4fcb4cbe9d14424a536244e2de1fad5","IPY_MODEL_02818dc1222a424183427d2c57afe58a"],"layout":"IPY_MODEL_e438876379f549cba0ec5f8e967a3aa6"}},"ae555d2d23ef48c29f22c5cd1438c645":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14d07f34e41143639b8114d4283c704c","placeholder":"​","style":"IPY_MODEL_df465fccb0fc4d08bef33a2c280a09a7","value":"tokenizer.json: 100%"}},"f4fcb4cbe9d14424a536244e2de1fad5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da892da0793c406c9eef3679b182f32a","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5717c02460954ee9a700fcbd3d973345","value":1355256}},"02818dc1222a424183427d2c57afe58a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b76133478c04a46af7292d4a9695d43","placeholder":"​","style":"IPY_MODEL_e4345b9f89e445dbbf9669d8ef45852f","value":" 1.36M/1.36M [00:00&lt;00:00, 5.38MB/s]"}},"e438876379f549cba0ec5f8e967a3aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14d07f34e41143639b8114d4283c704c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df465fccb0fc4d08bef33a2c280a09a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da892da0793c406c9eef3679b182f32a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5717c02460954ee9a700fcbd3d973345":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b76133478c04a46af7292d4a9695d43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4345b9f89e445dbbf9669d8ef45852f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup\n","(No need to change anything)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"rMcpSDdjIAiA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7ad22c5e-d9fe-4edf-a985-5fe45a7ea2fe","executionInfo":{"status":"ok","timestamp":1702654752944,"user_tz":300,"elapsed":149299,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running as a Colab notebook\n","Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n","  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-nnypmaz4\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-nnypmaz4\n","  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit ce82675a8e89b6d5e6229a89620c843c794f3b04\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate>=0.23.0 (from transformer-lens==0.0.0)\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens==0.0.0)\n","  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n","  Downloading jaxtyping-0.2.24-py3-none-any.whl (38 kB)\n","Collecting numpy>=1.24 (from transformer-lens==0.0.0)\n","  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.7.0)\n","Collecting torch!=2.0,!=2.1.0,>=1.10 (from transformer-lens==0.0.0)\n","  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.35.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.5.0)\n","Collecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n","  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.4.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (10.0.1)\n","Collecting pyarrow-hotfix (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.4.1)\n","Collecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.9.1)\n","Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.11->transformer-lens==0.0.0)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.3.post1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.1.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.1.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.15.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: transformer-lens\n","  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=118964 sha256=a41697fddfb7a0f1f60d43d2eec2ff20c9aa08409a7870e3d636f9e7617edcf1\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6hc7ikso/wheels/8a/1e/37/ffb9c15454a1725b13a9d9f5e74fb91725048884ad734b8c1f\n","Successfully built transformer-lens\n","Installing collected packages: typeguard, smmap, setproctitle, sentry-sdk, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, gitdb, nvidia-cusolver-cu12, GitPython, wandb, torch, datasets, accelerate, transformer-lens\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu121\n","    Uninstalling torch-2.1.0+cu121:\n","      Successfully uninstalled torch-2.1.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.40 accelerate-0.25.0 beartype-0.14.1 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 einops-0.7.0 fancy-einsum-0.0.3 gitdb-4.0.11 jaxtyping-0.2.24 multiprocess-0.70.15 numpy-1.26.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pyarrow-hotfix-0.6 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 torch-2.1.2 transformer-lens-0.0.0 typeguard-2.13.3 wandb-0.16.1\n"]}],"source":["# Janky code to do different setup when run in a Colab notebook vs VSCode\n","DEBUG_MODE = False\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n","    # Install another version of node that makes PySvelte work way faster\n","    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")\n","    from IPython import get_ipython\n","\n","    ipython = get_ipython()\n","    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xKoTs7VBIAiD","executionInfo":{"status":"ok","timestamp":1702654752944,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["# # Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n","# import plotly.io as pio\n","\n","# if IN_COLAB or not DEBUG_MODE:\n","#     # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n","#     pio.renderers.default = \"colab\"\n","# else:\n","#     pio.renderers.default = \"png\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Z6b1n2tvIAiD","executionInfo":{"status":"ok","timestamp":1702654757421,"user_tz":300,"elapsed":4479,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zuhzYxbsIAiE","executionInfo":{"status":"ok","timestamp":1702654758720,"user_tz":300,"elapsed":1310,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["# import pysvelte\n","\n","import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"cFMTUcQiIAiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702654758721,"user_tz":300,"elapsed":5,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"dece5aea-b379-44f3-b4e0-dcfaa8af1155"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7f592f95fac0>"]},"metadata":{},"execution_count":5}],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","metadata":{"id":"zyKb4C51IAiG"},"source":["Plotting helper functions:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KFs9BrbzIAiH","executionInfo":{"status":"ok","timestamp":1702654758721,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["# def imshow(tensor, renderer=None, **kwargs):\n","#     px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n","\n","# def line(tensor, renderer=None, **kwargs):\n","#     px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n","\n","# def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n","#     x = utils.to_numpy(x)\n","#     y = utils.to_numpy(y)\n","#     px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"OLkInsdjyHMx"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"xLwDyosvIAiJ","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["f7a6c278dc1441c589a5482d6fc74a3b","dc51cd1a05d34af9bf8a58775234fab5","4c6bf9e79e3c4362941fa2cdaeaa85c6","4209780c7fce405b9d686054c4539dbe","9d83e4528bcd4e6a9b7ca42e7234e615","252de5a71f2546ef9738409a4f9efa35","ccd47ff45f464f81920ff93e38989b0b","62baa868423849efad10d12b431484a8","793eb01506354363bd837375c82892ab","5cfbc51204304585be0bfe413e0ccc22","7ecafb6603b5483aafc6620a1caf453f","f143bdda96e04ba99311b82389e135e4","2e27fb9157254514aa6b5e57645d61dd","4c107b3278864d60b775ab9e8b9e667c","14fa5e3dae204f128f8c6d39a1f768a2","132b6a4f29cb4d86ac2b15236c101ab3","d8a76d0230e4420baf00c785e0ae386d","2f0855dd35464a179daf1678c2b4b996","32871329e51e4a0483cb44f49cf4339c","02bee95828c64d6f97509d84ed1283a8","d7dd2247b3f3471186590a7fcf16e2cb","e0c897a7d9994a61abe545ddae505c57","719f86ca01074faf8e8ee98dbc1810c5","8a8604663e9c4bc9b5203c66e04409e6","43995f5311244baf9d2ac7f52c0e93a3","4ebcbcfdd9604b8692b3876e17264f12","719a89531a314392bc61932dd7e4ec38","10723e849fb94122af4162e8ba8ac1f4","b180447b02dc4b9292bab0869a633205","5ddd58a9d6984c099bfc84b1d34ca38c","2b9de209a282474f921dac95d0fa7b59","59b3c7c74138472394af3550cfcaaef2","debf465377f741bcb5352382c7775024","3696b0b5208d4f21a08729a865b470f5","34503c3282b847ae8d1a5f04ae9189e0","336b8b2ccb4d4648968dd9fe0e97e9e9","13f593f399b64a9f82314e993d7bee78","0dad6e9b66224881b4f203512f8765d3","03a1dafc304a4542a22fcdff5329c280","2393e76612764ca5880d6d5ce5e47abf","f869409532ae42629b4849a952730471","a907176b935a4f3d9c9cb0983b8442f1","a81d8bf941484c81b3acd174306bacb1","368afc15b7a54aff8fc452549edb4cc4","ffe41c4d165d44628507110574d5bf5f","3e5a95ea9a4f41cda8c22de62b875d66","c3db5a2768684d3e808fdf8ae1ce1558","713df4a14e084d8f9845ed43b7d015cc","3966676e70e14696ba55bdf22736edc7","81743b0f338d447fbb2fe10e89a0f3e9","04be0f727d54431fabc8b6e29f2b9574","016d46b1029747e590504987919546b8","20c5168a05094cadb7a81ba30edd4b0d","d0d56593698544028e2cd578a4e220cb","395ab1b5d9ea48aa8e841bdf18889ed8","41a2b65a10fd496faecafa7de81349db","ae555d2d23ef48c29f22c5cd1438c645","f4fcb4cbe9d14424a536244e2de1fad5","02818dc1222a424183427d2c57afe58a","e438876379f549cba0ec5f8e967a3aa6","14d07f34e41143639b8114d4283c704c","df465fccb0fc4d08bef33a2c280a09a7","da892da0793c406c9eef3679b182f32a","5717c02460954ee9a700fcbd3d973345","5b76133478c04a46af7292d4a9695d43","e4345b9f89e445dbbf9669d8ef45852f"],"height":0},"executionInfo":{"status":"ok","timestamp":1702654772856,"user_tz":300,"elapsed":14138,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"72bb2ea1-f13c-425e-bbc3-952e0b962f13"},"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a6c278dc1441c589a5482d6fc74a3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f143bdda96e04ba99311b82389e135e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719f86ca01074faf8e8ee98dbc1810c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3696b0b5208d4f21a08729a865b470f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe41c4d165d44628507110574d5bf5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a2b65a10fd496faecafa7de81349db"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","source":["## Import functions from repo"],"metadata":{"id":"Z4iJEGh6b56v"}},{"cell_type":"code","source":["!git clone https://github.com/callummcdougall/ARENA_2.0.git"],"metadata":{"id":"Fdh5--MfYw7-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702654786963,"user_tz":300,"elapsed":14120,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9f07e001-d38b-48b5-bf65-b6a640aec946"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ARENA_2.0'...\n","remote: Enumerating objects: 9262, done.\u001b[K\n","remote: Counting objects: 100% (1976/1976), done.\u001b[K\n","remote: Compressing objects: 100% (344/344), done.\u001b[K\n","remote: Total 9262 (delta 1727), reused 1731 (delta 1628), pack-reused 7286\u001b[K\n","Receiving objects: 100% (9262/9262), 156.79 MiB | 16.11 MiB/s, done.\n","Resolving deltas: 100% (5620/5620), done.\n"]}]},{"cell_type":"code","source":["cd ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification"],"metadata":{"id":"iZ4C_bsXZFfj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702654786963,"user_tz":300,"elapsed":14,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d87116af-cf24-492c-eae9-c88be338ccb6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification\n"]}]},{"cell_type":"code","source":["import ioi_circuit_extraction as ioi_circuit_extraction"],"metadata":{"id":"OT0Sn571ZnkV","executionInfo":{"status":"ok","timestamp":1702654786963,"user_tz":300,"elapsed":11,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["1"],"metadata":{"id":"e9-3LxCYCvJ8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702654786964,"user_tz":300,"elapsed":12,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bb40e9c9-85ef-42ba-9ec5-d26e762972f1"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# Generate dataset with multiple prompts"],"metadata":{"id":"6Fuq8XW770vX"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.corr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.incorr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        # for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","        for targ in [key for key in pos_dict]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"4wXBNWj5FwVn","executionInfo":{"status":"ok","timestamp":1702654883483,"user_tz":300,"elapsed":406,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","prompts_list = []\n","\n","temps = ['done', 'lost', 'names']  #\n","\n","for i in temps:\n","    file_name = f'/content/nw_prompts_{i}.pkl'\n","    with open(file_name, 'rb') as file:\n","        filelist = pickle.load(file)\n","\n","    print(filelist[0]['text'])\n","    prompts_list += filelist [:512] #768 512\n","\n","len(prompts_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIe5yXuDhgEK","executionInfo":{"status":"ok","timestamp":1702654883483,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"51b57b67-f72c-4740-87fb-5c9711b30a13"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Van done in one. Hat done in two. Ring done in three. Desk done in four. Sun done in\n","Oil lost in one. Apple lost in two. Tree lost in three. Snow lost in four. Apple lost in\n","Marcus born in one. Victoria born in two. George born in three. Brandon born in four. Jamie born in\n"]},{"output_type":"execute_result","data":{"text/plain":["1536"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# pos_dict = {\n","#     'S1': 4,\n","#     'S2': 10,\n","#     'S3': 16,\n","#     'S4': 22,\n","# }\n","\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","\n","# pos_dict"],"metadata":{"id":"kS_Tlrb_70vg","executionInfo":{"status":"ok","timestamp":1702654883483,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"u0NPSKcZ1iDe","executionInfo":{"status":"ok","timestamp":1702654887158,"user_tz":300,"elapsed":3677,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# import random\n","\n","# def generate_prompts_list_corr(prompt_list):\n","#     outlist = []\n","#     for prompt_dict in prompts_list:\n","#         months = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve']\n","#         r1 = random.choice(months)\n","#         r2 = random.choice(months)\n","#         while True:\n","#             r3_ind = random.randint(0,len(months)-1)\n","#             r4_ind = random.randint(0,len(months)-1)\n","#             if months[r3_ind] != months[r4_ind-1]:\n","#                 break\n","#         r3 = months[r3_ind]\n","#         r4 = months[r4_ind]\n","\n","#         new_text = prompt_dict['text'].replace(\"done\", 'zzz')\n","#         new_text = new_text.replace(prompt_dict['S1'], str(r1)).replace(prompt_dict['S2'], str(r2)).replace(prompt_dict['S3'], str(r3)).replace(prompt_dict['S4'], str(r4))\n","#         new_text = new_text.replace(\"zzz\", 'done')\n","#         new_prompt_dict = {\n","#             'S1': str(r1),\n","#             'S2': str(r2),\n","#             'S3': str(r3),\n","#             'S4': str(r4),\n","#             'corr': prompt_dict['corr'],\n","#             'incorr': prompt_dict['incorr'],\n","#             'text': new_text\n","#         }\n","#         outlist.append(new_prompt_dict)\n","#     return outlist\n","# prompts_list_2 = generate_prompts_list_corr(prompts_list)"],"metadata":{"id":"YqAdblFt1_tx","executionInfo":{"status":"ok","timestamp":1702654887158,"user_tz":300,"elapsed":6,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import pickle\n","task = \"numwords\"\n","file_name = f'/content/randDS_{task}.pkl'\n","with open(file_name, 'rb') as file:\n","    prompts_list_2 = pickle.load(file)"],"metadata":{"id":"rIl4V4sMI3AD","executionInfo":{"status":"ok","timestamp":1702654903002,"user_tz":300,"elapsed":509,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["prompts_list_2[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ry2G9aV9EnEn","executionInfo":{"status":"ok","timestamp":1702654905583,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f8fb3bac-5dde-4e35-f48c-d75f8ee726dd"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'S1': 'two',\n","  'S2': 'two',\n","  'S3': 'twelve',\n","  'S4': 'ten',\n","  'corr': ' five',\n","  'incorr': ' four',\n","  'text': 'Van done in two. Hat done in two. Ring done in twelve. Desk done in ten. Sun done in'},\n"," {'S1': 'eight',\n","  'S2': 'six',\n","  'S3': 'eight',\n","  'S4': 'five',\n","  'corr': ' six',\n","  'incorr': ' five',\n","  'text': 'Ice done in eight. Snow done in six. Watch done in eight. Sun done in five. Table done in'},\n"," {'S1': 'ten',\n","  'S2': 'eleven',\n","  'S3': 'two',\n","  'S4': 'nine',\n","  'corr': ' seven',\n","  'incorr': ' six',\n","  'text': 'Ring done in ten. Moon done in eleven. Queen done in two. Book done in nine. Rose done in'},\n"," {'S1': 'one',\n","  'S2': 'three',\n","  'S3': 'two',\n","  'S4': 'four',\n","  'corr': ' eight',\n","  'incorr': ' seven',\n","  'text': 'Queen done in one. Oil done in three. Rose done in two. Desk done in four. Car done in'},\n"," {'S1': 'two',\n","  'S2': 'eleven',\n","  'S3': 'one',\n","  'S4': 'twelve',\n","  'corr': ' nine',\n","  'incorr': ' eight',\n","  'text': 'Light done in two. Arm done in eleven. Road done in one. Book done in twelve. Ice done in'},\n"," {'S1': 'one',\n","  'S2': 'one',\n","  'S3': 'four',\n","  'S4': 'three',\n","  'corr': ' ten',\n","  'incorr': ' nine',\n","  'text': 'Ball done in one. Cow done in one. Book done in four. Rose done in three. Key done in'},\n"," {'S1': 'three',\n","  'S2': 'three',\n","  'S3': 'five',\n","  'S4': 'five',\n","  'corr': ' eleven',\n","  'incorr': ' ten',\n","  'text': 'Road done in three. Key done in three. Ocean done in five. Key done in five. Queen done in'},\n"," {'S1': 'four',\n","  'S2': 'one',\n","  'S3': 'eight',\n","  'S4': 'eight',\n","  'corr': ' twelve',\n","  'incorr': ' eleven',\n","  'text': 'House done in four. Rose done in one. Key done in eight. Hat done in eight. Van done in'},\n"," {'S1': 'five',\n","  'S2': 'eight',\n","  'S3': 'one',\n","  'S4': 'four',\n","  'corr': ' five',\n","  'incorr': ' four',\n","  'text': 'Ring done in five. Car done in eight. Apple done in one. Pear done in four. Moon done in'},\n"," {'S1': 'one',\n","  'S2': 'eleven',\n","  'S3': 'two',\n","  'S4': 'six',\n","  'corr': ' six',\n","  'incorr': ' five',\n","  'text': 'Star done in one. Sun done in eleven. Road done in two. Queen done in six. Box done in'}]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# import pickle\n","# from google.colab import files\n","\n","# with open('randDS_numwords.pkl', 'wb') as file:\n","#     pickle.dump(prompts_list_2, file)\n","# files.download('randDS_numwords.pkl')"],"metadata":{"id":"gTwJYFodxMYd","executionInfo":{"status":"ok","timestamp":1702654907523,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["len(prompts_list_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6sRiXpRY9hL","executionInfo":{"status":"ok","timestamp":1702654907856,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2031a222-80ac-4df6-9bdc-7ed0fa10984d"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1536"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"msu6D4p_feW5","executionInfo":{"status":"ok","timestamp":1702654912258,"user_tz":300,"elapsed":3622,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# get orig score"],"metadata":{"id":"BHHvz84w70vh"}},{"cell_type":"code","source":["from torch import Tensor\n","\n","def logits_to_ave_logit_diff_2(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","    '''\n","    Returns logit difference between the correct and incorrect answer.\n","\n","    If per_prompt=True, return the array of differences rather than the average.\n","    '''\n","\n","    # Only the final logits are relevant for the answer\n","    # Get the logits corresponding to the indirect object / subject tokens respectively\n","    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n","    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n","    # Find logit difference\n","    answer_logit_diff = corr_logits - incorr_logits\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"],"metadata":{"id":"AFYffMoP70vh","executionInfo":{"status":"ok","timestamp":1702654912258,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)\n","# ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","ioi_logits_original = model(dataset.toks)\n","orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)"],"metadata":{"id":"OI3FcmpMaNxB","executionInfo":{"status":"ok","timestamp":1702654913105,"user_tz":300,"elapsed":848,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["import gc\n","\n","# del(ioi_cache)\n","del(ioi_logits_original)\n","\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702654913105,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"A-TjmW5PUwGC","outputId":"661adcd7-c26e-4d11-9fee-84ad45a22f4c"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["# Ablation Expm Functions"],"metadata":{"id":"b3Chees1fkO1"}},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, orig_score, print_output=True):\n","    # CIRCUIT = {\n","    #     \"number mover\": lst,\n","    #     \"number mover 4\": lst,\n","    #     \"number mover 3\": lst,\n","    #     \"number mover 2\": lst,\n","    #     \"number mover 1\": lst,\n","    # }\n","\n","    # SEQ_POS_TO_KEEP = {\n","    #     \"number mover\": \"end\",\n","    #     \"number mover 4\": \"S4\",\n","    #     \"number mover 3\": \"S3\",\n","    #     \"number mover 2\": \"S2\",\n","    #     \"number mover 1\": \"S1\",\n","    # }\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    # del(ioi_logits_minimal)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"QLK5m1Ps70vh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# rmv most impt heads from full"],"metadata":{"id":"w82u8B4EZdWi"}},{"cell_type":"code","source":["circ = [(layer, head) for layer in range(12) for head in range(12)]\n","to_loop = [(layer, head) for layer in range(12) for head in range(12)]\n","#  [(7, 11), (4, 4), (1, 5), (10, 7), (8, 8), (8, 6), (9, 1), (6, 6), (8, 1), (6, 10)]\n","\n","lh_scores = {}\n","# for lh in circ:\n","for lh in to_loop:\n","    copy_circuit = circ.copy()\n","    copy_circuit.remove(lh)\n","    print(\"removed: \" + str(lh))\n","    new_score = mean_ablate_by_lst(copy_circuit, model, orig_score, print_output=True).item()\n","    lh_scores[lh] = new_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b284ac01-051e-43ba-8344-a74190a75196","id":"msckx6kcZgAd","executionInfo":{"status":"ok","timestamp":1702604876560,"user_tz":300,"elapsed":1566214,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["removed: (0, 0)\n","Average logit difference (circuit / full) %: 100.9832\n","removed: (0, 1)\n","Average logit difference (circuit / full) %: 8.1006\n","removed: (0, 2)\n","Average logit difference (circuit / full) %: 100.4444\n","removed: (0, 3)\n","Average logit difference (circuit / full) %: 100.1392\n","removed: (0, 4)\n","Average logit difference (circuit / full) %: 102.1836\n","removed: (0, 5)\n","Average logit difference (circuit / full) %: 94.9820\n","removed: (0, 6)\n","Average logit difference (circuit / full) %: 103.1773\n","removed: (0, 7)\n","Average logit difference (circuit / full) %: 101.7027\n","removed: (0, 8)\n","Average logit difference (circuit / full) %: 100.7235\n","removed: (0, 9)\n","Average logit difference (circuit / full) %: 98.2363\n","removed: (0, 10)\n","Average logit difference (circuit / full) %: 104.2154\n","removed: (0, 11)\n","Average logit difference (circuit / full) %: 99.9846\n","removed: (1, 0)\n","Average logit difference (circuit / full) %: 102.1491\n","removed: (1, 1)\n","Average logit difference (circuit / full) %: 101.0809\n","removed: (1, 2)\n","Average logit difference (circuit / full) %: 99.7917\n","removed: (1, 3)\n","Average logit difference (circuit / full) %: 100.1018\n","removed: (1, 4)\n","Average logit difference (circuit / full) %: 99.6210\n","removed: (1, 5)\n","Average logit difference (circuit / full) %: 61.9710\n","removed: (1, 6)\n","Average logit difference (circuit / full) %: 104.1253\n","removed: (1, 7)\n","Average logit difference (circuit / full) %: 101.4928\n","removed: (1, 8)\n","Average logit difference (circuit / full) %: 99.1779\n","removed: (1, 9)\n","Average logit difference (circuit / full) %: 100.0035\n","removed: (1, 10)\n","Average logit difference (circuit / full) %: 100.2295\n","removed: (1, 11)\n","Average logit difference (circuit / full) %: 83.0106\n","removed: (2, 0)\n","Average logit difference (circuit / full) %: 98.6731\n","removed: (2, 1)\n","Average logit difference (circuit / full) %: 104.4946\n","removed: (2, 2)\n","Average logit difference (circuit / full) %: 101.8862\n","removed: (2, 3)\n","Average logit difference (circuit / full) %: 99.9624\n","removed: (2, 4)\n","Average logit difference (circuit / full) %: 100.7489\n","removed: (2, 5)\n","Average logit difference (circuit / full) %: 99.3046\n","removed: (2, 6)\n","Average logit difference (circuit / full) %: 99.2961\n","removed: (2, 7)\n","Average logit difference (circuit / full) %: 98.6675\n","removed: (2, 8)\n","Average logit difference (circuit / full) %: 101.9185\n","removed: (2, 9)\n","Average logit difference (circuit / full) %: 102.7391\n","removed: (2, 10)\n","Average logit difference (circuit / full) %: 100.2243\n","removed: (2, 11)\n","Average logit difference (circuit / full) %: 99.6377\n","removed: (3, 0)\n","Average logit difference (circuit / full) %: 94.9867\n","removed: (3, 1)\n","Average logit difference (circuit / full) %: 100.2632\n","removed: (3, 2)\n","Average logit difference (circuit / full) %: 99.6854\n","removed: (3, 3)\n","Average logit difference (circuit / full) %: 100.7991\n","removed: (3, 4)\n","Average logit difference (circuit / full) %: 99.4205\n","removed: (3, 5)\n","Average logit difference (circuit / full) %: 101.5247\n","removed: (3, 6)\n","Average logit difference (circuit / full) %: 100.4953\n","removed: (3, 7)\n","Average logit difference (circuit / full) %: 103.9178\n","removed: (3, 8)\n","Average logit difference (circuit / full) %: 99.4610\n","removed: (3, 9)\n","Average logit difference (circuit / full) %: 103.5773\n","removed: (3, 10)\n","Average logit difference (circuit / full) %: 96.6237\n","removed: (3, 11)\n","Average logit difference (circuit / full) %: 101.5965\n","removed: (4, 0)\n","Average logit difference (circuit / full) %: 101.4630\n","removed: (4, 1)\n","Average logit difference (circuit / full) %: 100.4194\n","removed: (4, 2)\n","Average logit difference (circuit / full) %: 100.2819\n","removed: (4, 3)\n","Average logit difference (circuit / full) %: 102.1657\n","removed: (4, 4)\n","Average logit difference (circuit / full) %: 47.9158\n","removed: (4, 5)\n","Average logit difference (circuit / full) %: 100.9502\n","removed: (4, 6)\n","Average logit difference (circuit / full) %: 98.9956\n","removed: (4, 7)\n","Average logit difference (circuit / full) %: 90.7591\n","removed: (4, 8)\n","Average logit difference (circuit / full) %: 98.9281\n","removed: (4, 9)\n","Average logit difference (circuit / full) %: 102.9231\n","removed: (4, 10)\n","Average logit difference (circuit / full) %: 89.0001\n","removed: (4, 11)\n","Average logit difference (circuit / full) %: 100.1763\n","removed: (5, 0)\n","Average logit difference (circuit / full) %: 96.5025\n","removed: (5, 1)\n","Average logit difference (circuit / full) %: 98.6550\n","removed: (5, 2)\n","Average logit difference (circuit / full) %: 100.1752\n","removed: (5, 3)\n","Average logit difference (circuit / full) %: 100.3317\n","removed: (5, 4)\n","Average logit difference (circuit / full) %: 100.2599\n","removed: (5, 5)\n","Average logit difference (circuit / full) %: 103.6487\n","removed: (5, 6)\n","Average logit difference (circuit / full) %: 99.6587\n","removed: (5, 7)\n","Average logit difference (circuit / full) %: 99.7635\n","removed: (5, 8)\n","Average logit difference (circuit / full) %: 98.5937\n","removed: (5, 9)\n","Average logit difference (circuit / full) %: 98.5645\n","removed: (5, 10)\n","Average logit difference (circuit / full) %: 101.2960\n","removed: (5, 11)\n","Average logit difference (circuit / full) %: 98.5013\n","removed: (6, 0)\n","Average logit difference (circuit / full) %: 100.9044\n","removed: (6, 1)\n","Average logit difference (circuit / full) %: 96.8042\n","removed: (6, 2)\n","Average logit difference (circuit / full) %: 100.7579\n","removed: (6, 3)\n","Average logit difference (circuit / full) %: 101.0584\n","removed: (6, 4)\n","Average logit difference (circuit / full) %: 99.3357\n","removed: (6, 5)\n","Average logit difference (circuit / full) %: 99.1882\n","removed: (6, 6)\n","Average logit difference (circuit / full) %: 85.5611\n","removed: (6, 7)\n","Average logit difference (circuit / full) %: 97.8506\n","removed: (6, 8)\n","Average logit difference (circuit / full) %: 100.6490\n","removed: (6, 9)\n","Average logit difference (circuit / full) %: 99.7782\n","removed: (6, 10)\n","Average logit difference (circuit / full) %: 74.9906\n","removed: (6, 11)\n","Average logit difference (circuit / full) %: 100.2628\n","removed: (7, 0)\n","Average logit difference (circuit / full) %: 99.3919\n","removed: (7, 1)\n","Average logit difference (circuit / full) %: 99.8254\n","removed: (7, 2)\n","Average logit difference (circuit / full) %: 96.7570\n","removed: (7, 3)\n","Average logit difference (circuit / full) %: 103.5290\n","removed: (7, 4)\n","Average logit difference (circuit / full) %: 99.4788\n","removed: (7, 5)\n","Average logit difference (circuit / full) %: 98.9887\n","removed: (7, 6)\n","Average logit difference (circuit / full) %: 92.0419\n","removed: (7, 7)\n","Average logit difference (circuit / full) %: 93.8601\n","removed: (7, 8)\n","Average logit difference (circuit / full) %: 100.0607\n","removed: (7, 9)\n","Average logit difference (circuit / full) %: 98.6710\n","removed: (7, 10)\n","Average logit difference (circuit / full) %: 100.2658\n","removed: (7, 11)\n","Average logit difference (circuit / full) %: 38.4908\n","removed: (8, 0)\n","Average logit difference (circuit / full) %: 101.3147\n","removed: (8, 1)\n","Average logit difference (circuit / full) %: 87.6323\n","removed: (8, 2)\n","Average logit difference (circuit / full) %: 100.1698\n","removed: (8, 3)\n","Average logit difference (circuit / full) %: 98.0744\n","removed: (8, 4)\n","Average logit difference (circuit / full) %: 99.8147\n","removed: (8, 5)\n","Average logit difference (circuit / full) %: 99.1161\n","removed: (8, 6)\n","Average logit difference (circuit / full) %: 88.0455\n","removed: (8, 7)\n","Average logit difference (circuit / full) %: 99.2126\n","removed: (8, 8)\n","Average logit difference (circuit / full) %: 85.3152\n","removed: (8, 9)\n","Average logit difference (circuit / full) %: 96.9789\n","removed: (8, 10)\n","Average logit difference (circuit / full) %: 101.1316\n","removed: (8, 11)\n","Average logit difference (circuit / full) %: 93.7067\n","removed: (9, 0)\n","Average logit difference (circuit / full) %: 99.6630\n","removed: (9, 1)\n","Average logit difference (circuit / full) %: 70.0730\n","removed: (9, 2)\n","Average logit difference (circuit / full) %: 99.2319\n","removed: (9, 3)\n","Average logit difference (circuit / full) %: 100.2606\n","removed: (9, 4)\n","Average logit difference (circuit / full) %: 99.3544\n","removed: (9, 5)\n","Average logit difference (circuit / full) %: 95.2661\n","removed: (9, 6)\n","Average logit difference (circuit / full) %: 99.2525\n","removed: (9, 7)\n","Average logit difference (circuit / full) %: 97.0310\n","removed: (9, 8)\n","Average logit difference (circuit / full) %: 99.9169\n","removed: (9, 9)\n","Average logit difference (circuit / full) %: 102.5796\n","removed: (9, 10)\n","Average logit difference (circuit / full) %: 99.8329\n","removed: (9, 11)\n","Average logit difference (circuit / full) %: 98.7421\n","removed: (10, 0)\n","Average logit difference (circuit / full) %: 99.9939\n","removed: (10, 1)\n","Average logit difference (circuit / full) %: 99.1430\n","removed: (10, 2)\n","Average logit difference (circuit / full) %: 106.5835\n","removed: (10, 3)\n","Average logit difference (circuit / full) %: 100.1149\n","removed: (10, 4)\n","Average logit difference (circuit / full) %: 99.5793\n","removed: (10, 5)\n","Average logit difference (circuit / full) %: 100.2779\n","removed: (10, 6)\n","Average logit difference (circuit / full) %: 100.0042\n","removed: (10, 7)\n","Average logit difference (circuit / full) %: 78.8141\n","removed: (10, 8)\n","Average logit difference (circuit / full) %: 99.3501\n","removed: (10, 9)\n","Average logit difference (circuit / full) %: 99.9390\n","removed: (10, 10)\n","Average logit difference (circuit / full) %: 100.3905\n","removed: (10, 11)\n","Average logit difference (circuit / full) %: 99.5404\n","removed: (11, 0)\n","Average logit difference (circuit / full) %: 99.7315\n","removed: (11, 1)\n","Average logit difference (circuit / full) %: 100.1477\n","removed: (11, 2)\n","Average logit difference (circuit / full) %: 99.9832\n","removed: (11, 3)\n","Average logit difference (circuit / full) %: 100.1309\n","removed: (11, 4)\n","Average logit difference (circuit / full) %: 99.6292\n","removed: (11, 5)\n","Average logit difference (circuit / full) %: 100.0339\n","removed: (11, 6)\n","Average logit difference (circuit / full) %: 100.0085\n","removed: (11, 7)\n","Average logit difference (circuit / full) %: 99.9361\n","removed: (11, 8)\n","Average logit difference (circuit / full) %: 99.7353\n","removed: (11, 9)\n","Average logit difference (circuit / full) %: 99.1789\n","removed: (11, 10)\n","Average logit difference (circuit / full) %: 98.8212\n","removed: (11, 11)\n","Average logit difference (circuit / full) %: 100.3020\n"]}]},{"cell_type":"code","source":["# Sort the dictionary by values in descending order\n","sorted_lh_scores = dict(sorted(lh_scores.items(), key=lambda item: -item[1], reverse=True))\n","\n","# Select the top 10 items\n","top_10_lh_scores = dict(list(sorted_lh_scores.items())[:10])\n","top_10_lh_scores"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702604876560,"user_tz":300,"elapsed":19,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"LTNyxnWMZgAd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d4e1188-a649-42db-e82b-d9e7c061485c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(0, 1): 8.100605964660645,\n"," (7, 11): 38.49078369140625,\n"," (4, 4): 47.915802001953125,\n"," (1, 5): 61.97099304199219,\n"," (9, 1): 70.07295227050781,\n"," (6, 10): 74.99057006835938,\n"," (10, 7): 78.8140869140625,\n"," (1, 11): 83.01056671142578,\n"," (8, 8): 85.3151626586914,\n"," (6, 6): 85.56108856201172}"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# for lh, score in sorted_lh_scores.items():\n","#     print(lh, -round(100-score, 2))\n","\n","# Sort the dictionary by values in descending order\n","sorted_lh_scores = sorted(lh_scores.items(), key=lambda item: -item[1], reverse=True)\n","\n","# Iterate over the top 10 items and print them\n","for lh, score in sorted_lh_scores[:10]:\n","    modified_score = -round(100 - score, 2)\n","    print(lh, modified_score)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702604876561,"user_tz":300,"elapsed":6,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"D3E9dzLOZgAe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"feeba893-8704-414a-cf2b-e326034605c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(0, 1) -91.9\n","(7, 11) -61.51\n","(4, 4) -52.08\n","(1, 5) -38.03\n","(9, 1) -29.93\n","(6, 10) -25.01\n","(10, 7) -21.19\n","(1, 11) -16.99\n","(8, 8) -14.68\n","(6, 6) -14.44\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","import matplotlib.pyplot as plt\n","\n","lh_scores_drop = {key: min(0, val-100) for key, val in lh_scores.items()}\n","\n","# Extracting only the values (scores) from the dictionary\n","scores = list(lh_scores_drop.values())\n","\n","# Creating a histogram for the scores\n","plt.hist(scores, bins=10, edgecolor='black')  # Adjust the number of bins as needed\n","\n","# Creating a histogram for the scores\n","n, bins, patches = plt.hist(scores, bins=10, edgecolor='black')  # Adjust the number of bins as needed\n","\n","# Annotating the histogram with the number of values in each bin\n","for i in range(len(n)):\n","    plt.text(bins[i]+5, n[i], str(int(n[i])), va='bottom', ha='center')\n","\n","# Setting x-axis ticks at intervals of 10 from 0 to 100\n","plt.xticks(range(-100, 0, 10))\n","\n","# Adding labels and title for clarity\n","plt.xlabel('Percentage Drop from Full Performance')\n","plt.ylabel('Number of Attention Heads')\n","# plt.title('Distribution of Attention Head Performance Drop Percentages')\n","\n","# Displaying the plot\n","# plt.show()\n","\n","# Save the figure\n","pdf_filename = 'lh_scores_distribution.pdf'\n","plt.savefig(pdf_filename)\n","\n","# Download the file in Colab\n","files.download(pdf_filename)"],"metadata":{"id":"yVAAmsZikKQV","executionInfo":{"status":"error","timestamp":1702607499824,"user_tz":300,"elapsed":385,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"18907989-a065-4256-855f-442cd04203df"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-7a879d02e07b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Creating a histogram for the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust the number of bins as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Creating a histogram for the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vertical'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m         label=None, stacked=False, *, data=None, **kwargs):\n\u001b[0;32m-> 2645\u001b[0;31m     return gca().hist(\n\u001b[0m\u001b[1;32m   2646\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0mcumulative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcumulative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6702\u001b[0m         \u001b[0;31m# Massage 'x' for processing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6703\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reshape_2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6704\u001b[0m         \u001b[0mnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_reshape_2D\u001b[0;34m(X, name)\u001b[0m\n\u001b[1;32m   1397\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m                 \u001b[0mis_1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0mnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnd\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import statistics\n","\n","# Assuming lh_scores_drop is already defined\n","# For example, lh_scores_drop = {key: max(0, 100 - val) for key, val in lh_scores.items()}\n","\n","# Extracting the values from the dictionary\n","scores = list(lh_scores_drop.values())\n","\n","# Calculating the mean\n","mean_score = statistics.mean(scores)\n","\n","print(\"Mean of the scores:\", mean_score)\n"],"metadata":{"id":"A6E_Lbomm1js","executionInfo":{"status":"ok","timestamp":1702604877209,"user_tz":300,"elapsed":11,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"91fc07a4-1846-4581-9c36-a27f82a04e8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean of the scores: -3.4924875034226313\n"]}]},{"cell_type":"code","source":["# def get_probability_ranking(value, distribution):\n","#     # Convert distribution to a probability distribution (if not already)\n","#     total = sum(distribution)\n","#     prob_distribution = [x / total for x in distribution]\n","\n","#     # Sort the probability distribution\n","#     sorted_distribution = sorted(prob_distribution)\n","\n","#     # Calculate the cumulative probability up to the given value\n","#     cumulative_prob = sum(prob for prob in sorted_distribution if prob <= value)\n","\n","#     return cumulative_prob\n","\n","# # Example usage\n","# distribution = lh_scores_drop.values()\n","# value = -0.69  # Value to get the probability ranking for\n","# ranking = get_probability_ranking(value, distribution)\n","# print(f\"Probability Ranking of {value} in the distribution: {ranking}\")\n"],"metadata":{"id":"74LSPw6fg4ru"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1702607507115,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":300},"id":"uYomEea9KMhW","outputId":"62aa9375-3099-476a-fadb-ed5d50047432"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_a32e0263-2756-478d-92e7-efbd14762350\", \"nw_lh_scores.pkl\", 5682)"]},"metadata":{}}],"source":["import pickle\n","\n","# Saving the dictionary to a file using pickle\n","with open('nw_lh_scores.pkl', 'wb') as file:\n","    pickle.dump(lh_scores, file)\n","\n","from google.colab import files\n","\n","# Download the file to your local machine\n","files.download('nw_lh_scores.pkl')"]},{"cell_type":"markdown","source":["# MLP ablation fns"],"metadata":{"id":"G9FQY3H3zkFV"}},{"cell_type":"code","source":["from torch import Tensor\n","from typing import Dict, Tuple, List\n","from jaxtyping import Float, Bool\n","import torch as t\n","\n","def logits_to_ave_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","    '''\n","    Returns logit difference between the correct and incorrect answer.\n","\n","    If per_prompt=True, return the array of differences rather than the average.\n","    '''\n","\n","    # Only the final logits are relevant for the answer\n","    # Get the logits corresponding to the indirect object / subject tokens respectively\n","    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n","    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n","    # Find logit difference\n","    answer_logit_diff = corr_logits - incorr_logits\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"],"metadata":{"id":"ZOTRN8KnheFO","executionInfo":{"status":"ok","timestamp":1702654916287,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def compute_means_by_template_MLP(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Returns the mean of each head's output over the means dataset. This mean is\n","    computed separately for each group of prompts with the same template (these\n","    are given by means_dataset.groups).\n","    '''\n","    # Cache the outputs of every head\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"mlp_out\"),\n","    )\n","    # Create tensor to store means\n","    n_layers, d_model = model.cfg.n_layers, model.cfg.d_model\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, d_model), device=model.cfg.device)\n","\n","    # Get set of different templates for this data\n","    for layer in range(n_layers):\n","        mlp_output_for_this_layer: Float[Tensor, \"batch seq d_model\"] = means_cache[utils.get_act_name(\"mlp_out\", layer)]\n","        for template_group in means_dataset.groups:  # here, we only have one group\n","            mlp_output_for_this_template = mlp_output_for_this_layer[template_group]\n","            # aggregate all batches\n","            mlp_output_means_for_this_template = einops.reduce(mlp_output_for_this_template, \"batch seq d_model -> seq d_model\", \"mean\")\n","            means[layer, template_group] = mlp_output_means_for_this_template\n","            # at layer, each batch ind is tempalte group (a tensor of size seq d_model)\n","            # is assigned the SAME mean, \"mlp_output_means_for_this_template\"\n","\n","    del(means_cache)\n","\n","    return means"],"metadata":{"id":"McmRZoY7Wudl","executionInfo":{"status":"ok","timestamp":1702654916803,"user_tz":300,"elapsed":518,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def get_mlp_outputs_and_posns_to_keep(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[int]],  # Adjusted to hold list of layers instead of (layer, head) tuples\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq\"]]:  # Adjusted the return type to \"batch seq\"\n","    '''\n","    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n","    MLP output which *shouldn't* be mean-ablated.\n","\n","    The output of this function will be used for the hook function that does ablation.\n","    '''\n","    mlp_outputs_and_posns_to_keep = {}\n","    batch, seq = len(means_dataset), means_dataset.max_len\n","\n","    for layer in range(model.cfg.n_layers):\n","        mask = t.zeros(size=(batch, seq))\n","\n","        for (mlp_type, layer_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[mlp_type]\n","            indices = means_dataset.word_idx[seq_pos]\n","            if layer in layer_list:  # Check if the current layer is in the layer list for this mlp_type\n","                mask[:, indices] = 1\n","\n","        mlp_outputs_and_posns_to_keep[layer] = mask.bool()\n","\n","    return mlp_outputs_and_posns_to_keep"],"metadata":{"id":"MH4KI_wCu7M-","executionInfo":{"status":"ok","timestamp":1702654916803,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def hook_fn_mask_mlp_out(\n","    mlp_out: Float[Tensor, \"batch seq d_mlp\"],\n","    hook: HookPoint,\n","    mlp_outputs_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq\"]],\n","    means: Float[Tensor, \"layer batch seq d_mlp\"],\n",") -> Float[Tensor, \"batch seq d_mlp\"]:\n","    '''\n","    Hook function which masks the MLP output of a transformer layer.\n","\n","    mlp_outputs_and_posns_to_keep\n","        Dict created with the get_mlp_outputs_and_posns_to_keep function. This tells\n","        us where to mask.\n","\n","    means\n","        Tensor of mean MLP output values of the means_dataset over each group of prompts\n","        with the same template. This tells us what values to mask with.\n","    '''\n","    # Get the mask for this layer, adapted for MLP output structure\n","    mask_for_this_layer = mlp_outputs_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(mlp_out.device)\n","\n","    # Set MLP output values to the mean where necessary\n","    mlp_out = t.where(mask_for_this_layer, mlp_out, means[hook.layer()])\n","\n","    return mlp_out"],"metadata":{"id":"fXWq7V0Mv0F4","executionInfo":{"status":"ok","timestamp":1702654916803,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["CIRCUIT = {}\n","SEQ_POS_TO_KEEP = {}\n","def add_mean_ablation_hook_MLP(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n","    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Adds a permanent hook to the model, which ablates according to the circuit and\n","    seq_pos_to_keep dictionaries.\n","\n","    In other words, when the model is run on ioi_dataset, every head's output will\n","    be replaced with the mean over means_dataset for sequences with the same template,\n","    except for a subset of heads and sequence positions as specified by the circuit\n","    and seq_pos_to_keep dicts.\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template_MLP(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_mlp_out,\n","        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"],"metadata":{"id":"sJlawX18v-yD","executionInfo":{"status":"ok","timestamp":1702654916804,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=True):\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = add_mean_ablation_hook_MLP(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    new_logits = model(dataset.toks)\n","\n","    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","    del(new_logits)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"Ko6itvH15NtO","executionInfo":{"status":"ok","timestamp":1702654916804,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# loop thru MLPs from full"],"metadata":{"id":"zkx8xD8dwWOL"}},{"cell_type":"code","source":["for i in range(12):\n","    lst = [layer for layer in range(12) if layer != i]\n","    perc_of_orig = mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=False).item()\n","    print(i, perc_of_orig)"],"metadata":{"id":"I9SR5ETh6BWw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702604904058,"user_tz":300,"elapsed":26856,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8554dd5d-cdb9-44ed-c22b-472079aa1b90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 4.0249505043029785\n","1 65.28792572021484\n","2 79.81611633300781\n","3 81.80935668945312\n","4 50.76289749145508\n","5 74.83795166015625\n","6 66.5409164428711\n","7 70.29489135742188\n","8 56.82688522338867\n","9 15.896557807922363\n","10 57.913150787353516\n","11 75.0255355834961\n"]}]},{"cell_type":"markdown","source":["# MLP and Head together fns"],"metadata":{"id":"DlpH0Wib-v1j"}},{"cell_type":"markdown","source":["## head fns"],"metadata":{"id":"Zv5yGHXhpAK_"}},{"cell_type":"code","source":["def get_heads_and_posns_to_keep(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","    '''\n","    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n","    z output which *shouldn't* be mean-ablated.\n","\n","    The output of this function will be used for the hook function that does ablation.\n","    '''\n","    heads_and_posns_to_keep = {}\n","    batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","\n","    for layer in range(model.cfg.n_layers):\n","\n","        mask = t.zeros(size=(batch, seq, n_heads))\n","\n","        for (head_type, head_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[head_type]\n","            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","            for (layer_idx, head_idx) in head_list:\n","                if layer_idx == layer:\n","                    mask[:, indices, head_idx] = 1\n","\n","        heads_and_posns_to_keep[layer] = mask.bool()\n","\n","    return heads_and_posns_to_keep\n","\n","def hook_fn_mask_z(\n","    z: Float[Tensor, \"batch seq head d_head\"],\n","    hook: HookPoint,\n","    heads_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n","    means: Float[Tensor, \"layer batch seq head d_head\"],\n",") -> Float[Tensor, \"batch seq head d_head\"]:\n","    '''\n","    Hook function which masks the z output of a transformer head.\n","\n","    heads_and_posns_to_keep\n","        Dict created with the get_heads_and_posns_to_keep function. This tells\n","        us where to mask.\n","\n","    means\n","        Tensor of mean z values of the means_dataset over each group of prompts\n","        with the same template. This tells us what values to mask with.\n","    '''\n","    # Get the mask for this layer, and add d_head=1 dimension so it broadcasts correctly\n","    mask_for_this_layer = heads_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n","\n","    # Set z values to the mean\n","    z = t.where(mask_for_this_layer, z, means[hook.layer()])\n","\n","    return z\n","\n","def compute_means_by_template(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Returns the mean of each head's output over the means dataset. This mean is\n","    computed separately for each group of prompts with the same template (these\n","    are given by means_dataset.groups).\n","    '''\n","    # Cache the outputs of every head\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"z\"),\n","    )\n","    # Create tensor to store means\n","    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n","\n","    # Get set of different templates for this data\n","    for layer in range(model.cfg.n_layers):\n","        z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n","        for template_group in means_dataset.groups:\n","            z_for_this_template = z_for_this_layer[template_group]\n","            z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n","            means[layer, template_group] = z_means_for_this_template\n","\n","    del(means_cache)\n","\n","    return means\n","\n","def add_mean_ablation_hook(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n","    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Adds a permanent hook to the model, which ablates according to the circuit and\n","    seq_pos_to_keep dictionaries.\n","\n","    In other words, when the model is run on ioi_dataset, every head's output will\n","    be replaced with the mean over means_dataset for sequences with the same template,\n","    except for a subset of heads and sequence positions as specified by the circuit\n","    and seq_pos_to_keep dicts.\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_z,\n","        heads_and_posns_to_keep=heads_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    return model"],"metadata":{"id":"cE7xLtws_Pnt","executionInfo":{"status":"ok","timestamp":1702654920284,"user_tz":300,"elapsed":362,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["## both"],"metadata":{"id":"4zlcncHKo2h6"}},{"cell_type":"code","source":["def add_mean_ablation_hook_MLP_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    heads_lst, mlp_lst,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = heads_lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_z,\n","        heads_and_posns_to_keep=heads_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    ########################\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = mlp_lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template_MLP(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_mlp_out,\n","        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"],"metadata":{"id":"mnGaDWe__CyA","executionInfo":{"status":"ok","timestamp":1702654920284,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## loop iters"],"metadata":{"id":"049GQVcZoyrI"}},{"cell_type":"code","source":["def find_circuit_forw(heads_not_ablate=None, mlps_not_ablate=None, orig_score=100, threshold=10):\n","    # threshold is T, a %. if performance is less than T%, allow its removal\n","    # we don't ablate the curr circuits\n","    if heads_not_ablate == []: # Start with full circuit\n","        heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]\n","    if mlps_not_ablate == []:\n","        mlps_not_ablate = [layer for layer in range(12)]\n","\n","    comp_scores = {}\n","    for layer in range(0, 12):\n","        for head in range(12):\n","            print(layer, head)\n","            if (layer, head) not in heads_not_ablate:\n","                continue\n","\n","            copy_heads_not_ablate = heads_not_ablate.copy()\n","            copy_heads_not_ablate.remove((layer, head))\n","\n","            model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","            ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, copy_heads_not_ablate, mlps_not_ablate)\n","\n","            new_logits = ablated_model(dataset.toks)\n","            new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","            new_perc = 100 * new_score / orig_score\n","            comp_scores[layer] = new_perc\n","            print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","            if (100 - new_perc) < threshold:\n","                heads_not_ablate.remove((layer, head))\n","                print(\"Removed:\", (layer, head))\n","            del(new_logits)\n","\n","        # print(layer)\n","        # if layer in mlps_not_ablate:\n","        #     copy_mlps_not_ablate = mlps_not_ablate.copy()\n","        #     copy_mlps_not_ablate.remove(layer)\n","\n","        #     model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","        #     ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, copy_mlps_not_ablate)\n","\n","        #     new_logits = ablated_model(dataset.toks)\n","        #     new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","        #     new_perc = 100 * new_score / orig_score\n","        #     comp_scores[(layer, head)] = new_perc\n","        #     print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","        #     if (100 - new_perc) < threshold:\n","        #         mlps_not_ablate.remove(layer)\n","        #         print(\"Removed: MLP \", layer)\n","        #     del(new_logits)\n","\n","    return heads_not_ablate, mlps_not_ablate, new_perc, comp_scores"],"metadata":{"id":"Tx5xoj2HtT4C","executionInfo":{"status":"ok","timestamp":1702654920284,"user_tz":300,"elapsed":1,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def find_circuit_backw(heads_not_ablate=None, mlps_not_ablate=None, orig_score=100, threshold=10, iter=1):\n","    # threshold is T, a %. if performance is less than T%, allow its removal\n","    # we don't ablate the curr circuits\n","    if heads_not_ablate == []: # Start with full circuit\n","        heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]\n","    if mlps_not_ablate == []:\n","        mlps_not_ablate = [layer for layer in range(12)]\n","\n","    comp_scores = {}\n","    for layer in range(11, -1, -1):  # go thru all heads in a layer first\n","        print(layer)\n","        if layer in mlps_not_ablate and iter < 2:\n","            copy_mlps_not_ablate = mlps_not_ablate.copy()\n","            copy_mlps_not_ablate.remove(layer)\n","\n","            model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","            ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, copy_mlps_not_ablate)\n","\n","            new_logits = ablated_model(dataset.toks)\n","            new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","            new_perc = 100 * new_score / orig_score\n","            comp_scores[layer] = new_perc\n","            print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","            if (100 - new_perc) < threshold:\n","                mlps_not_ablate.remove(layer)\n","                print(\"Removed: MLP \", layer)\n","            del(new_logits)\n","\n","        for head in range(12):\n","            print(layer, head)\n","            if (layer, head) not in heads_not_ablate:\n","                continue\n","\n","            copy_heads_not_ablate = heads_not_ablate.copy()\n","            copy_heads_not_ablate.remove((layer, head))\n","\n","            model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","            ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, copy_heads_not_ablate, mlps_not_ablate)\n","\n","            new_logits = ablated_model(dataset.toks)\n","            new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","            new_perc = 100 * new_score / orig_score\n","            comp_scores[(layer, head)] = new_perc\n","            print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","            if (100 - new_perc) < threshold:\n","                heads_not_ablate.remove((layer, head))\n","                print(\"Removed:\", (layer, head))\n","            del(new_logits)\n","\n","    return heads_not_ablate, mlps_not_ablate, new_score, comp_scores"],"metadata":{"id":"hJYEoSI3ox5l","executionInfo":{"status":"ok","timestamp":1702654920285,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["# iter backw fwd, threshold 20"],"metadata":{"id":"0NYZB-G19liQ"}},{"cell_type":"code","source":["threshold = 20\n","curr_circ_heads = []\n","curr_circ_mlps = []\n","prev_score = 100\n","new_score = 0\n","iter = 1\n","all_comp_scores = []\n","while prev_score != new_score:\n","    print('\\nbackw prune, iter ', str(iter))\n","    # prev_score = new_score # save old score before finding new one\n","    old_circ_heads = curr_circ_heads.copy() # save old before finding new one\n","    old_circ_mlps = curr_circ_mlps.copy()\n","    curr_circ_heads, curr_circ_mlps, new_score, comp_scores = find_circuit_backw(curr_circ_heads, curr_circ_mlps, orig_score, threshold, iter)\n","    if old_circ_heads == curr_circ_heads and old_circ_mlps == curr_circ_mlps:\n","        break\n","    all_comp_scores.append(comp_scores)\n","    print('\\nfwd prune, iter ', str(iter))\n","    # track changes in circuit as for some reason it doesn't work with scores\n","    old_circ_heads = curr_circ_heads.copy()\n","    old_circ_mlps = curr_circ_mlps.copy()\n","    curr_circ_heads, curr_circ_mlps, new_score, comp_scores = find_circuit_forw(curr_circ_heads, curr_circ_mlps, orig_score, threshold)\n","    if old_circ_heads == curr_circ_heads and old_circ_mlps == curr_circ_mlps:\n","        break\n","    all_comp_scores.append(comp_scores)\n","    iter += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUqLZl-QsahY","outputId":"351a3b45-c448-43f7-a4e8-27105ac8770c","executionInfo":{"status":"ok","timestamp":1702606782049,"user_tz":300,"elapsed":1878018,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","backw prune, iter  1\n","11\n","(cand circuit / full) %: 75.0255\n","11 0\n","(cand circuit / full) %: 99.7315\n","Removed: (11, 0)\n","11 1\n","(cand circuit / full) %: 99.8717\n","Removed: (11, 1)\n","11 2\n","(cand circuit / full) %: 99.8522\n","Removed: (11, 2)\n","11 3\n","(cand circuit / full) %: 99.9779\n","Removed: (11, 3)\n","11 4\n","(cand circuit / full) %: 99.5896\n","Removed: (11, 4)\n","11 5\n","(cand circuit / full) %: 99.6130\n","Removed: (11, 5)\n","11 6\n","(cand circuit / full) %: 99.6287\n","Removed: (11, 6)\n","11 7\n","(cand circuit / full) %: 99.5665\n","Removed: (11, 7)\n","11 8\n","(cand circuit / full) %: 99.2456\n","Removed: (11, 8)\n","11 9\n","(cand circuit / full) %: 98.4471\n","Removed: (11, 9)\n","11 10\n","(cand circuit / full) %: 97.2846\n","Removed: (11, 10)\n","11 11\n","(cand circuit / full) %: 97.5267\n","Removed: (11, 11)\n","10\n","(cand circuit / full) %: 55.1619\n","10 0\n","(cand circuit / full) %: 97.5296\n","Removed: (10, 0)\n","10 1\n","(cand circuit / full) %: 96.6200\n","Removed: (10, 1)\n","10 2\n","(cand circuit / full) %: 103.0986\n","Removed: (10, 2)\n","10 3\n","(cand circuit / full) %: 103.1752\n","Removed: (10, 3)\n","10 4\n","(cand circuit / full) %: 102.8503\n","Removed: (10, 4)\n","10 5\n","(cand circuit / full) %: 103.0583\n","Removed: (10, 5)\n","10 6\n","(cand circuit / full) %: 103.0420\n","Removed: (10, 6)\n","10 7\n","(cand circuit / full) %: 82.9645\n","Removed: (10, 7)\n","10 8\n","(cand circuit / full) %: 82.0917\n","Removed: (10, 8)\n","10 9\n","(cand circuit / full) %: 82.0015\n","Removed: (10, 9)\n","10 10\n","(cand circuit / full) %: 82.2982\n","Removed: (10, 10)\n","10 11\n","(cand circuit / full) %: 81.9594\n","Removed: (10, 11)\n","9\n","(cand circuit / full) %: -4.8277\n","9 0\n","(cand circuit / full) %: 81.6518\n","Removed: (9, 0)\n","9 1\n","(cand circuit / full) %: 51.0207\n","9 2\n","(cand circuit / full) %: 81.1010\n","Removed: (9, 2)\n","9 3\n","(cand circuit / full) %: 81.3401\n","Removed: (9, 3)\n","9 4\n","(cand circuit / full) %: 81.1352\n","Removed: (9, 4)\n","9 5\n","(cand circuit / full) %: 77.8402\n","9 6\n","(cand circuit / full) %: 81.3287\n","Removed: (9, 6)\n","9 7\n","(cand circuit / full) %: 78.4445\n","9 8\n","(cand circuit / full) %: 81.2661\n","Removed: (9, 8)\n","9 9\n","(cand circuit / full) %: 83.8745\n","Removed: (9, 9)\n","9 10\n","(cand circuit / full) %: 83.6589\n","Removed: (9, 10)\n","9 11\n","(cand circuit / full) %: 82.7668\n","Removed: (9, 11)\n","8\n","(cand circuit / full) %: 45.0075\n","8 0\n","(cand circuit / full) %: 84.0692\n","Removed: (8, 0)\n","8 1\n","(cand circuit / full) %: 72.0123\n","8 2\n","(cand circuit / full) %: 84.1730\n","Removed: (8, 2)\n","8 3\n","(cand circuit / full) %: 82.7624\n","Removed: (8, 3)\n","8 4\n","(cand circuit / full) %: 82.6156\n","Removed: (8, 4)\n","8 5\n","(cand circuit / full) %: 81.8293\n","Removed: (8, 5)\n","8 6\n","(cand circuit / full) %: 74.7779\n","8 7\n","(cand circuit / full) %: 80.5339\n","Removed: (8, 7)\n","8 8\n","(cand circuit / full) %: 68.8623\n","8 9\n","(cand circuit / full) %: 77.1535\n","8 10\n","(cand circuit / full) %: 81.4991\n","Removed: (8, 10)\n","8 11\n","(cand circuit / full) %: 77.0962\n","7\n","(cand circuit / full) %: 56.6796\n","7 0\n","(cand circuit / full) %: 81.0082\n","Removed: (7, 0)\n","7 1\n","(cand circuit / full) %: 79.9971\n","7 2\n","(cand circuit / full) %: 78.3408\n","7 3\n","(cand circuit / full) %: 82.7943\n","Removed: (7, 3)\n","7 4\n","(cand circuit / full) %: 82.4377\n","Removed: (7, 4)\n","7 5\n","(cand circuit / full) %: 82.0841\n","Removed: (7, 5)\n","7 6\n","(cand circuit / full) %: 76.3060\n","7 7\n","(cand circuit / full) %: 77.0865\n","7 8\n","(cand circuit / full) %: 82.3306\n","Removed: (7, 8)\n","7 9\n","(cand circuit / full) %: 81.6089\n","Removed: (7, 9)\n","7 10\n","(cand circuit / full) %: 83.2278\n","Removed: (7, 10)\n","7 11\n","(cand circuit / full) %: 34.4446\n","6\n","(cand circuit / full) %: 58.8147\n","6 0\n","(cand circuit / full) %: 83.5456\n","Removed: (6, 0)\n","6 1\n","(cand circuit / full) %: 79.3929\n","6 2\n","(cand circuit / full) %: 84.0745\n","Removed: (6, 2)\n","6 3\n","(cand circuit / full) %: 84.5003\n","Removed: (6, 3)\n","6 4\n","(cand circuit / full) %: 84.0207\n","Removed: (6, 4)\n","6 5\n","(cand circuit / full) %: 83.5108\n","Removed: (6, 5)\n","6 6\n","(cand circuit / full) %: 72.8243\n","6 7\n","(cand circuit / full) %: 82.2668\n","Removed: (6, 7)\n","6 8\n","(cand circuit / full) %: 82.5493\n","Removed: (6, 8)\n","6 9\n","(cand circuit / full) %: 82.3046\n","Removed: (6, 9)\n","6 10\n","(cand circuit / full) %: 62.2469\n","6 11\n","(cand circuit / full) %: 82.4603\n","Removed: (6, 11)\n","5\n","(cand circuit / full) %: 59.9598\n","5 0\n","(cand circuit / full) %: 79.3317\n","5 1\n","(cand circuit / full) %: 82.3682\n","Removed: (5, 1)\n","5 2\n","(cand circuit / full) %: 82.0719\n","Removed: (5, 2)\n","5 3\n","(cand circuit / full) %: 82.3245\n","Removed: (5, 3)\n","5 4\n","(cand circuit / full) %: 82.8528\n","Removed: (5, 4)\n","5 5\n","(cand circuit / full) %: 82.8809\n","Removed: (5, 5)\n","5 6\n","(cand circuit / full) %: 83.0030\n","Removed: (5, 6)\n","5 7\n","(cand circuit / full) %: 83.4331\n","Removed: (5, 7)\n","5 8\n","(cand circuit / full) %: 79.3661\n","5 9\n","(cand circuit / full) %: 83.4332\n","Removed: (5, 9)\n","5 10\n","(cand circuit / full) %: 84.2986\n","Removed: (5, 10)\n","5 11\n","(cand circuit / full) %: 83.8882\n","Removed: (5, 11)\n","4\n","(cand circuit / full) %: 47.6231\n","4 0\n","(cand circuit / full) %: 84.5496\n","Removed: (4, 0)\n","4 1\n","(cand circuit / full) %: 84.3306\n","Removed: (4, 1)\n","4 2\n","(cand circuit / full) %: 85.1026\n","Removed: (4, 2)\n","4 3\n","(cand circuit / full) %: 85.5440\n","Removed: (4, 3)\n","4 4\n","(cand circuit / full) %: 47.2279\n","4 5\n","(cand circuit / full) %: 86.6013\n","Removed: (4, 5)\n","4 6\n","(cand circuit / full) %: 85.1428\n","Removed: (4, 6)\n","4 7\n","(cand circuit / full) %: 82.4447\n","Removed: (4, 7)\n","4 8\n","(cand circuit / full) %: 81.6286\n","Removed: (4, 8)\n","4 9\n","(cand circuit / full) %: 82.7772\n","Removed: (4, 9)\n","4 10\n","(cand circuit / full) %: 75.6470\n","4 11\n","(cand circuit / full) %: 81.9813\n","Removed: (4, 11)\n","3\n","(cand circuit / full) %: 68.8176\n","3 0\n","(cand circuit / full) %: 80.2916\n","Removed: (3, 0)\n","3 1\n","(cand circuit / full) %: 80.2281\n","Removed: (3, 1)\n","3 2\n","(cand circuit / full) %: 80.3324\n","Removed: (3, 2)\n","3 3\n","(cand circuit / full) %: 80.1209\n","Removed: (3, 3)\n","3 4\n","(cand circuit / full) %: 80.0833\n","Removed: (3, 4)\n","3 5\n","(cand circuit / full) %: 80.4858\n","Removed: (3, 5)\n","3 6\n","(cand circuit / full) %: 80.3159\n","Removed: (3, 6)\n","3 7\n","(cand circuit / full) %: 81.2451\n","Removed: (3, 7)\n","3 8\n","(cand circuit / full) %: 81.0321\n","Removed: (3, 8)\n","3 9\n","(cand circuit / full) %: 82.0483\n","Removed: (3, 9)\n","3 10\n","(cand circuit / full) %: 81.1998\n","Removed: (3, 10)\n","3 11\n","(cand circuit / full) %: 82.7855\n","Removed: (3, 11)\n","2\n","(cand circuit / full) %: 66.3393\n","2 0\n","(cand circuit / full) %: 82.7518\n","Removed: (2, 0)\n","2 1\n","(cand circuit / full) %: 84.7410\n","Removed: (2, 1)\n","2 2\n","(cand circuit / full) %: 84.7464\n","Removed: (2, 2)\n","2 3\n","(cand circuit / full) %: 84.9554\n","Removed: (2, 3)\n","2 4\n","(cand circuit / full) %: 85.1146\n","Removed: (2, 4)\n","2 5\n","(cand circuit / full) %: 85.3503\n","Removed: (2, 5)\n","2 6\n","(cand circuit / full) %: 84.5559\n","Removed: (2, 6)\n","2 7\n","(cand circuit / full) %: 84.4610\n","Removed: (2, 7)\n","2 8\n","(cand circuit / full) %: 84.5228\n","Removed: (2, 8)\n","2 9\n","(cand circuit / full) %: 84.8697\n","Removed: (2, 9)\n","2 10\n","(cand circuit / full) %: 85.1852\n","Removed: (2, 10)\n","2 11\n","(cand circuit / full) %: 84.9685\n","Removed: (2, 11)\n","1\n","(cand circuit / full) %: 58.3136\n","1 0\n","(cand circuit / full) %: 83.6309\n","Removed: (1, 0)\n","1 1\n","(cand circuit / full) %: 83.5867\n","Removed: (1, 1)\n","1 2\n","(cand circuit / full) %: 83.4702\n","Removed: (1, 2)\n","1 3\n","(cand circuit / full) %: 83.5527\n","Removed: (1, 3)\n","1 4\n","(cand circuit / full) %: 83.5802\n","Removed: (1, 4)\n","1 5\n","(cand circuit / full) %: 62.1275\n","1 6\n","(cand circuit / full) %: 84.0768\n","Removed: (1, 6)\n","1 7\n","(cand circuit / full) %: 84.7284\n","Removed: (1, 7)\n","1 8\n","(cand circuit / full) %: 84.6297\n","Removed: (1, 8)\n","1 9\n","(cand circuit / full) %: 84.4815\n","Removed: (1, 9)\n","1 10\n","(cand circuit / full) %: 84.5452\n","Removed: (1, 10)\n","1 11\n","(cand circuit / full) %: 76.1067\n","0\n","(cand circuit / full) %: 8.7044\n","0 0\n","(cand circuit / full) %: 84.2882\n","Removed: (0, 0)\n","0 1\n","(cand circuit / full) %: 20.0481\n","0 2\n","(cand circuit / full) %: 84.1241\n","Removed: (0, 2)\n","0 3\n","(cand circuit / full) %: 81.8311\n","Removed: (0, 3)\n","0 4\n","(cand circuit / full) %: 82.9654\n","Removed: (0, 4)\n","0 5\n","(cand circuit / full) %: 77.6335\n","0 6\n","(cand circuit / full) %: 84.0697\n","Removed: (0, 6)\n","0 7\n","(cand circuit / full) %: 83.5802\n","Removed: (0, 7)\n","0 8\n","(cand circuit / full) %: 83.6606\n","Removed: (0, 8)\n","0 9\n","(cand circuit / full) %: 83.3728\n","Removed: (0, 9)\n","0 10\n","(cand circuit / full) %: 87.8999\n","Removed: (0, 10)\n","0 11\n","(cand circuit / full) %: 88.0059\n","Removed: (0, 11)\n","\n","fwd prune, iter  1\n","0 0\n","0 1\n","(cand circuit / full) %: 42.1592\n","0 2\n","0 3\n","0 4\n","0 5\n","(cand circuit / full) %: 84.2994\n","Removed: (0, 5)\n","0 6\n","0 7\n","0 8\n","0 9\n","0 10\n","0 11\n","1 0\n","1 1\n","1 2\n","1 3\n","1 4\n","1 5\n","(cand circuit / full) %: 63.9582\n","1 6\n","1 7\n","1 8\n","1 9\n","1 10\n","1 11\n","(cand circuit / full) %: 84.1879\n","Removed: (1, 11)\n","2 0\n","2 1\n","2 2\n","2 3\n","2 4\n","2 5\n","2 6\n","2 7\n","2 8\n","2 9\n","2 10\n","2 11\n","3 0\n","3 1\n","3 2\n","3 3\n","3 4\n","3 5\n","3 6\n","3 7\n","3 8\n","3 9\n","3 10\n","3 11\n","4 0\n","4 1\n","4 2\n","4 3\n","4 4\n","(cand circuit / full) %: 48.5472\n","4 5\n","4 6\n","4 7\n","4 8\n","4 9\n","4 10\n","(cand circuit / full) %: 78.6561\n","4 11\n","5 0\n","(cand circuit / full) %: 83.4241\n","Removed: (5, 0)\n","5 1\n","5 2\n","5 3\n","5 4\n","5 5\n","5 6\n","5 7\n","5 8\n","(cand circuit / full) %: 78.0385\n","5 9\n","5 10\n","5 11\n","6 0\n","6 1\n","(cand circuit / full) %: 79.3381\n","6 2\n","6 3\n","6 4\n","6 5\n","6 6\n","(cand circuit / full) %: 73.8584\n","6 7\n","6 8\n","6 9\n","6 10\n","(cand circuit / full) %: 61.0523\n","6 11\n","7 0\n","7 1\n","(cand circuit / full) %: 82.6374\n","Removed: (7, 1)\n","7 2\n","(cand circuit / full) %: 79.7791\n","7 3\n","7 4\n","7 5\n","7 6\n","(cand circuit / full) %: 77.7893\n","7 7\n","(cand circuit / full) %: 81.1096\n","Removed: (7, 7)\n","7 8\n","7 9\n","7 10\n","7 11\n","(cand circuit / full) %: 36.3295\n","8 0\n","8 1\n","(cand circuit / full) %: 68.5016\n","8 2\n","8 3\n","8 4\n","8 5\n","8 6\n","(cand circuit / full) %: 74.8917\n","8 7\n","8 8\n","(cand circuit / full) %: 67.9027\n","8 9\n","(cand circuit / full) %: 77.7477\n","8 10\n","8 11\n","(cand circuit / full) %: 70.2452\n","9 0\n","9 1\n","(cand circuit / full) %: 53.3702\n","9 2\n","9 3\n","9 4\n","9 5\n","(cand circuit / full) %: 75.2664\n","9 6\n","9 7\n","(cand circuit / full) %: 78.0327\n","9 8\n","9 9\n","9 10\n","9 11\n","10 0\n","10 1\n","10 2\n","10 3\n","10 4\n","10 5\n","10 6\n","10 7\n","10 8\n","10 9\n","10 10\n","10 11\n","11 0\n","11 1\n","11 2\n","11 3\n","11 4\n","11 5\n","11 6\n","11 7\n","11 8\n","11 9\n","11 10\n","11 11\n","\n","backw prune, iter  2\n","11\n","11 0\n","11 1\n","11 2\n","11 3\n","11 4\n","11 5\n","11 6\n","11 7\n","11 8\n","11 9\n","11 10\n","11 11\n","10\n","10 0\n","10 1\n","10 2\n","10 3\n","10 4\n","10 5\n","10 6\n","10 7\n","10 8\n","10 9\n","10 10\n","10 11\n","9\n","9 0\n","9 1\n","(cand circuit / full) %: 53.3702\n","9 2\n","9 3\n","9 4\n","9 5\n","(cand circuit / full) %: 75.2664\n","9 6\n","9 7\n","(cand circuit / full) %: 78.0327\n","9 8\n","9 9\n","9 10\n","9 11\n","8\n","8 0\n","8 1\n","(cand circuit / full) %: 68.5016\n","8 2\n","8 3\n","8 4\n","8 5\n","8 6\n","(cand circuit / full) %: 74.8917\n","8 7\n","8 8\n","(cand circuit / full) %: 67.9027\n","8 9\n","(cand circuit / full) %: 77.7477\n","8 10\n","8 11\n","(cand circuit / full) %: 70.2452\n","7\n","7 0\n","7 1\n","7 2\n","(cand circuit / full) %: 78.2655\n","7 3\n","7 4\n","7 5\n","7 6\n","(cand circuit / full) %: 76.1487\n","7 7\n","7 8\n","7 9\n","7 10\n","7 11\n","(cand circuit / full) %: 36.3295\n","6\n","6 0\n","6 1\n","(cand circuit / full) %: 76.6214\n","6 2\n","6 3\n","6 4\n","6 5\n","6 6\n","(cand circuit / full) %: 71.4101\n","6 7\n","6 8\n","6 9\n","6 10\n","(cand circuit / full) %: 56.8324\n","6 11\n","5\n","5 0\n","5 1\n","5 2\n","5 3\n","5 4\n","5 5\n","5 6\n","5 7\n","5 8\n","(cand circuit / full) %: 75.9623\n","5 9\n","5 10\n","5 11\n","4\n","4 0\n","4 1\n","4 2\n","4 3\n","4 4\n","(cand circuit / full) %: 46.9987\n","4 5\n","4 6\n","4 7\n","4 8\n","4 9\n","4 10\n","(cand circuit / full) %: 75.3836\n","4 11\n","3\n","3 0\n","3 1\n","3 2\n","3 3\n","3 4\n","3 5\n","3 6\n","3 7\n","3 8\n","3 9\n","3 10\n","3 11\n","2\n","2 0\n","2 1\n","2 2\n","2 3\n","2 4\n","2 5\n","2 6\n","2 7\n","2 8\n","2 9\n","2 10\n","2 11\n","1\n","1 0\n","1 1\n","1 2\n","1 3\n","1 4\n","1 5\n","(cand circuit / full) %: 62.4586\n","1 6\n","1 7\n","1 8\n","1 9\n","1 10\n","1 11\n","0\n","0 0\n","0 1\n","(cand circuit / full) %: 2.3691\n","0 2\n","0 3\n","0 4\n","0 5\n","0 6\n","0 7\n","0 8\n","0 9\n","0 10\n","0 11\n"]}]},{"cell_type":"code","source":["import pickle\n","from google.colab import files\n","\n","with open('nw_bf_20_scores.pkl', 'wb') as file:\n","    pickle.dump(all_comp_scores, file)\n","files.download('nw_bf_20_scores.pkl')"],"metadata":{"id":"V93FIJs2MiU9","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1702607452895,"user_tz":300,"elapsed":340,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ec018fe9-8a81-4f3b-b56e-8ba379fa1d7e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_1f59f011-b8a6-47ac-a4a7-d1a912f7c44d\", \"nw_bf_20_scores.pkl\", 48095)"]},"metadata":{}}]},{"cell_type":"code","source":["curr_circ_heads"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPlS7M_vyBSG","executionInfo":{"status":"ok","timestamp":1702606782050,"user_tz":300,"elapsed":9,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7c542775-e29c-4583-cb2d-a65599559584"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 1),\n"," (1, 5),\n"," (4, 4),\n"," (4, 10),\n"," (5, 8),\n"," (6, 1),\n"," (6, 6),\n"," (6, 10),\n"," (7, 2),\n"," (7, 6),\n"," (7, 11),\n"," (8, 1),\n"," (8, 6),\n"," (8, 8),\n"," (8, 9),\n"," (8, 11),\n"," (9, 1),\n"," (9, 5),\n"," (9, 7)]"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["curr_circ_mlps"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrSFmCgtyFwY","executionInfo":{"status":"ok","timestamp":1702606782050,"user_tz":300,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d6c31b08-a481-47e6-c660-dcad332a57f3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["## loop rmv and check for most impt heads"],"metadata":{"id":"8At2Kqx69liS"}},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_mean_ablation_hook_MLP_head(model, dataset_2, curr_circ_heads, curr_circ_mlps)\n","\n","new_logits = model(dataset.toks)\n","new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")\n","\n","# len(curr_circ_heads)\n","# len(curr_circ_mlps)"],"metadata":{"id":"ivoDzNKY9liS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702606790271,"user_tz":300,"elapsed":8224,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"05b71bbe-a398-4fb5-c46d-59cc7b7f1638"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(cand circuit / full) %: 81.1096\n"]}]},{"cell_type":"code","source":["lh_scores = {}\n","for lh in curr_circ_heads:\n","    copy_circuit = curr_circ_heads.copy()\n","    copy_circuit.remove(lh)\n","    print(\"removed: \" + str(lh))\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    model = add_mean_ablation_hook_MLP_head(model, dataset_2, copy_circuit, curr_circ_mlps)\n","\n","    new_logits = model(dataset.toks)\n","    new_score = logits_to_ave_logit_diff(new_logits, dataset).item()\n","    new_perc = 100 * new_score / orig_score\n","    print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","    lh_scores[lh] = new_perc"],"metadata":{"id":"vsUtHR-y9liS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702606928672,"user_tz":300,"elapsed":8453,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4462e397-7fd6-4963-cfca-612f03a0fce4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["removed: (0, 1)\n","(cand circuit / full) %: 2.3691\n","removed: (1, 5)\n","(cand circuit / full) %: 62.4586\n","removed: (4, 4)\n","(cand circuit / full) %: 46.9987\n","removed: (4, 10)\n","(cand circuit / full) %: 75.3836\n","removed: (5, 8)\n","(cand circuit / full) %: 75.9623\n","removed: (6, 1)\n","(cand circuit / full) %: 76.6214\n","removed: (6, 6)\n","(cand circuit / full) %: 71.4101\n","removed: (6, 10)\n","(cand circuit / full) %: 56.8324\n","removed: (7, 2)\n","(cand circuit / full) %: 78.2655\n","removed: (7, 6)\n","(cand circuit / full) %: 76.1487\n","removed: (7, 11)\n","(cand circuit / full) %: 36.3295\n","removed: (8, 1)\n","(cand circuit / full) %: 68.5016\n","removed: (8, 6)\n","(cand circuit / full) %: 74.8917\n","removed: (8, 8)\n","(cand circuit / full) %: 67.9027\n","removed: (8, 9)\n","(cand circuit / full) %: 77.7477\n","removed: (8, 11)\n","(cand circuit / full) %: 70.2452\n","removed: (9, 1)\n","(cand circuit / full) %: 53.3702\n","removed: (9, 5)\n","(cand circuit / full) %: 75.2664\n","removed: (9, 7)\n","(cand circuit / full) %: 78.0327\n"]}]},{"cell_type":"code","source":["sorted_lh_scores = dict(sorted(lh_scores.items(), key=lambda item: item[1]))\n","sorted_lh_scores"],"metadata":{"id":"MNzdWLFj9liT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702606928672,"user_tz":300,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"45fd4203-489e-47bc-cade-42a8915958dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(0, 1): tensor(2.3691, device='cuda:0'),\n"," (7, 11): tensor(36.3295, device='cuda:0'),\n"," (4, 4): tensor(46.9987, device='cuda:0'),\n"," (9, 1): tensor(53.3702, device='cuda:0'),\n"," (6, 10): tensor(56.8324, device='cuda:0'),\n"," (1, 5): tensor(62.4586, device='cuda:0'),\n"," (8, 8): tensor(67.9027, device='cuda:0'),\n"," (8, 1): tensor(68.5016, device='cuda:0'),\n"," (8, 11): tensor(70.2452, device='cuda:0'),\n"," (6, 6): tensor(71.4101, device='cuda:0'),\n"," (8, 6): tensor(74.8917, device='cuda:0'),\n"," (9, 5): tensor(75.2664, device='cuda:0'),\n"," (4, 10): tensor(75.3836, device='cuda:0'),\n"," (5, 8): tensor(75.9623, device='cuda:0'),\n"," (7, 6): tensor(76.1487, device='cuda:0'),\n"," (6, 1): tensor(76.6214, device='cuda:0'),\n"," (8, 9): tensor(77.7477, device='cuda:0'),\n"," (9, 7): tensor(78.0327, device='cuda:0'),\n"," (7, 2): tensor(78.2655, device='cuda:0')}"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["for lh, score in sorted_lh_scores.items():\n","    print(lh, -round(circ_score-score.item(), 2))"],"metadata":{"id":"RPCynBNH9liT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702607380687,"user_tz":300,"elapsed":352,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"823043b5-6b69-4ab5-a7b0-8a357ffbbae9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(0, 1) -78.74\n","(7, 11) -44.78\n","(4, 4) -34.11\n","(9, 1) -27.74\n","(6, 10) -24.28\n","(1, 5) -18.65\n","(8, 8) -13.21\n","(8, 1) -12.61\n","(8, 11) -10.86\n","(6, 6) -9.7\n","(8, 6) -6.22\n","(9, 5) -5.84\n","(4, 10) -5.73\n","(5, 8) -5.15\n","(7, 6) -4.96\n","(6, 1) -4.49\n","(8, 9) -3.36\n","(9, 7) -3.08\n","(7, 2) -2.84\n"]}]},{"cell_type":"markdown","source":["# try other tasks circs, thres 20"],"metadata":{"id":"xbZkzn0nrrxt"}},{"cell_type":"code","source":["heads_not_ablate = [(0, 1), (1, 5), (4, 4), (4, 10), (5, 0), (6, 1), (6, 6), (6, 10), (7, 11), (8, 1), (8, 6), (8, 8), (8, 9), (9, 1)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11]\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0GzjXOWFYkO","executionInfo":{"status":"ok","timestamp":1702654930125,"user_tz":300,"elapsed":6715,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bb41c51d-a484-4d4e-9b61-df4c2ae03088"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["(cand circuit / full) %: 48.4090\n"]}]},{"cell_type":"code","source":["# numwords\n","heads_not_ablate = [(0, 1), (1, 5), (4, 4), (4, 10), (5, 8), (6, 1), (6, 6), (6, 10), (7, 2), (7, 6), (7, 11), (8, 1), (8, 6), (8, 8), (8, 9), (8, 11), (9, 1), (9, 5), (9, 7)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"id":"XgtbBeiarrx3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702654937593,"user_tz":300,"elapsed":7479,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"84cad187-c133-4eee-87a5-6833e6199896"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["(cand circuit / full) %: 81.1096\n"]}]},{"cell_type":"code","source":["# months\n","heads_not_ablate = [(0, 1), (0, 5), (4, 4), (6, 1), (6, 6), (6, 10), (7, 6), (7, 9), (7, 10), (7, 11), (8, 8), (9, 1), (10, 7)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"id":"IYcsLUjIrrx3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702654944171,"user_tz":300,"elapsed":6590,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7105e6d9-00ab-44bc-aeeb-c97790463617"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["(cand circuit / full) %: 32.3630\n"]}]},{"cell_type":"code","source":["CIRCUIT = {\n","    \"name mover\": [(9, 9), (10, 0), (9, 6)],\n","    \"backup name mover\": [(10, 10), (10, 6), (10, 2), (10, 1), (11, 2), (9, 7), (9, 0), (11, 9)],\n","    \"negative name mover\": [(10, 7), (11, 10)],\n","    \"s2 inhibition\": [(7, 3), (7, 9), (8, 6), (8, 10)],\n","    \"induction\": [(5, 5), (5, 8), (5, 9), (6, 9)],\n","    \"duplicate token\": [(0, 1), (0, 10), (3, 0)],\n","    \"previous token\": [(2, 2), (4, 11)],\n","}\n","\n","import itertools\n","a = [val for val in CIRCUIT.values()]\n","IOI_heads = list(itertools.chain.from_iterable(a))\n","\n","mlps_not_ablate = list(range(12))\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_mean_ablation_hook_MLP_head(model, dataset_2, IOI_heads, mlps_not_ablate)\n","\n","new_logits = model(dataset.toks)\n","new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YGt4ghgOJgD0","executionInfo":{"status":"ok","timestamp":1702655249592,"user_tz":300,"elapsed":9262,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8ce9fff7-07cc-4f52-f6af-2ecd55c574b6"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["(cand circuit / full) %: -15.8257\n"]}]},{"cell_type":"code","source":["IOI_heads"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NI5kUajZKSTl","executionInfo":{"status":"ok","timestamp":1702655258790,"user_tz":300,"elapsed":3,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2a0bfc73-1b7a-40ba-d654-d557f22e4927"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(9, 9),\n"," (10, 0),\n"," (9, 6),\n"," (10, 10),\n"," (10, 6),\n"," (10, 2),\n"," (10, 1),\n"," (11, 2),\n"," (9, 7),\n"," (9, 0),\n"," (11, 9),\n"," (10, 7),\n"," (11, 10),\n"," (7, 3),\n"," (7, 9),\n"," (8, 6),\n"," (8, 10),\n"," (5, 5),\n"," (5, 8),\n"," (5, 9),\n"," (6, 9),\n"," (0, 1),\n"," (0, 10),\n"," (3, 0),\n"," (2, 2),\n"," (4, 11)]"]},"metadata":{},"execution_count":46}]}]}