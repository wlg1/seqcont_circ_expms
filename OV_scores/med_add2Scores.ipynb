{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["xq3CDDcx9Cmo","pc6wzXDl1G5E"],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyNE2T89B6m4OCFv5D+sAHzb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4a0d8589c1dd4390a9dc2bde136536a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_673d95059a3249a5910b2e74f89cd9bf","IPY_MODEL_0f29274c490b4a02a7ce3921955ca565","IPY_MODEL_4f1287c5e0f149c0959c3247992ef0af"],"layout":"IPY_MODEL_5113fffe3f6a4b1fbff3afadda028068"}},"673d95059a3249a5910b2e74f89cd9bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffffab88d2e04562b1e0c133e9d2764e","placeholder":"​","style":"IPY_MODEL_22c76de64a374667b4cde49e0a7d4b4e","value":"Downloading (…)lve/main/config.json: 100%"}},"0f29274c490b4a02a7ce3921955ca565":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59ef306372424cd7822829a7ddfbae79","max":718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a21c25863d84d1ca62025d0755bf65b","value":718}},"4f1287c5e0f149c0959c3247992ef0af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8a4529826594414a1701570991c5640","placeholder":"​","style":"IPY_MODEL_f447d3ae15304ce98a181fa1f2b9e1b4","value":" 718/718 [00:00&lt;00:00, 65.3kB/s]"}},"5113fffe3f6a4b1fbff3afadda028068":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffffab88d2e04562b1e0c133e9d2764e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22c76de64a374667b4cde49e0a7d4b4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59ef306372424cd7822829a7ddfbae79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a21c25863d84d1ca62025d0755bf65b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8a4529826594414a1701570991c5640":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f447d3ae15304ce98a181fa1f2b9e1b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe8f91a798324555a3672530adc7c5ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_415f69b114274d12bb6e0d997b57d067","IPY_MODEL_a3e696dfbe1645cf94cb80e62784a30d","IPY_MODEL_35064135e7a54a999a73b34c9d211279"],"layout":"IPY_MODEL_47b2008e180a4d3a85d6b9009b7d33b2"}},"415f69b114274d12bb6e0d997b57d067":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6bee7edf3f04ff28bc3d6d3c2fa5894","placeholder":"​","style":"IPY_MODEL_101cf4d982a44a9daeecf54016a39bff","value":"Downloading model.safetensors: 100%"}},"a3e696dfbe1645cf94cb80e62784a30d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0716e0677b384dac9977e3d5e3c9f8c3","max":1519984962,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2f30ce079e340ff9827cbccee5509d7","value":1519984962}},"35064135e7a54a999a73b34c9d211279":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca245a836b9745f381d0a26254846eb8","placeholder":"​","style":"IPY_MODEL_46b5ac98c2fe4b1db3bac561db073a77","value":" 1.52G/1.52G [00:17&lt;00:00, 83.2MB/s]"}},"47b2008e180a4d3a85d6b9009b7d33b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6bee7edf3f04ff28bc3d6d3c2fa5894":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"101cf4d982a44a9daeecf54016a39bff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0716e0677b384dac9977e3d5e3c9f8c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2f30ce079e340ff9827cbccee5509d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca245a836b9745f381d0a26254846eb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46b5ac98c2fe4b1db3bac561db073a77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5dae5f5521a4f79bbaa2e91a0371728":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd1aa9ba656045d99d4331ed2ab144b1","IPY_MODEL_b6b2999c2360421a901084f4e4a52abc","IPY_MODEL_70b51b87562a4749b15788550e3e7340"],"layout":"IPY_MODEL_db9930716a014416a0289f35c8c5974d"}},"fd1aa9ba656045d99d4331ed2ab144b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41ce1358eacc47a4af833497a0e95778","placeholder":"​","style":"IPY_MODEL_a59ca85c7ef44e6eace427771e5dab2c","value":"Downloading (…)neration_config.json: 100%"}},"b6b2999c2360421a901084f4e4a52abc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_028c25d000c447f4a2d69b391f20e20b","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fdadc1d8487410285cf6ac2c7027e74","value":124}},"70b51b87562a4749b15788550e3e7340":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7e332208a61497d9ad065401c8462c6","placeholder":"​","style":"IPY_MODEL_b5a3395148f148a1b8d5d87b5a9e7594","value":" 124/124 [00:00&lt;00:00, 10.9kB/s]"}},"db9930716a014416a0289f35c8c5974d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41ce1358eacc47a4af833497a0e95778":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a59ca85c7ef44e6eace427771e5dab2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"028c25d000c447f4a2d69b391f20e20b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fdadc1d8487410285cf6ac2c7027e74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7e332208a61497d9ad065401c8462c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5a3395148f148a1b8d5d87b5a9e7594":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc54a2456c2744f995bf4fa0729e900e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7df0992a18049a796a45684a6a6a521","IPY_MODEL_a2d7ddd5aea5474083e2882c070c73fe","IPY_MODEL_6433a26cc91b468baec695339fe5d2ee"],"layout":"IPY_MODEL_a0bcaa22a08d4737a64aea5d6210e538"}},"d7df0992a18049a796a45684a6a6a521":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_608a3e86a64543e0bdce09a4072bdaac","placeholder":"​","style":"IPY_MODEL_c61f0486963747bda243be653264eb6e","value":"Downloading (…)olve/main/vocab.json: 100%"}},"a2d7ddd5aea5474083e2882c070c73fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9ff5b3eb9c64b3794a75997c6035373","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4809e789405467e8fd166889b3e6b24","value":1042301}},"6433a26cc91b468baec695339fe5d2ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a478820f2cda4259b2e71e762317cd71","placeholder":"​","style":"IPY_MODEL_dc3e3e122cc243beac7003aa6c2ef41c","value":" 1.04M/1.04M [00:00&lt;00:00, 1.33MB/s]"}},"a0bcaa22a08d4737a64aea5d6210e538":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"608a3e86a64543e0bdce09a4072bdaac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c61f0486963747bda243be653264eb6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9ff5b3eb9c64b3794a75997c6035373":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4809e789405467e8fd166889b3e6b24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a478820f2cda4259b2e71e762317cd71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc3e3e122cc243beac7003aa6c2ef41c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cc43cd388234c828e937bbc10d1c239":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d5cbe142b9141e4b4ab1bc0640dd54a","IPY_MODEL_4f80b088cc6f45ef834fc26b860566b9","IPY_MODEL_33ecbc483cdb4b40b466c536a11e68d9"],"layout":"IPY_MODEL_39060a05182d48078fe0a95577aa074c"}},"9d5cbe142b9141e4b4ab1bc0640dd54a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27971f698e5a4598aab3a128c66a6a43","placeholder":"​","style":"IPY_MODEL_005bc59e95034b42b23d92f274457a2c","value":"Downloading (…)olve/main/merges.txt: 100%"}},"4f80b088cc6f45ef834fc26b860566b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bee75182e8c4c3a8c617a90dec78c93","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56909c7e1d7947bdb674110de36506df","value":456318}},"33ecbc483cdb4b40b466c536a11e68d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00fb7f7b0abb402ab903c7de5ffd709d","placeholder":"​","style":"IPY_MODEL_caf20e8de2464fa780431a826dcb8a20","value":" 456k/456k [00:00&lt;00:00, 1.17MB/s]"}},"39060a05182d48078fe0a95577aa074c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27971f698e5a4598aab3a128c66a6a43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"005bc59e95034b42b23d92f274457a2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bee75182e8c4c3a8c617a90dec78c93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56909c7e1d7947bdb674110de36506df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"00fb7f7b0abb402ab903c7de5ffd709d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caf20e8de2464fa780431a826dcb8a20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63192c0ed1264169884992ee07555feb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1abe548c4f2543d49962b4c7d6d749c1","IPY_MODEL_11ffe3e394014239a18210dc335773dd","IPY_MODEL_213414cbac11413da2bad90a3aac2178"],"layout":"IPY_MODEL_f331bcf0b9324c7294c17d30c5718591"}},"1abe548c4f2543d49962b4c7d6d749c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ccad991ab49446aba842407475f0fe2","placeholder":"​","style":"IPY_MODEL_ad3c30caf452467d9821409e6dccddb6","value":"Downloading (…)/main/tokenizer.json: 100%"}},"11ffe3e394014239a18210dc335773dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8589d34183804f83a1caacb2fd33d6eb","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a74db07361ba4f97ba7d6ff39dceac76","value":1355256}},"213414cbac11413da2bad90a3aac2178":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_259f8c337fb34b3f831343505ebf5ec8","placeholder":"​","style":"IPY_MODEL_147f198bbbfb4d0f96169d4afaf1ad20","value":" 1.36M/1.36M [00:00&lt;00:00, 1.39MB/s]"}},"f331bcf0b9324c7294c17d30c5718591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ccad991ab49446aba842407475f0fe2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad3c30caf452467d9821409e6dccddb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8589d34183804f83a1caacb2fd33d6eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a74db07361ba4f97ba7d6ff39dceac76":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"259f8c337fb34b3f831343505ebf5ec8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"147f198bbbfb4d0f96169d4afaf1ad20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"b13177b7"},"source":["<a href=\"https://colab.research.google.com/github/wlg100/numseqcont_circuit_expms/blob/main/notebook_templates/headFNs_expms_template.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."]},{"cell_type":"markdown","metadata":{"id":"epqaV4bo4uuI"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3ANUnkaP41tk","executionInfo":{"status":"ok","timestamp":1698343548330,"user_tz":240,"elapsed":27195,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["%%capture\n","%pip install git+https://github.com/redwoodresearch/Easy-Transformer.git\n","%pip install einops datasets transformers fancy_einsum"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rmhOgC9o4uuI","executionInfo":{"status":"ok","timestamp":1698343553994,"user_tz":240,"elapsed":5677,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["from copy import deepcopy\n","import torch\n","\n","assert torch.cuda.device_count() == 1\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import torch as t\n","from easy_transformer.EasyTransformer import (\n","    EasyTransformer,\n",")\n","from time import ctime\n","from functools import partial\n","\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","\n","from easy_transformer.experiments import (\n","    ExperimentMetric,\n","    AblationConfig,\n","    EasyAblation,\n","    EasyPatching,\n","    PatchingConfig,\n",")\n","import plotly.express as px\n","import plotly.io as pio\n","import plotly.graph_objects as go\n","import random\n","import einops\n","from IPython import get_ipython\n","from copy import deepcopy\n","from easy_transformer.ioi_dataset import (\n","    IOIDataset,\n",")\n","from easy_transformer.ioi_utils import (\n","    path_patching,\n","    max_2d,\n","    CLASS_COLORS,\n","    show_pp,\n","    show_attention_patterns,\n","    scatter_attention_and_contribution,\n",")\n","from random import randint as ri\n","from easy_transformer.ioi_circuit_extraction import (\n","    do_circuit_extraction,\n","    get_heads_circuit,\n","    CIRCUIT,\n",")\n","from easy_transformer.ioi_utils import logit_diff, probs\n","from easy_transformer.ioi_utils import get_top_tokens_and_probs as g\n","\n","ipython = get_ipython()\n","if ipython is not None:\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"markdown","metadata":{"id":"q2AuFrzz4uuJ"},"source":[" Initialise model (use larger N or fewer templates for no warnings about in-template ablation)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["4a0d8589c1dd4390a9dc2bde136536a6","673d95059a3249a5910b2e74f89cd9bf","0f29274c490b4a02a7ce3921955ca565","4f1287c5e0f149c0959c3247992ef0af","5113fffe3f6a4b1fbff3afadda028068","ffffab88d2e04562b1e0c133e9d2764e","22c76de64a374667b4cde49e0a7d4b4e","59ef306372424cd7822829a7ddfbae79","3a21c25863d84d1ca62025d0755bf65b","b8a4529826594414a1701570991c5640","f447d3ae15304ce98a181fa1f2b9e1b4","fe8f91a798324555a3672530adc7c5ed","415f69b114274d12bb6e0d997b57d067","a3e696dfbe1645cf94cb80e62784a30d","35064135e7a54a999a73b34c9d211279","47b2008e180a4d3a85d6b9009b7d33b2","e6bee7edf3f04ff28bc3d6d3c2fa5894","101cf4d982a44a9daeecf54016a39bff","0716e0677b384dac9977e3d5e3c9f8c3","f2f30ce079e340ff9827cbccee5509d7","ca245a836b9745f381d0a26254846eb8","46b5ac98c2fe4b1db3bac561db073a77","f5dae5f5521a4f79bbaa2e91a0371728","fd1aa9ba656045d99d4331ed2ab144b1","b6b2999c2360421a901084f4e4a52abc","70b51b87562a4749b15788550e3e7340","db9930716a014416a0289f35c8c5974d","41ce1358eacc47a4af833497a0e95778","a59ca85c7ef44e6eace427771e5dab2c","028c25d000c447f4a2d69b391f20e20b","7fdadc1d8487410285cf6ac2c7027e74","d7e332208a61497d9ad065401c8462c6","b5a3395148f148a1b8d5d87b5a9e7594","cc54a2456c2744f995bf4fa0729e900e","d7df0992a18049a796a45684a6a6a521","a2d7ddd5aea5474083e2882c070c73fe","6433a26cc91b468baec695339fe5d2ee","a0bcaa22a08d4737a64aea5d6210e538","608a3e86a64543e0bdce09a4072bdaac","c61f0486963747bda243be653264eb6e","c9ff5b3eb9c64b3794a75997c6035373","b4809e789405467e8fd166889b3e6b24","a478820f2cda4259b2e71e762317cd71","dc3e3e122cc243beac7003aa6c2ef41c","4cc43cd388234c828e937bbc10d1c239","9d5cbe142b9141e4b4ab1bc0640dd54a","4f80b088cc6f45ef834fc26b860566b9","33ecbc483cdb4b40b466c536a11e68d9","39060a05182d48078fe0a95577aa074c","27971f698e5a4598aab3a128c66a6a43","005bc59e95034b42b23d92f274457a2c","5bee75182e8c4c3a8c617a90dec78c93","56909c7e1d7947bdb674110de36506df","00fb7f7b0abb402ab903c7de5ffd709d","caf20e8de2464fa780431a826dcb8a20","63192c0ed1264169884992ee07555feb","1abe548c4f2543d49962b4c7d6d749c1","11ffe3e394014239a18210dc335773dd","213414cbac11413da2bad90a3aac2178","f331bcf0b9324c7294c17d30c5718591","1ccad991ab49446aba842407475f0fe2","ad3c30caf452467d9821409e6dccddb6","8589d34183804f83a1caacb2fd33d6eb","a74db07361ba4f97ba7d6ff39dceac76","259f8c337fb34b3f831343505ebf5ec8","147f198bbbfb4d0f96169d4afaf1ad20"],"height":278},"executionInfo":{"elapsed":31512,"status":"ok","timestamp":1698343867178,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"J_cd_q8q4uuK","outputId":"5b925ac2-5bd6-4474-de05-d8250e2fb07d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0d8589c1dd4390a9dc2bde136536a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe8f91a798324555a3672530adc7c5ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5dae5f5521a4f79bbaa2e91a0371728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc54a2456c2744f995bf4fa0729e900e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cc43cd388234c828e937bbc10d1c239"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63192c0ed1264169884992ee07555feb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/easy_transformer/components.py:616: UserWarning: Moved LN1 to the attention block\n","  warnings.warn(\"Moved LN1 to the attention block\")\n"]},{"output_type":"stream","name":"stdout","text":["Moving model to device:  cuda\n","Finished loading pretrained model gpt2-medium into EasyTransformer!\n"]}],"source":["model = EasyTransformer.from_pretrained(\"gpt2-medium\").cuda()\n","# model = EasyTransformer.from_pretrained(\"gpt2\")\n","model.set_use_attn_result(True)"]},{"cell_type":"markdown","source":["# Generate dataset with multiple prompts"],"metadata":{"id":"cGX9iHAz_UKX"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        # self.io_tokenIDs = [\n","        #     self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","        # ]\n","        # self.s_tokenIDs = [\n","        #     self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","        # ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"4wXBNWj5FwVn","executionInfo":{"status":"ok","timestamp":1698343774929,"user_tz":240,"elapsed":822,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+2),\n","            'S3': str(i+4),\n","            'S4': str(i+6),\n","            'S5': str(i+8),\n","            'S6': str(i+10),\n","            # 'corr': str(i+12),\n","            # 'incorr': str(i+10),\n","            'text': f\"{i} {i+2} {i+4} {i+6} {i+8} {i+10}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 100)\n","dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"tub8nC_9Mp6a","executionInfo":{"status":"ok","timestamp":1698345041410,"user_tz":240,"elapsed":311,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmnpvUF-4uuN"},"source":["# add2 score on last digit of seq (the one to pred next)"]},{"cell_type":"code","source":["def get_addTwo_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_added = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        # for word in words:\n","        word = words[-1]\n","\n","        pred_tokens = [\n","            model.tokenizer.decode(token)\n","            for token in torch.topk(\n","                logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","            ).indices\n","        ]\n","\n","        # get prev member after digit prompt[word]\n","        targword = str(int(prompt[word]) + 2)\n","\n","        targToken_in_topK = 'no'\n","        if \" \" + targword in pred_tokens or targword in pred_tokens:\n","            n_right += 1\n","            words_added.append(prompt[word])\n","            targToken_in_topK = 'yes'\n","            # if prompt[word] == '99':\n","            #     pdb.set_trace()\n","        # if prompt[word] in pred_tokens_dict:\n","        #     pdb.set_trace()\n","        pred_tokens_dict[prompt[word]] = (pred_tokens, targword, targToken_in_topK)\n","\n","    # percent_right = (n_right / (dataset.N * len(words))) * 100\n","    percent_right = (n_right / (dataset.N)) * 100\n","    if percent_right > 0:\n","        print(f\"Head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        print(n_right)\n","        print((dataset.N))\n","        print(words_added)\n","        return pred_tokens_dict\n","    else:\n","        if percent_right > 0:\n","            return words_added"],"metadata":{"id":"7Tp-ILOn9ae2","executionInfo":{"status":"ok","timestamp":1698345045108,"user_tz":240,"elapsed":23,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["## Find add2 heads of all gpt2 med"],"metadata":{"id":"_bo5zg6tU3jU"}},{"cell_type":"code","source":["all_heads = [(layer, head) for layer in range(24) for head in range(16)]\n","for index, (layer, head) in enumerate(all_heads):\n","    results = get_addTwo_scores(model, layer, head, dataset, print_tokens=False)\n","    if results:\n","        print((layer, head), results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fe7EONJ05szB","executionInfo":{"status":"ok","timestamp":1698346356320,"user_tz":240,"elapsed":79809,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6ae53c87-398a-41ad-e80f-fc79a7ab395d"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Head 5.8 (sign=1) : Top 5 accuracy: 1.0101010101010102%\n","(5, 8) ['29']\n","Head 6.1 (sign=1) : Top 5 accuracy: 3.0303030303030303%\n","(6, 1) ['26', '33', '36']\n","Head 7.2 (sign=1) : Top 5 accuracy: 6.0606060606060606%\n","(7, 2) ['26', '32', '34', '36', '46', '74']\n","Head 9.4 (sign=1) : Top 5 accuracy: 15.151515151515152%\n","(9, 4) ['12', '13', '18', '21', '23', '27', '28', '29', '32', '38', '43', '45', '48', '63', '68']\n","Head 9.9 (sign=1) : Top 5 accuracy: 15.151515151515152%\n","(9, 9) ['19', '26', '30', '34', '36', '39', '49', '50', '53', '69', '74', '93', '102', '103', '105']\n","Head 10.1 (sign=1) : Top 5 accuracy: 6.0606060606060606%\n","(10, 1) ['19', '26', '29', '32', '33', '34']\n","Head 10.8 (sign=1) : Top 5 accuracy: 38.38383838383838%\n","(10, 8) ['12', '14', '17', '19', '20', '25', '26', '30', '32', '34', '39', '41', '44', '45', '49', '50', '51', '52', '53', '62', '66', '69', '70', '71', '74', '76', '79', '82', '84', '87', '94', '99', '102', '103', '104', '106', '107', '109']\n","Head 11.1 (sign=1) : Top 5 accuracy: 28.28282828282828%\n","(11, 1) ['20', '30', '34', '36', '45', '49', '50', '51', '60', '61', '66', '68', '69', '70', '71', '74', '83', '85', '89', '90', '91', '94', '95', '96', '100', '101', '103', '105']\n","Head 12.0 (sign=1) : Top 5 accuracy: 2.0202020202020203%\n","(12, 0) ['63', '95']\n","Head 12.13 (sign=1) : Top 5 accuracy: 24.242424242424242%\n","(12, 13) ['12', '13', '14', '17', '18', '19', '20', '22', '24', '27', '29', '32', '33', '42', '43', '44', '49', '62', '72', '82', '94', '102', '104', '106']\n","Head 12.15 (sign=1) : Top 5 accuracy: 6.0606060606060606%\n","(12, 15) ['61', '62', '93', '96', '97', '107']\n","Head 13.0 (sign=1) : Top 5 accuracy: 4.040404040404041%\n","(13, 0) ['13', '29', '33', '98']\n","Head 13.12 (sign=1) : Top 5 accuracy: 9.090909090909092%\n","(13, 12) ['50', '69', '71', '79', '89', '90', '91', '101', '103']\n","Head 13.14 (sign=1) : Top 5 accuracy: 3.0303030303030303%\n","(13, 14) ['74', '97', '103']\n","Head 14.5 (sign=1) : Top 5 accuracy: 11.11111111111111%\n","(14, 5) ['11', '12', '14', '19', '24', '26', '29', '36', '49', '74', '101']\n","Head 14.9 (sign=1) : Top 5 accuracy: 2.0202020202020203%\n","(14, 9) ['29', '89']\n","Head 14.14 (sign=1) : Top 5 accuracy: 57.57575757575758%\n","(14, 14) ['20', '30', '31', '32', '34', '35', '36', '37', '40', '41', '47', '49', '50', '51', '52', '53', '56', '57', '60', '61', '62', '63', '64', '66', '67', '68', '69', '70', '71', '72', '74', '75', '76', '79', '80', '81', '82', '83', '84', '85', '86', '87', '90', '91', '92', '93', '95', '96', '100', '101', '103', '104', '105', '106', '107', '108', '109']\n","Head 15.5 (sign=1) : Top 5 accuracy: 24.242424242424242%\n","(15, 5) ['12', '13', '14', '15', '16', '17', '19', '20', '24', '25', '26', '29', '32', '39', '49', '52', '56', '60', '63', '65', '68', '69', '105', '106']\n","Head 15.12 (sign=1) : Top 5 accuracy: 8.080808080808081%\n","(15, 12) ['26', '29', '32', '33', '82', '83', '91', '92']\n","Head 16.15 (sign=1) : Top 5 accuracy: 2.0202020202020203%\n","(16, 15) ['74', '84']\n","Head 17.2 (sign=1) : Top 5 accuracy: 12.121212121212121%\n","(17, 2) ['11', '21', '23', '26', '61', '73', '80', '81', '85', '91', '95', '96']\n","Head 18.3 (sign=1) : Top 5 accuracy: 1.0101010101010102%\n","(18, 3) ['74']\n","Head 19.12 (sign=1) : Top 5 accuracy: 73.73737373737373%\n","(19, 12) ['11', '12', '13', '14', '15', '16', '17', '19', '20', '21', '22', '24', '25', '26', '27', '29', '30', '31', '32', '35', '36', '37', '39', '40', '41', '42', '44', '45', '46', '47', '49', '50', '51', '52', '54', '56', '57', '59', '60', '61', '64', '65', '66', '67', '69', '71', '72', '74', '75', '76', '77', '79', '81', '82', '84', '85', '86', '87', '89', '90', '91', '92', '94', '95', '96', '97', '100', '101', '104', '105', '106', '107', '109']\n","Head 20.8 (sign=1) : Top 5 accuracy: 31.313131313131315%\n","(20, 8) ['11', '12', '15', '20', '21', '24', '25', '26', '36', '42', '49', '51', '52', '53', '54', '61', '66', '84', '85', '86', '90', '91', '94', '95', '97', '100', '101', '102', '103', '104', '109']\n","Head 21.0 (sign=1) : Top 5 accuracy: 54.54545454545454%\n","(21, 0) ['14', '19', '21', '22', '26', '27', '29', '32', '35', '36', '37', '39', '41', '42', '44', '45', '47', '49', '51', '52', '54', '56', '57', '59', '61', '64', '65', '66', '67', '69', '71', '72', '74', '76', '77', '79', '81', '82', '84', '86', '87', '89', '91', '92', '93', '94', '95', '96', '101', '102', '104', '105', '107', '109']\n","Head 23.1 (sign=1) : Top 5 accuracy: 18.181818181818183%\n","(23, 1) ['11', '12', '14', '16', '17', '19', '27', '35', '36', '37', '61', '65', '74', '76', '77', '79', '99', '105']\n"]}]},{"cell_type":"markdown","source":["## Find add2 heads of add2 circ"],"metadata":{"id":"7Ir5Ct8U-VUE"}},{"cell_type":"code","source":["circ_246 =  [(0, 2), (0, 3), (0, 4), (0, 5), (0, 9), (0, 10), (0, 14), (1, 2), (1, 4), (1, 7), (1, 14), (2, 3), (2, 4), (2, 5), (2, 7), (2, 8), (2, 9), (2, 15), (3, 0), (3, 3), (3, 13), (3, 14), (3, 15), (4, 2), (4, 6), (4, 8), (4, 10), (4, 11), (5, 8), (6, 14), (6, 15), (7, 2), (7, 11), (7, 13), (8, 0), (9, 3), (9, 4), (9, 5), (9, 6), (9, 12), (9, 15), (10, 1), (10, 4), (10, 9), (10, 10), (10, 13), (10, 14), (11, 1), (11, 4), (11, 5), (11, 8), (12, 1), (12, 4), (12, 12), (12, 13), (12, 15), (13, 5), (13, 12), (13, 13), (14, 5), (14, 14), (15, 5), (15, 7), (15, 11), (15, 12), (15, 15), (16, 6), (16, 7), (16, 9), (16, 11), (16, 13), (16, 14), (17, 0), (17, 1), (17, 12), (18, 3), (18, 11), (18, 13), (19, 1), (19, 4), (20, 0), (20, 1), (20, 14), (21, 0), (21, 2), (21, 7)]\n","for index, (layer, head) in enumerate(circ_246):\n","    print((layer, head), get_addTwo_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsbSH4Gdx6kB","executionInfo":{"status":"ok","timestamp":1698345069459,"user_tz":240,"elapsed":18419,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e954be9f-3fea-4810-dff8-ad8d6e1c76f7"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["(0, 2) None\n","(0, 3) None\n","(0, 4) None\n","(0, 5) None\n","(0, 9) None\n","(0, 10) None\n","(0, 14) None\n","(1, 2) None\n","(1, 4) None\n","(1, 7) None\n","(1, 14) None\n","(2, 3) None\n","(2, 4) None\n","(2, 5) None\n","(2, 7) None\n","(2, 8) None\n","(2, 9) None\n","(2, 15) None\n","(3, 0) None\n","(3, 3) None\n","(3, 13) None\n","(3, 14) None\n","(3, 15) None\n","(4, 2) None\n","(4, 6) None\n","(4, 8) None\n","(4, 10) None\n","(4, 11) None\n","Head 5.8 (sign=1) : Top 5 accuracy: 1.0101010101010102%\n","(5, 8) ['29']\n","(6, 14) None\n","(6, 15) None\n","Head 7.2 (sign=1) : Top 5 accuracy: 6.0606060606060606%\n","(7, 2) ['26', '32', '34', '36', '46', '74']\n","(7, 11) None\n","(7, 13) None\n","(8, 0) None\n","(9, 3) None\n","Head 9.4 (sign=1) : Top 5 accuracy: 15.151515151515152%\n","(9, 4) ['12', '13', '18', '21', '23', '27', '28', '29', '32', '38', '43', '45', '48', '63', '68']\n","(9, 5) None\n","(9, 6) None\n","(9, 12) None\n","(9, 15) None\n","Head 10.1 (sign=1) : Top 5 accuracy: 6.0606060606060606%\n","(10, 1) ['19', '26', '29', '32', '33', '34']\n","(10, 4) None\n","(10, 9) None\n","(10, 10) None\n","(10, 13) None\n","(10, 14) None\n","Head 11.1 (sign=1) : Top 5 accuracy: 28.28282828282828%\n","(11, 1) ['20', '30', '34', '36', '45', '49', '50', '51', '60', '61', '66', '68', '69', '70', '71', '74', '83', '85', '89', '90', '91', '94', '95', '96', '100', '101', '103', '105']\n","(11, 4) None\n","(11, 5) None\n","(11, 8) None\n","(12, 1) None\n","(12, 4) None\n","(12, 12) None\n","Head 12.13 (sign=1) : Top 5 accuracy: 24.242424242424242%\n","(12, 13) ['12', '13', '14', '17', '18', '19', '20', '22', '24', '27', '29', '32', '33', '42', '43', '44', '49', '62', '72', '82', '94', '102', '104', '106']\n","Head 12.15 (sign=1) : Top 5 accuracy: 6.0606060606060606%\n","(12, 15) ['61', '62', '93', '96', '97', '107']\n","(13, 5) None\n","Head 13.12 (sign=1) : Top 5 accuracy: 9.090909090909092%\n","(13, 12) ['50', '69', '71', '79', '89', '90', '91', '101', '103']\n","(13, 13) None\n","Head 14.5 (sign=1) : Top 5 accuracy: 11.11111111111111%\n","(14, 5) ['11', '12', '14', '19', '24', '26', '29', '36', '49', '74', '101']\n","Head 14.14 (sign=1) : Top 5 accuracy: 57.57575757575758%\n","(14, 14) ['20', '30', '31', '32', '34', '35', '36', '37', '40', '41', '47', '49', '50', '51', '52', '53', '56', '57', '60', '61', '62', '63', '64', '66', '67', '68', '69', '70', '71', '72', '74', '75', '76', '79', '80', '81', '82', '83', '84', '85', '86', '87', '90', '91', '92', '93', '95', '96', '100', '101', '103', '104', '105', '106', '107', '108', '109']\n","Head 15.5 (sign=1) : Top 5 accuracy: 24.242424242424242%\n","(15, 5) ['12', '13', '14', '15', '16', '17', '19', '20', '24', '25', '26', '29', '32', '39', '49', '52', '56', '60', '63', '65', '68', '69', '105', '106']\n","(15, 7) None\n","(15, 11) None\n","Head 15.12 (sign=1) : Top 5 accuracy: 8.080808080808081%\n","(15, 12) ['26', '29', '32', '33', '82', '83', '91', '92']\n","(15, 15) None\n","(16, 6) None\n","(16, 7) None\n","(16, 9) None\n","(16, 11) None\n","(16, 13) None\n","(16, 14) None\n","(17, 0) None\n","(17, 1) None\n","(17, 12) None\n","Head 18.3 (sign=1) : Top 5 accuracy: 1.0101010101010102%\n","(18, 3) ['74']\n","(18, 11) None\n","(18, 13) None\n","(19, 1) None\n","(19, 4) None\n","(20, 0) None\n","(20, 1) None\n","(20, 14) None\n","Head 21.0 (sign=1) : Top 5 accuracy: 54.54545454545454%\n","(21, 0) ['14', '19', '21', '22', '26', '27', '29', '32', '35', '36', '37', '39', '41', '42', '44', '45', '47', '49', '51', '52', '54', '56', '57', '59', '61', '64', '65', '66', '67', '69', '71', '72', '74', '76', '77', '79', '81', '82', '84', '86', '87', '89', '91', '92', '93', '94', '95', '96', '101', '102', '104', '105', '107', '109']\n","(21, 2) None\n","(21, 7) None\n"]}]},{"cell_type":"code","source":["get_addTwo_scores(model, 14, 14, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rTl9Yd0Bcro","executionInfo":{"status":"ok","timestamp":1698345080400,"user_tz":240,"elapsed":416,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d11b7e09-6ff7-434b-a21c-89c6bd410bb9"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Head 14.14 (sign=1) : Top 5 accuracy: 57.57575757575758%\n","57\n","99\n","['20', '30', '31', '32', '34', '35', '36', '37', '40', '41', '47', '49', '50', '51', '52', '53', '56', '57', '60', '61', '62', '63', '64', '66', '67', '68', '69', '70', '71', '72', '74', '75', '76', '79', '80', '81', '82', '83', '84', '85', '86', '87', '90', '91', '92', '93', '95', '96', '100', '101', '103', '104', '105', '106', '107', '108', '109']\n"]},{"output_type":"execute_result","data":{"text/plain":["{'11': ([' 12', ' Eleven', ' 11', ' Fields', '12'], '13', 'no'),\n"," '12': ([' 13', '13', ' 12', '12', ' thirteen'], '14', 'no'),\n"," '13': ([' 14', ' Sieg', ' 13', ' 1400', '13'], '15', 'no'),\n"," '14': ([' 15', '15', ' 14', '14', '1500'], '16', 'no'),\n"," '15': ([' 16', ' 15', '16', ' 1600', '15'], '17', 'no'),\n"," '16': ([' 17', '17', ' 1700', ' 16', 'eenth'], '18', 'no'),\n"," '17': ([' 18', ' 17', ' 1800', ' 179', '17'], '19', 'no'),\n"," '18': ([' 19', 'eteenth', ' Mae', ' 18', '19'], '20', 'no'),\n"," '19': ([' 20', ' 19', ' 1919', '20', ' 1920'], '21', 'no'),\n"," '20': ([' 21', '21', ' 20', '221', ' 22'], '22', 'yes'),\n"," '21': ([' 22', ' 1921', ' 1922', ' 222', '22'], '23', 'no'),\n"," '22': ([' 1923', ' 23', ' 223', '23', ' 1922'], '24', 'no'),\n"," '23': ([' 24', ' 23', ' 1923', '24', ' 1924'], '25', 'no'),\n"," '24': ([' 25', '25', ' 24', '24', ' 249'], '26', 'no'),\n"," '25': ([' 26', '26', ' 25', 'NR', '25'], '27', 'no'),\n"," '26': ([' 27', '27', '26', ' 26', ' 267'], '28', 'no'),\n"," '27': ([' 28', ' 27', '28', '27', ' 278'], '29', 'no'),\n"," '28': (['29', ' 29', '28', ' 28', '028'], '30', 'no'),\n"," '29': ([' 30', '30', '29', '��', ' 29'], '31', 'no'),\n"," '30': (['31', ' 31', ' 32', ' 30', '30'], '32', 'yes'),\n"," '31': ([' 32', '32', '31', ' 31', '33'], '33', 'yes'),\n"," '32': ([' 33', '33', ' 32', '32', ' 34'], '34', 'yes'),\n"," '33': ([' 34', ' 33', '33', '34', ' 334'], '35', 'no'),\n"," '34': ([' 35', ' 34', '35', '34', ' 36'], '36', 'yes'),\n"," '35': ([' 36', ' 35', '36', '35', ' 37'], '37', 'yes'),\n"," '36': ([' 37', '37', ' 36', ' 38', '36'], '38', 'yes'),\n"," '37': ([' 38', ' 37', '38', ' Krug', ' 39'], '39', 'yes'),\n"," '38': ([' 39', ' 38', 'SR', '38', '39'], '40', 'no'),\n"," '39': ([' 39', ' 40', '39', '40', ' 1840'], '41', 'no'),\n"," '40': ([' 41', '41', ' 40', '441', '42'], '42', 'yes'),\n"," '41': ([' 42', '42', ' 41', '41', ' 43'], '43', 'yes'),\n"," '42': ([' 43', '43', ' 1943', ' 42', ' Thompson'], '44', 'no'),\n"," '43': ([' 44', ' 43', '44', '43', ' 1944'], '45', 'no'),\n"," '44': ([' 44', '44', ' 45', '45', '544'], '46', 'no'),\n"," '45': ([' 46', '46', ' 45', '45', ' Faust'], '47', 'no'),\n"," '46': ([' 47', '47', '46', ' 46', '�'], '48', 'no'),\n"," '47': ([' 48', ' 47', '47', '48', ' 49'], '49', 'yes'),\n"," '48': ([' 49', '49', ' 48', '48', ' marqu'], '50', 'no'),\n"," '49': ([' 49', ' 50', '49', '50', ' 51'], '51', 'yes'),\n"," '50': ([' 51', '51', ' 52', '52', '551'], '52', 'yes'),\n"," '51': ([' 52', '52', ' 51', '51', ' 53'], '53', 'yes'),\n"," '52': ([' 53', '53', ' 52', '52', ' 54'], '54', 'yes'),\n"," '53': ([' 54', ' 53', '54', '53', ' 55'], '55', 'yes'),\n"," '54': ([' 55', ' 54', '55', '54', ' Nek'], '56', 'no'),\n"," '55': ([' 56', ' 55', '56', '55', ' chim'], '57', 'no'),\n"," '56': ([' 57', '57', '56', ' 56', ' 58'], '58', 'yes'),\n"," '57': ([' 58', ' 57', '58', '57', ' 59'], '59', 'yes'),\n"," '58': ([' 59', ' 58', '58', '59', ' 1959'], '60', 'no'),\n"," '59': ([' 60', '60', ' 59', '59', 'ECH'], '61', 'no'),\n"," '60': ([' 61', '61', ' 62', '62', ' 60'], '62', 'yes'),\n"," '61': ([' 62', '62', ' 61', ' 63', '63'], '63', 'yes'),\n"," '62': ([' 63', '63', ' 62', '62', '64'], '64', 'yes'),\n"," '63': ([' 63', ' 64', '64', '63', ' 65'], '65', 'yes'),\n"," '64': ([' 65', '65', ' 64', '64', ' 66'], '66', 'yes'),\n"," '65': ([' 66', '66', ' 65', '65', ' 68'], '67', 'no'),\n"," '66': ([' 67', '67', ' 66', ' 68', '66'], '68', 'yes'),\n"," '67': ([' 68', ' 67', ' 69', '68', '67'], '69', 'yes'),\n"," '68': ([' 69', ' 68', '69', '68', ' 70'], '70', 'yes'),\n"," '69': ([' 69', ' 70', '69', '70', ' 71'], '71', 'yes'),\n"," '70': ([' 71', '71', ' 72', ' 70', ' 73'], '72', 'yes'),\n"," '71': ([' 72', ' 71', ' 73', '72', ' 76'], '73', 'yes'),\n"," '72': ([' 73', ' 72', '73', ' 74', ' 77'], '74', 'yes'),\n"," '73': ([' 74', ' 73', '74', '73', ' 76'], '75', 'no'),\n"," '74': ([' 75', ' 74', '74', ' 76', '75'], '76', 'yes'),\n"," '75': ([' 76', '76', ' 77', ' 75', ' Marqu'], '77', 'yes'),\n"," '76': ([' 77', '77', ' 76', '76', ' 78'], '78', 'yes'),\n"," '77': ([' 78', ' 77', '78', '77', ' 978'], '79', 'no'),\n"," '78': ([' 79', ' 78', '79', '78', ' 1979'], '80', 'no'),\n"," '79': ([' 79', ' 80', '79', '80', ' 81'], '81', 'yes'),\n"," '80': ([' 81', '81', ' 82', ' 80', ' 84'], '82', 'yes'),\n"," '81': ([' 82', ' 81', '82', ' 83', ' 84'], '83', 'yes'),\n"," '82': ([' 83', ' 82', '83', ' 84', ' 1983'], '84', 'yes'),\n"," '83': ([' 84', ' 83', '84', ' 85', '83'], '85', 'yes'),\n"," '84': ([' 84', ' 85', '85', '84', ' 86'], '86', 'yes'),\n"," '85': ([' 86', '86', ' 85', '85', ' 87'], '87', 'yes'),\n"," '86': ([' 87', '87', ' 86', '86', ' 88'], '88', 'yes'),\n"," '87': ([' 88', ' 87', ' 89', '88', '87'], '89', 'yes'),\n"," '88': ([' 89', ' 88', '89', '88', '889'], '90', 'no'),\n"," '89': ([' 89', '89', ' 90', ' 88', '90'], '91', 'no'),\n"," '90': ([' 91', ' 90', '91', ' 92', ' 89'], '92', 'yes'),\n"," '91': ([' 92', ' 91', ' 93', '92', ' 94'], '93', 'yes'),\n"," '92': ([' 93', ' 92', '93', ' 94', ' 1893'], '94', 'yes'),\n"," '93': ([' 94', ' 93', '94', ' 95', ' 1994'], '95', 'yes'),\n"," '94': ([' 94', ' 95', '94', '95', ' 1995'], '96', 'no'),\n"," '95': ([' 96', ' 95', '95', '96', ' 97'], '97', 'yes'),\n"," '96': ([' 97', ' 96', '97', ' 98', '96'], '98', 'yes'),\n"," '97': ([' 98', ' 97', '98', '97', ' 978'], '99', 'no'),\n"," '98': ([' 98', ' 99', '98', ' Sakura', 'olas'], '100', 'no'),\n"," '99': ([' 99', '99', ' 100', 'izo', 'apego'], '101', 'no'),\n"," '100': ([' 101', '101', ' 102', ' vulner', '1001'], '102', 'yes'),\n"," '101': ([' 102', '102', ' 101', ' 103', '101'], '103', 'yes'),\n"," '102': ([' 103', '103', ' 102', '102', ' 105'], '104', 'no'),\n"," '103': ([' 104', ' 103', ' 105', '103', '104'], '105', 'yes'),\n"," '104': (['105', ' 105', '104', ' 104', '106'], '106', 'yes'),\n"," '105': ([' 105', ' 106', '106', '105', ' 107'], '107', 'yes'),\n"," '106': ([' 107', ' 106', '107', '106', ' 108'], '108', 'yes'),\n"," '107': ([' 108', ' 107', '108', '107', ' 109'], '109', 'yes'),\n"," '108': ([' 109', ' 108', '109', ' 110', '108'], '110', 'yes'),\n"," '109': ([' 110', '110', ' 109', '109', ' 111'], '111', 'yes')}"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["# Compare Copy scores for last pos"],"metadata":{"id":"bS_7sy-8BMBu"}},{"cell_type":"code","source":["def get_copy_scores(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_moved = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        # for word in words:\n","        word = words[-1]\n","\n","        pred_tokens = [\n","            model.tokenizer.decode(token)\n","            for token in torch.topk(\n","                logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","            ).indices\n","        ]\n","\n","        token_in_topK = 'no'\n","        if \" \" + prompt[word] in pred_tokens or prompt[word] in pred_tokens:\n","            n_right += 1\n","            words_moved.append(prompt[word])\n","            token_in_topK = 'yes'\n","        pred_tokens_dict[prompt[word]] = (pred_tokens, token_in_topK)\n","\n","    # percent_right = (n_right / (dataset.N * len(words))) * 100\n","    percent_right = (n_right / (dataset.N)) * 100\n","    print(f\"Copy circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        return pred_tokens_dict\n","    else:\n","        return words_moved"],"metadata":{"id":"v_ZNkjTzBP3D","executionInfo":{"status":"ok","timestamp":1698347438433,"user_tz":240,"elapsed":75,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["for index, (layer, head) in enumerate(circ_246):\n","    print((layer, head), get_copy_scores(model, layer, head, dataset, print_tokens=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RC9ONjNG-9hp","executionInfo":{"status":"ok","timestamp":1698347461588,"user_tz":240,"elapsed":19946,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0abd15a0-9e04-4a35-95f2-981672d8066a"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Copy circuit for head 0.2 (sign=1) : Top 5 accuracy: 0.0%\n","(0, 2) []\n","Copy circuit for head 0.3 (sign=1) : Top 5 accuracy: 0.0%\n","(0, 3) []\n","Copy circuit for head 0.4 (sign=1) : Top 5 accuracy: 0.0%\n","(0, 4) []\n","Copy circuit for head 0.5 (sign=1) : Top 5 accuracy: 0.0%\n","(0, 5) []\n","Copy circuit for head 0.9 (sign=1) : Top 5 accuracy: 0.0%\n","(0, 9) []\n","Copy circuit for head 0.10 (sign=1) : Top 5 accuracy: 0.0%\n","(0, 10) []\n","Copy circuit for head 0.14 (sign=1) : Top 5 accuracy: 0.0%\n","(0, 14) []\n","Copy circuit for head 1.2 (sign=1) : Top 5 accuracy: 0.0%\n","(1, 2) []\n","Copy circuit for head 1.4 (sign=1) : Top 5 accuracy: 0.0%\n","(1, 4) []\n","Copy circuit for head 1.7 (sign=1) : Top 5 accuracy: 0.0%\n","(1, 7) []\n","Copy circuit for head 1.14 (sign=1) : Top 5 accuracy: 0.0%\n","(1, 14) []\n","Copy circuit for head 2.3 (sign=1) : Top 5 accuracy: 0.0%\n","(2, 3) []\n","Copy circuit for head 2.4 (sign=1) : Top 5 accuracy: 0.0%\n","(2, 4) []\n","Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.0%\n","(2, 5) []\n","Copy circuit for head 2.7 (sign=1) : Top 5 accuracy: 0.0%\n","(2, 7) []\n","Copy circuit for head 2.8 (sign=1) : Top 5 accuracy: 0.0%\n","(2, 8) []\n","Copy circuit for head 2.9 (sign=1) : Top 5 accuracy: 0.0%\n","(2, 9) []\n","Copy circuit for head 2.15 (sign=1) : Top 5 accuracy: 0.0%\n","(2, 15) []\n","Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 0.0%\n","(3, 0) []\n","Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 0.0%\n","(3, 3) []\n","Copy circuit for head 3.13 (sign=1) : Top 5 accuracy: 0.0%\n","(3, 13) []\n","Copy circuit for head 3.14 (sign=1) : Top 5 accuracy: 0.0%\n","(3, 14) []\n","Copy circuit for head 3.15 (sign=1) : Top 5 accuracy: 0.0%\n","(3, 15) []\n","Copy circuit for head 4.2 (sign=1) : Top 5 accuracy: 0.0%\n","(4, 2) []\n","Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 0.0%\n","(4, 6) []\n","Copy circuit for head 4.8 (sign=1) : Top 5 accuracy: 0.0%\n","(4, 8) []\n","Copy circuit for head 4.10 (sign=1) : Top 5 accuracy: 0.0%\n","(4, 10) []\n","Copy circuit for head 4.11 (sign=1) : Top 5 accuracy: 0.0%\n","(4, 11) []\n","Copy circuit for head 5.8 (sign=1) : Top 5 accuracy: 2.0202020202020203%\n","(5, 8) ['31', '101']\n","Copy circuit for head 6.14 (sign=1) : Top 5 accuracy: 0.0%\n","(6, 14) []\n","Copy circuit for head 6.15 (sign=1) : Top 5 accuracy: 0.0%\n","(6, 15) []\n","Copy circuit for head 7.2 (sign=1) : Top 5 accuracy: 25.252525252525253%\n","(7, 2) ['12', '13', '14', '26', '28', '31', '32', '33', '34', '36', '38', '41', '44', '46', '48', '51', '52', '54', '66', '68', '72', '76', '82', '98', '101']\n","Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 0.0%\n","(7, 11) []\n","Copy circuit for head 7.13 (sign=1) : Top 5 accuracy: 0.0%\n","(7, 13) []\n","Copy circuit for head 8.0 (sign=1) : Top 5 accuracy: 0.0%\n","(8, 0) []\n","Copy circuit for head 9.3 (sign=1) : Top 5 accuracy: 0.0%\n","(9, 3) []\n","Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 20.2020202020202%\n","(9, 4) ['15', '16', '20', '23', '25', '29', '30', '31', '32', '34', '35', '40', '45', '47', '50', '58', '60', '65', '70', '108']\n","Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 0.0%\n","(9, 5) []\n","Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 0.0%\n","(9, 6) []\n","Copy circuit for head 9.12 (sign=1) : Top 5 accuracy: 0.0%\n","(9, 12) []\n","Copy circuit for head 9.15 (sign=1) : Top 5 accuracy: 0.0%\n","(9, 15) []\n","Copy circuit for head 10.1 (sign=1) : Top 5 accuracy: 28.28282828282828%\n","(10, 1) ['11', '12', '13', '14', '15', '16', '21', '22', '25', '26', '28', '31', '32', '34', '35', '36', '38', '51', '60', '66', '82', '85', '92', '93', '95', '101', '102', '105']\n","Copy circuit for head 10.4 (sign=1) : Top 5 accuracy: 0.0%\n","(10, 4) []\n","Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 0.0%\n","(10, 9) []\n","Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.0%\n","(10, 10) []\n","Copy circuit for head 10.13 (sign=1) : Top 5 accuracy: 0.0%\n","(10, 13) []\n","Copy circuit for head 10.14 (sign=1) : Top 5 accuracy: 0.0%\n","(10, 14) []\n","Copy circuit for head 11.1 (sign=1) : Top 5 accuracy: 100.0%\n","(11, 1) ['11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109']\n","Copy circuit for head 11.4 (sign=1) : Top 5 accuracy: 0.0%\n","(11, 4) []\n","Copy circuit for head 11.5 (sign=1) : Top 5 accuracy: 0.0%\n","(11, 5) []\n","Copy circuit for head 11.8 (sign=1) : Top 5 accuracy: 0.0%\n","(11, 8) []\n","Copy circuit for head 12.1 (sign=1) : Top 5 accuracy: 100.0%\n","(12, 1) ['11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109']\n","Copy circuit for head 12.4 (sign=1) : Top 5 accuracy: 0.0%\n","(12, 4) []\n","Copy circuit for head 12.12 (sign=1) : Top 5 accuracy: 0.0%\n","(12, 12) []\n","Copy circuit for head 12.13 (sign=1) : Top 5 accuracy: 53.535353535353536%\n","(12, 13) ['11', '12', '13', '14', '15', '16', '18', '19', '20', '21', '22', '24', '25', '26', '30', '31', '32', '34', '35', '36', '40', '41', '42', '44', '45', '46', '47', '48', '50', '51', '54', '57', '60', '64', '65', '70', '73', '74', '75', '76', '80', '84', '86', '90', '92', '95', '96', '100', '101', '104', '105', '106', '108']\n","Copy circuit for head 12.15 (sign=1) : Top 5 accuracy: 27.27272727272727%\n","(12, 15) ['27', '30', '33', '53', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '77', '78', '87', '88', '97', '98', '102', '103', '105', '107']\n","Copy circuit for head 13.5 (sign=1) : Top 5 accuracy: 0.0%\n","(13, 5) []\n","Copy circuit for head 13.12 (sign=1) : Top 5 accuracy: 98.98989898989899%\n","(13, 12) ['11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '101', '102', '103', '104', '105', '106', '107', '108', '109']\n","Copy circuit for head 13.13 (sign=1) : Top 5 accuracy: 0.0%\n","(13, 13) []\n","Copy circuit for head 14.5 (sign=1) : Top 5 accuracy: 14.14141414141414%\n","(14, 5) ['11', '12', '15', '16', '18', '20', '22', '25', '26', '30', '31', '32', '34', '70']\n","Copy circuit for head 14.14 (sign=1) : Top 5 accuracy: 95.95959595959596%\n","(14, 14) ['11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '101', '102', '103', '104', '105', '106', '107', '108', '109']\n","Copy circuit for head 15.5 (sign=1) : Top 5 accuracy: 24.242424242424242%\n","(15, 5) ['11', '12', '17', '18', '19', '20', '21', '24', '26', '27', '28', '31', '51', '58', '60', '65', '67', '68', '70', '71', '78', '80', '87', '108']\n","Copy circuit for head 15.7 (sign=1) : Top 5 accuracy: 0.0%\n","(15, 7) []\n","Copy circuit for head 15.11 (sign=1) : Top 5 accuracy: 0.0%\n","(15, 11) []\n","Copy circuit for head 15.12 (sign=1) : Top 5 accuracy: 66.66666666666666%\n","(15, 12) ['11', '12', '15', '16', '17', '19', '20', '22', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '40', '43', '44', '45', '47', '50', '55', '56', '57', '60', '61', '62', '63', '65', '66', '67', '70', '72', '73', '75', '76', '77', '80', '82', '83', '84', '85', '86', '87', '88', '90', '91', '92', '93', '96', '97', '98', '99', '101', '102', '104', '105', '106', '108', '109']\n","Copy circuit for head 15.15 (sign=1) : Top 5 accuracy: 0.0%\n","(15, 15) []\n","Copy circuit for head 16.6 (sign=1) : Top 5 accuracy: 0.0%\n","(16, 6) []\n","Copy circuit for head 16.7 (sign=1) : Top 5 accuracy: 0.0%\n","(16, 7) []\n","Copy circuit for head 16.9 (sign=1) : Top 5 accuracy: 0.0%\n","(16, 9) []\n","Copy circuit for head 16.11 (sign=1) : Top 5 accuracy: 0.0%\n","(16, 11) []\n","Copy circuit for head 16.13 (sign=1) : Top 5 accuracy: 0.0%\n","(16, 13) []\n","Copy circuit for head 16.14 (sign=1) : Top 5 accuracy: 0.0%\n","(16, 14) []\n","Copy circuit for head 17.0 (sign=1) : Top 5 accuracy: 0.0%\n","(17, 0) []\n","Copy circuit for head 17.1 (sign=1) : Top 5 accuracy: 3.0303030303030303%\n","(17, 1) ['73', '76', '82']\n","Copy circuit for head 17.12 (sign=1) : Top 5 accuracy: 0.0%\n","(17, 12) []\n","Copy circuit for head 18.3 (sign=1) : Top 5 accuracy: 1.0101010101010102%\n","(18, 3) ['76']\n","Copy circuit for head 18.11 (sign=1) : Top 5 accuracy: 0.0%\n","(18, 11) []\n","Copy circuit for head 18.13 (sign=1) : Top 5 accuracy: 10.1010101010101%\n","(18, 13) ['16', '30', '31', '32', '33', '46', '48', '63', '64', '100']\n","Copy circuit for head 19.1 (sign=1) : Top 5 accuracy: 0.0%\n","(19, 1) []\n","Copy circuit for head 19.4 (sign=1) : Top 5 accuracy: 0.0%\n","(19, 4) []\n","Copy circuit for head 20.0 (sign=1) : Top 5 accuracy: 44.44444444444444%\n","(20, 0) ['11', '12', '13', '14', '15', '16', '17', '18', '19', '21', '22', '23', '24', '25', '26', '27', '28', '29', '31', '32', '33', '34', '36', '37', '42', '43', '44', '46', '49', '51', '52', '53', '54', '55', '56', '57', '59', '61', '62', '66', '67', '83', '86', '96']\n","Copy circuit for head 20.1 (sign=1) : Top 5 accuracy: 0.0%\n","(20, 1) []\n","Copy circuit for head 20.14 (sign=1) : Top 5 accuracy: 0.0%\n","(20, 14) []\n","Copy circuit for head 21.0 (sign=1) : Top 5 accuracy: 100.0%\n","(21, 0) ['11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109']\n","Copy circuit for head 21.2 (sign=1) : Top 5 accuracy: 0.0%\n","(21, 2) []\n","Copy circuit for head 21.7 (sign=1) : Top 5 accuracy: 0.0%\n","(21, 7) []\n"]}]},{"cell_type":"markdown","source":["# Input the first token of seq, not last"],"metadata":{"id":"Nq137A958tMQ"}},{"cell_type":"code","source":["def get_addTwo_scores_token0(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_added = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        # for word in words:\n","        word = words[0]  ######### CHANGE TO THIS from last to first! #########\n","\n","        pred_tokens = [\n","            model.tokenizer.decode(token)\n","            for token in torch.topk(\n","                logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","            ).indices\n","        ]\n","\n","        # get +2 after digit prompt[word]\n","        targword = str(int(prompt[word]) + 2)\n","\n","        targToken_in_topK = 'no'\n","        if \" \" + targword in pred_tokens or targword in pred_tokens:\n","            n_right += 1\n","            words_added.append(prompt[word])\n","            targToken_in_topK = 'yes'\n","        pred_tokens_dict[prompt[word]] = (pred_tokens, targword, targToken_in_topK)\n","\n","    # percent_right = (n_right / (dataset.N * len(words))) * 100\n","    percent_right = (n_right / (dataset.N)) * 100\n","    if percent_right > 0:\n","        print(f\"Head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        print(n_right)\n","        print((dataset.N))\n","        print(words_added)\n","        return pred_tokens_dict\n","    else:\n","        if percent_right > 0:\n","            return words_added"],"metadata":{"id":"WEraVCo-8v8S","executionInfo":{"status":"ok","timestamp":1698347220262,"user_tz":240,"elapsed":324,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["get_addTwo_scores_token0(model, 14, 14, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlQX-8w184nX","executionInfo":{"status":"ok","timestamp":1698347226970,"user_tz":240,"elapsed":45,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7fa61049-f72c-45fd-9220-fcc7671fe41a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Head 14.14 (sign=1) : Top 5 accuracy: 8.080808080808081%\n","8\n","99\n","['20', '30', '40', '62', '71', '81', '86', '91']\n"]},{"output_type":"execute_result","data":{"text/plain":["{'1': (['bara', ' Ai', 'isphere', 'ahime', '²'], '3', 'no'),\n"," '2': (['3', 'ahime', 'alion', ' 3', 'atform'], '4', 'no'),\n"," '3': (['4', ' 4', 'daq', 'atform', 'iola'], '5', 'no'),\n"," '4': (['5', 'baugh', 'BB', 'Balt', 'acea'], '6', 'no'),\n"," '5': (['cill', ' 6', 'agu', ' brush', '6'], '7', 'no'),\n"," '6': ([' 7', '7', '�', ' 07', '�'], '8', 'no'),\n"," '7': ([' 8', ' 808', ' 7', ' Lawson', '8'], '9', 'no'),\n"," '8': ([' Caldwell', 'acea', ' 8', 'bara', ' 9'], '10', 'no'),\n"," '9': ([' Abe', ' Ark', ' AA', ' AQ', ' 9'], '11', 'no'),\n"," '10': (['1111', '11', 'CT', '910', ' 11'], '12', 'no'),\n"," '11': (['arta', '12', 'IDA', ' 12', ' Dawson'], '13', 'no'),\n"," '12': (['arta', 'tailed', '13', '12', ' 13'], '14', 'no'),\n"," '13': ([' 14', '14', 'BW', ' 1914', ' 13'], '15', 'no'),\n"," '14': (['15', 'arta', '1500', ' 15', 'ayer'], '16', 'no'),\n"," '15': ([' 16', 'ayson', '16', '1600', 'bara'], '17', 'no'),\n"," '16': (['17', ' 17', 'alde', 'eenth', '�'], '18', 'no'),\n"," '17': ([' 18', ' DD', ' Awoken', 'ッド', ' 17'], '19', 'no'),\n"," '18': ([' ABE', 'bara', ' Abe', ' Caldwell', ' 19'], '20', 'no'),\n"," '19': ([' 20', 'izo', '20', 'enhagen', ' 19'], '21', 'no'),\n"," '20': ([' 21', '21', 'acon', ' 22', '221'], '22', 'yes'),\n"," '21': ([' 22', '22', '422', ' 1922', ' 222'], '23', 'no'),\n"," '22': ([' 1923', ' 23', ' 223', '23', '22'], '24', 'no'),\n"," '23': ([' 24', '24', ' 23', ' 1924', ' 1923'], '25', 'no'),\n"," '24': (['24', ' 25', ' 24', '25', ' ABE'], '26', 'no'),\n"," '25': ([' 26', '26', ' 25', '25', ' 1926'], '27', 'no'),\n"," '26': ([' 27', '27', ' 26', '26', ' 1927'], '28', 'no'),\n"," '27': ([' 28', '28', ' 27', ' 1928', '27'], '29', 'no'),\n"," '28': (['29', '�', 'bara', '28', ' 29'], '30', 'no'),\n"," '29': (['29', '30', ' 30', ' 29', ' 1930'], '31', 'no'),\n"," '30': (['31', ' 31', '30', '32', ' 32'], '32', 'yes'),\n"," '31': (['32', ' 32', '31', ' 332', ' 432'], '33', 'no'),\n"," '32': (['33', ' 33', 'lda', '32', '833'], '34', 'no'),\n"," '33': (['33', ' 33', ' 34', ' 334', ' 337'], '35', 'no'),\n"," '34': (['34', ' 35', '35', ' 34', ' 1934'], '36', 'no'),\n"," '35': (['36', ' 36', ' 35', '35', '�'], '37', 'no'),\n"," '36': ([' 37', '37', '36', ' Burke', ' 36'], '38', 'no'),\n"," '37': ([' 38', ' 37', '38', 'reb', ' Krug'], '39', 'no'),\n"," '38': ([' 1840', ' 39', ' 38', '38', '�'], '40', 'no'),\n"," '39': ([' 1840', ' 40', '40', ' 39', '39'], '41', 'no'),\n"," '40': (['41', ' 41', '42', 'bara', 'aban'], '42', 'yes'),\n"," '41': (['42', ' 42', '41', 'aban', 'arta'], '43', 'no'),\n"," '42': ([' 43', '43', ' 1943', '42', 'Ba'], '44', 'no'),\n"," '43': ([' 44', ' 43', ' 1944', ' 444', ' WD'], '45', 'no'),\n"," '44': (['bara', '44', ' 1948', ' 44', 'iations'], '46', 'no'),\n"," '45': (['46', 'bara', ' 46', 'PA', '45'], '47', 'no'),\n"," '46': (['47', ' 47', '46', '�', ' 747'], '48', 'no'),\n"," '47': (['47', ' 48', '48', ' 47', ' DB'], '49', 'no'),\n"," '48': (['bara', '48', '49', ' 49', 'sb'], '50', 'no'),\n"," '49': (['49', '50', ' 50', ' 49', ' 1850'], '51', 'no'),\n"," '50': (['51', ' 51', ' 1951', '50', '501'], '52', 'no'),\n"," '51': ([' 52', '52', '51', ' 51', '��極'], '53', 'no'),\n"," '52': ([' 53', 'lyak', '53', 'lda', 'ahime'], '54', 'no'),\n"," '53': ([' 53', ' 54', 'atform', '53', '54'], '55', 'no'),\n"," '54': ([' ANC', '55', ' 55', '855', 'acket'], '56', 'no'),\n"," '55': (['abo', ' 56', '56', '855', ' ANC'], '57', 'no'),\n"," '56': ([' 57', 'Cath', '56', '57', ' Burke'], '58', 'no'),\n"," '57': (['58', 'BW', 'CV', ' 58', 'ucl'], '59', 'no'),\n"," '58': (['58', ' 1959', ' 58', 'VC', '1959'], '60', 'no'),\n"," '59': (['60', ' Doll', 'daq', 'adelphia', 'CE'], '61', 'no'),\n"," '60': (['61', ' 61', 'elf', 'ELF', '60'], '62', 'no'),\n"," '61': ([' 62', '62', '61', ' 61', 'inelli'], '63', 'no'),\n"," '62': ([' 63', '63', ' 62', '62', '64'], '64', 'yes'),\n"," '63': (['64', '63', ' 63', ' 64', '864'], '65', 'no'),\n"," '64': (['64', '665', 'vc', '65', 'otta'], '66', 'no'),\n"," '65': (['66', ' 66', '65', 'alez', 'cart'], '67', 'no'),\n"," '66': ([' BG', '67', 'angelo', 'gerald', ' 67'], '68', 'no'),\n"," '67': (['BG', ' 68', ' 67', ' BG', '67'], '69', 'no'),\n"," '68': ([' 68', 'BG', 'CG', ' 69', '68'], '70', 'no'),\n"," '69': (['BG', 'bg', ' BG', 'EG', 'CG'], '71', 'no'),\n"," '70': ([' 71', '71', 'BG', 'aban', 'bg'], '72', 'no'),\n"," '71': (['aban', ' 72', '72', ' 71', ' 73'], '73', 'yes'),\n"," '72': ([' 73', '73', ' 72', 'amiya', 'aban'], '74', 'no'),\n"," '73': ([' 74', ' 73', '74', ' Lawson', '73'], '75', 'no'),\n"," '74': (['aban', '74', 'eva', ' DAC', ' 74'], '76', 'no'),\n"," '75': ([' 76', 'aban', '76', 'rain', 'gall'], '77', 'no'),\n"," '76': ([' 77', '77', '76', ' 76', 'utan'], '78', 'no'),\n"," '77': ([' 78', ' 77', ' 978', ' 1977', ' Seraph'], '79', 'no'),\n"," '78': ([' 78', '78', ' 79', '79', 'haar'], '80', 'no'),\n"," '79': (['79', ' WC', '80', ' 79', ' 80'], '81', 'no'),\n"," '80': ([' 81', '81', 'ao', 'aul', 'aos'], '82', 'no'),\n"," '81': ([' Lawson', ' 82', '82', ' 81', ' 83'], '83', 'yes'),\n"," '82': ([' 83', ' Lawson', 'lda', ' 82', '83'], '84', 'no'),\n"," '83': ([' 84', ' DAV', ' 83', 'lda', '84'], '85', 'no'),\n"," '84': (['84', ' 84', ' DAC', '85', ' 85'], '86', 'no'),\n"," '85': ([' 86', '86', ' 1886', 'aza', '85'], '87', 'no'),\n"," '86': ([' 87', '86', ' 86', '87', ' 88'], '88', 'yes'),\n"," '87': ([' 88', ' 87', '88', ' 1888', 'aza'], '89', 'no'),\n"," '88': ([' 88', '88', ' 89', '89', '889'], '90', 'no'),\n"," '89': (['89', ' 89', 'abbage', ' 90', 'iola'], '91', 'no'),\n"," '90': ([' 91', '91', ' Hawkins', 'ao', 'abo'], '92', 'no'),\n"," '91': ([' 92', ' 91', 'aji', ' 93', ' 96'], '93', 'yes'),\n"," '92': ([' 93', ' 1893', ' 92', '93', '1993'], '94', 'no'),\n"," '93': ([' 94', ' 93', ' 1893', '94', ' 1994'], '95', 'no'),\n"," '94': ([' 94', ' Ara', '94', ' 95', 'adish'], '96', 'no'),\n"," '95': ([' 96', 'abo', ' 1896', '96', ' 95'], '97', 'no'),\n"," '96': ([' 97', ' 96', '97', 'agna', '1997'], '98', 'no'),\n"," '97': ([' 98', ' 97', 'DCS', 'abbage', 'anks'], '99', 'no'),\n"," '98': (['gado', ' 98', '98', ' 99', 'aces'], '100', 'no'),\n"," '99': (['ague', ' 99', 'orest', 'iage', 'acular'], '101', 'no')}"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["# Input the second token of seq, not last"],"metadata":{"id":"xq3CDDcx9Cmo"}},{"cell_type":"code","source":["def get_addTwo_scores_token1(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_added = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        # for word in words:\n","        word = words[1]  ######### CHANGE TO THIS from last to 2nd! #########\n","\n","        pred_tokens = [\n","            model.tokenizer.decode(token)\n","            for token in torch.topk(\n","                logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","            ).indices\n","        ]\n","\n","        # get +2 after digit prompt[word]\n","        targword = str(int(prompt[word]) + 2)\n","\n","        targToken_in_topK = 'no'\n","        if \" \" + targword in pred_tokens or targword in pred_tokens:\n","            n_right += 1\n","            words_added.append(prompt[word])\n","            targToken_in_topK = 'yes'\n","        pred_tokens_dict[prompt[word]] = (pred_tokens, targword, targToken_in_topK)\n","\n","    # percent_right = (n_right / (dataset.N * len(words))) * 100\n","    percent_right = (n_right / (dataset.N)) * 100\n","    if percent_right > 0:\n","        print(f\"Head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        print(n_right)\n","        print((dataset.N))\n","        print(words_added)\n","        return pred_tokens_dict\n","    else:\n","        if percent_right > 0:\n","            return words_added"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698347272054,"user_tz":240,"elapsed":16,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"ojn785-n9Cmy"},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["get_addTwo_scores_token1(model, 14, 14, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698347275843,"user_tz":240,"elapsed":1533,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"761afe5e-765b-4299-c671-9d698f30af0a","id":"IpZCun1p9Cmz"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Head 14.14 (sign=1) : Top 5 accuracy: 47.474747474747474%\n","47\n","99\n","['25', '30', '32', '35', '36', '37', '40', '41', '44', '47', '49', '50', '51', '52', '53', '57', '60', '61', '62', '64', '66', '67', '68', '69', '70', '71', '72', '74', '75', '76', '79', '80', '81', '83', '84', '85', '86', '87', '89', '90', '91', '92', '93', '95', '96', '100', '101']\n"]},{"output_type":"execute_result","data":{"text/plain":["{'3': ([' fourth', ' 4', '4', ' four', ' Fourth'], '5', 'no'),\n"," '4': (['5', ' 5', ' fifth', ' 4', '五'], '6', 'no'),\n"," '5': ([' 6', ' sixth', ' brush', ' Sixth', '6'], '7', 'no'),\n"," '6': ([' 7', ' seventh', '7', ' Seventh', ' 6'], '8', 'no'),\n"," '7': ([' 7', ' 8', ' seventh', ' VIII', ' eighth'], '9', 'no'),\n"," '8': ([' 9', ' 8', ' ninth', '9', '889'], '10', 'no'),\n"," '9': ([' 980', ' 9', 'apo', ' Sapp', ' 10'], '11', 'no'),\n"," '10': ([' 11', '11', ' 111', '1111', ' 1911'], '12', 'no'),\n"," '11': ([' 12', ' Eleven', ' 1280', ' 11', ' sidx'], '13', 'no'),\n"," '12': ([' 13', '13', 'ASE', ' thirteen', ' 12'], '14', 'no'),\n"," '13': ([' 14', ' Sieg', ' 13', ' 1400', '14'], '15', 'no'),\n"," '14': (['15', ' 15', ' 14', '14', '1500'], '16', 'no'),\n"," '15': ([' 16', '16', ' 1600', ' 15', ' sixteen'], '17', 'no'),\n"," '16': ([' 17', '17', ' 1700', ' 16', ' seventeen'], '18', 'no'),\n"," '17': ([' 18', ' 17', ' 1800', ' 179', '17'], '19', 'no'),\n"," '18': ([' 19', ' 18', 'eteenth', ' Hammond', '19'], '20', 'no'),\n"," '19': ([' 19', ' 20', ' 1919', ' 1920', ' 190'], '21', 'no'),\n"," '20': ([' 21', '21', ' 1921', ' 20', ' 2021'], '22', 'no'),\n"," '21': ([' 1921', ' 22', ' 1922', ' 222', '22'], '23', 'no'),\n"," '22': ([' 1923', ' 23', ' 1922', ' 223', '23'], '24', 'no'),\n"," '23': ([' 24', ' 23', ' 1923', ' 2400', ' 1924'], '25', 'no'),\n"," '24': ([' 25', '25', ' 24', '24', ' 249'], '26', 'no'),\n"," '25': ([' 26', '26', ' 25', ' 27', ' Randolph'], '27', 'yes'),\n"," '26': ([' 27', '27', ' 26', '26', ' 1927'], '28', 'no'),\n"," '27': ([' 28', ' 27', '28', '27', ' 278'], '29', 'no'),\n"," '28': (['29', ' 29', '28', ' 28', '028'], '30', 'no'),\n"," '29': ([' 30', '29', '30', ' 29', '��'], '31', 'no'),\n"," '30': (['31', ' 31', ' 32', '30', ' 30'], '32', 'yes'),\n"," '31': ([' 32', '32', '31', ' 31', ' 321'], '33', 'no'),\n"," '32': ([' 33', '33', ' 32', '32', ' 34'], '34', 'yes'),\n"," '33': ([' 34', ' 33', '33', '34', ' 334'], '35', 'no'),\n"," '34': ([' 35', ' 34', '35', '34', ' 1934'], '36', 'no'),\n"," '35': ([' 36', ' 35', '36', '35', ' 37'], '37', 'yes'),\n"," '36': ([' 37', '37', ' 36', ' 38', '36'], '38', 'yes'),\n"," '37': ([' 38', ' 37', '38', ' 39', '37'], '39', 'yes'),\n"," '38': ([' 39', ' 38', 'SR', '39', ' SR'], '40', 'no'),\n"," '39': ([' 39', ' 40', '39', ' 1840', '40'], '41', 'no'),\n"," '40': ([' 41', '41', '441', '42', ' 1941'], '42', 'yes'),\n"," '41': ([' 42', '42', ' 41', '41', ' 43'], '43', 'yes'),\n"," '42': ([' 43', '43', ' 1943', ' 42', '42'], '44', 'no'),\n"," '43': ([' 44', ' 43', '44', ' 444', '43'], '45', 'no'),\n"," '44': ([' 44', ' 45', '44', '45', '46'], '46', 'yes'),\n"," '45': ([' 46', '46', ' 45', '45', ' Faust'], '47', 'no'),\n"," '46': ([' 47', '47', '46', ' 46', '�'], '48', 'no'),\n"," '47': ([' 47', ' 48', '47', '48', ' 49'], '49', 'yes'),\n"," '48': ([' 49', ' 48', '49', '48', ' marqu'], '50', 'no'),\n"," '49': ([' 49', ' 50', '49', ' 51', ' 52'], '51', 'yes'),\n"," '50': ([' 51', '51', ' 52', ' 1951', '52'], '52', 'yes'),\n"," '51': ([' 52', '52', ' 51', '51', ' 53'], '53', 'yes'),\n"," '52': ([' 53', ' 52', '53', '52', ' 54'], '54', 'yes'),\n"," '53': ([' 54', ' 53', '54', '53', ' 55'], '55', 'yes'),\n"," '54': ([' 55', ' 54', '54', '55', ' Nek'], '56', 'no'),\n"," '55': ([' 56', ' 55', '56', '55', ' 1955'], '57', 'no'),\n"," '56': ([' 57', '57', ' 56', '56', ' 1957'], '58', 'no'),\n"," '57': ([' 58', ' 57', '57', '58', ' 59'], '59', 'yes'),\n"," '58': ([' 59', ' 58', '59', '58', ' 1958'], '60', 'no'),\n"," '59': ([' 60', '60', ' 59', '59', 'ococ'], '61', 'no'),\n"," '60': ([' 61', '61', ' 62', '62', ' 1961'], '62', 'yes'),\n"," '61': ([' 62', '62', ' 63', ' 61', '63'], '63', 'yes'),\n"," '62': ([' 63', '63', ' 62', '62', ' 64'], '64', 'yes'),\n"," '63': ([' 63', ' 64', '64', '63', '864'], '65', 'no'),\n"," '64': ([' 65', ' 64', '64', '65', ' 66'], '66', 'yes'),\n"," '65': ([' 66', '66', ' 65', '65', ' 68'], '67', 'no'),\n"," '66': ([' 67', '67', ' 66', ' 68', '66'], '68', 'yes'),\n"," '67': ([' 68', ' 67', ' 69', '68', '67'], '69', 'yes'),\n"," '68': ([' 69', ' 68', '69', '68', ' 70'], '70', 'yes'),\n"," '69': ([' 69', ' 70', '69', '70', ' 71'], '71', 'yes'),\n"," '70': ([' 71', '71', ' 72', ' 70', ' 76'], '72', 'yes'),\n"," '71': ([' 72', ' 71', '72', ' 73', ' 76'], '73', 'yes'),\n"," '72': ([' 73', ' 72', '73', ' 74', ' 77'], '74', 'yes'),\n"," '73': ([' 74', ' 73', '74', ' 76', '73'], '75', 'no'),\n"," '74': ([' 74', '74', ' 75', ' 76', '75'], '76', 'yes'),\n"," '75': ([' 76', '76', ' 77', ' 75', ' Marqu'], '77', 'yes'),\n"," '76': ([' 77', ' 76', '77', '76', ' 78'], '78', 'yes'),\n"," '77': ([' 77', ' 78', '78', '77', ' 978'], '79', 'no'),\n"," '78': ([' 79', ' 78', '79', '78', ' 1979'], '80', 'no'),\n"," '79': ([' 79', '79', ' 80', '80', ' 81'], '81', 'yes'),\n"," '80': ([' 81', '81', ' 82', '-----------', ' 1981'], '82', 'yes'),\n"," '81': ([' 82', ' 81', '82', ' 83', ' Lawson'], '83', 'yes'),\n"," '82': ([' 83', ' 82', '83', '82', ' 1983'], '84', 'no'),\n"," '83': ([' 84', ' 83', '84', ' 85', ' 88'], '85', 'yes'),\n"," '84': ([' 84', ' 85', '85', '84', ' 86'], '86', 'yes'),\n"," '85': ([' 86', '86', ' 85', '85', ' 87'], '87', 'yes'),\n"," '86': ([' 87', '87', ' 86', '86', ' 88'], '88', 'yes'),\n"," '87': ([' 88', ' 87', ' 89', '88', '87'], '89', 'yes'),\n"," '88': ([' 89', ' 88', '89', '88', ' 1889'], '90', 'no'),\n"," '89': ([' 89', '89', ' 90', ' 88', ' 91'], '91', 'yes'),\n"," '90': ([' 91', ' 92', ' 90', '91', ' 89'], '92', 'yes'),\n"," '91': ([' 92', ' 91', ' 93', '92', ' 96'], '93', 'yes'),\n"," '92': ([' 93', ' 92', ' 1893', '93', ' 94'], '94', 'yes'),\n"," '93': ([' 94', ' 93', '94', ' 95', ' 92'], '95', 'yes'),\n"," '94': ([' 94', ' 95', '94', '95', ' 1895'], '96', 'no'),\n"," '95': ([' 96', ' 95', ' 97', '96', '95'], '97', 'yes'),\n"," '96': ([' 97', ' 96', '97', ' 98', '96'], '98', 'yes'),\n"," '97': ([' 97', ' 98', '98', ' 978', '97'], '99', 'no'),\n"," '98': ([' 98', ' 99', '98', ' Sakura', ' 97'], '100', 'no'),\n"," '99': ([' 99', '99', ' 100', ' 980', 'izo'], '101', 'no'),\n"," '100': ([' 101', '101', '1001', ' 102', ' vulner'], '102', 'yes'),\n"," '101': ([' 102', '102', ' 101', ' 103', '101'], '103', 'yes')}"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["Perhaps first token has a space char in front? This is more similar to 'last'."],"metadata":{"id":"3uTmq2Bj9VWG"}},{"cell_type":"markdown","source":["# Input the all tokens of seq"],"metadata":{"id":"8atJh1tl9-m_"}},{"cell_type":"code","source":["def get_addTwo_scores_allPos(model, layer, head, dataset, verbose=False, neg=False, print_tokens=True):\n","    cache = {}\n","    model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","    model(dataset.toks.long())\n","    if neg:\n","        sign = -1\n","    else:\n","        sign = 1\n","    z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","\n","    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","    logits = model.unembed(model.ln_final(o))\n","\n","    k = 5\n","    n_right = 0\n","\n","    pred_tokens_dict = {}\n","    words_added = []\n","    # get the keys from the first prompt in the dataset\n","    words = [key for key in dataset.prompts[0].keys() if key != 'text']\n","\n","    for seq_idx, prompt in enumerate(dataset.prompts):\n","        for word in words:  ######### CHANGE TO USE THIS #########\n","        # word = words[1]\n","\n","            pred_tokens = [\n","                model.tokenizer.decode(token)\n","                for token in torch.topk(\n","                    logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","                ).indices\n","            ]\n","\n","            # get +2 after digit prompt[word]\n","            targword = str(int(prompt[word]) + 2)\n","\n","            targToken_in_topK = 'no'\n","            if \" \" + targword in pred_tokens or targword in pred_tokens:\n","                n_right += 1\n","                words_added.append(prompt[word])\n","                targToken_in_topK = 'yes'\n","            pred_tokens_dict[prompt[word]] = (pred_tokens, targword, targToken_in_topK)\n","\n","    percent_right = (n_right / (dataset.N * len(words))) * 100\n","    # percent_right = (n_right / (dataset.N)) * 100\n","    if percent_right > 0:\n","        print(f\"Head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\")\n","\n","    if print_tokens == True:\n","        print(n_right)\n","        print((dataset.N))\n","        print(words_added)\n","        return pred_tokens_dict\n","    else:\n","        if percent_right > 0:\n","            return words_added"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698347379255,"user_tz":240,"elapsed":75,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"iboMoS2R9-nU"},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["get_addTwo_scores_allPos(model, 14, 14, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698347384083,"user_tz":240,"elapsed":72,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"aba74b94-3888-4f96-bbad-61a0e3c5d28a","id":"ifKh8wVn9-nV"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Head 14.14 (sign=1) : Top 5 accuracy: 44.78114478114478%\n","266\n","99\n","['20', '20', '20', '20', '20', '30', '25', '31', '30', '32', '25', '30', '32', '34', '35', '30', '32', '34', '36', '35', '37', '30', '32', '36', '35', '37', '30', '32', '34', '36', '40', '35', '37', '41', '36', '40', '35', '37', '41', '36', '40', '37', '41', '40', '41', '47', '40', '41', '47', '49', '40', '44', '50', '47', '49', '51', '44', '50', '52', '47', '49', '51', '53', '50', '52', '47', '49', '51', '53', '50', '52', '56', '49', '51', '53', '57', '50', '52', '56', '51', '53', '57', '52', '56', '60', '53', '57', '61', '56', '60', '62', '57', '61', '63', '60', '62', '64', '57', '61', '60', '62', '64', '66', '61', '67', '60', '62', '64', '66', '68', '61', '67', '69', '62', '64', '66', '68', '70', '67', '69', '71', '62', '64', '66', '68', '70', '72', '67', '69', '71', '66', '68', '70', '72', '74', '67', '69', '71', '75', '68', '70', '72', '74', '76', '69', '71', '75', '70', '72', '74', '76', '71', '75', '79', '72', '74', '76', '80', '71', '75', '79', '81', '74', '76', '80', '82', '75', '79', '81', '83', '76', '80', '82', '84', '79', '81', '83', '85', '80', '82', '84', '86', '79', '81', '83', '85', '87', '80', '82', '84', '86', '81', '83', '85', '87', '84', '86', '90', '81', '83', '85', '87', '91', '84', '86', '90', '92', '85', '87', '91', '93', '86', '90', '92', '87', '91', '93', '95', '86', '90', '92', '96', '89', '91', '93', '95', '90', '92', '96', '91', '93', '95', '92', '96', '100', '91', '93', '95', '101', '96', '100', '95', '101', '103', '96', '100', '104', '101', '103', '105', '100', '104', '106', '101', '103', '105', '107', '100', '104', '106', '108', '101', '103', '105', '107', '109']\n"]},{"output_type":"execute_result","data":{"text/plain":["{'1': (['bara', ' Ai', 'isphere', 'ahime', '²'], '3', 'no'),\n"," '3': (['4', ' 4', 'daq', 'atform', 'iola'], '5', 'no'),\n"," '5': (['cill', ' 6', 'agu', ' brush', '6'], '7', 'no'),\n"," '7': ([' 8', ' 808', ' 7', ' Lawson', '8'], '9', 'no'),\n"," '9': ([' Abe', ' Ark', ' AA', ' AQ', ' 9'], '11', 'no'),\n"," '11': (['arta', '12', 'IDA', ' 12', ' Dawson'], '13', 'no'),\n"," '2': (['3', 'ahime', 'alion', ' 3', 'atform'], '4', 'no'),\n"," '4': (['5', 'baugh', 'BB', 'Balt', 'acea'], '6', 'no'),\n"," '6': ([' 7', '7', '�', ' 07', '�'], '8', 'no'),\n"," '8': ([' Caldwell', 'acea', ' 8', 'bara', ' 9'], '10', 'no'),\n"," '10': (['1111', '11', 'CT', '910', ' 11'], '12', 'no'),\n"," '12': (['arta', 'tailed', '13', '12', ' 13'], '14', 'no'),\n"," '13': ([' 14', '14', 'BW', ' 1914', ' 13'], '15', 'no'),\n"," '14': (['15', 'arta', '1500', ' 15', 'ayer'], '16', 'no'),\n"," '15': ([' 16', 'ayson', '16', '1600', 'bara'], '17', 'no'),\n"," '16': (['17', ' 17', 'alde', 'eenth', '�'], '18', 'no'),\n"," '17': ([' 18', ' DD', ' Awoken', 'ッド', ' 17'], '19', 'no'),\n"," '18': ([' ABE', 'bara', ' Abe', ' Caldwell', ' 19'], '20', 'no'),\n"," '19': ([' 20', 'izo', '20', 'enhagen', ' 19'], '21', 'no'),\n"," '20': ([' 21', '21', 'acon', ' 22', '221'], '22', 'yes'),\n"," '21': ([' 22', '22', '422', ' 1922', ' 222'], '23', 'no'),\n"," '22': ([' 1923', ' 23', ' 223', '23', '22'], '24', 'no'),\n"," '23': ([' 24', '24', ' 23', ' 1924', ' 1923'], '25', 'no'),\n"," '24': (['24', ' 25', ' 24', '25', ' ABE'], '26', 'no'),\n"," '25': ([' 26', '26', ' 25', '25', ' 1926'], '27', 'no'),\n"," '26': ([' 27', '27', ' 26', '26', ' 1927'], '28', 'no'),\n"," '27': ([' 28', '28', ' 27', ' 1928', '27'], '29', 'no'),\n"," '28': (['29', '�', 'bara', '28', ' 29'], '30', 'no'),\n"," '29': (['29', '30', ' 30', ' 29', ' 1930'], '31', 'no'),\n"," '30': (['31', ' 31', '30', '32', ' 32'], '32', 'yes'),\n"," '31': (['32', ' 32', '31', ' 332', ' 432'], '33', 'no'),\n"," '32': (['33', ' 33', 'lda', '32', '833'], '34', 'no'),\n"," '33': (['33', ' 33', ' 34', ' 334', ' 337'], '35', 'no'),\n"," '34': (['34', ' 35', '35', ' 34', ' 1934'], '36', 'no'),\n"," '35': (['36', ' 36', ' 35', '35', '�'], '37', 'no'),\n"," '36': ([' 37', '37', '36', ' Burke', ' 36'], '38', 'no'),\n"," '37': ([' 38', ' 37', '38', 'reb', ' Krug'], '39', 'no'),\n"," '38': ([' 1840', ' 39', ' 38', '38', '�'], '40', 'no'),\n"," '39': ([' 1840', ' 40', '40', ' 39', '39'], '41', 'no'),\n"," '40': (['41', ' 41', '42', 'bara', 'aban'], '42', 'yes'),\n"," '41': (['42', ' 42', '41', 'aban', 'arta'], '43', 'no'),\n"," '42': ([' 43', '43', ' 1943', '42', 'Ba'], '44', 'no'),\n"," '43': ([' 44', ' 43', ' 1944', ' 444', ' WD'], '45', 'no'),\n"," '44': (['bara', '44', ' 1948', ' 44', 'iations'], '46', 'no'),\n"," '45': (['46', 'bara', ' 46', 'PA', '45'], '47', 'no'),\n"," '46': (['47', ' 47', '46', '�', ' 747'], '48', 'no'),\n"," '47': (['47', ' 48', '48', ' 47', ' DB'], '49', 'no'),\n"," '48': (['bara', '48', '49', ' 49', 'sb'], '50', 'no'),\n"," '49': (['49', '50', ' 50', ' 49', ' 1850'], '51', 'no'),\n"," '50': (['51', ' 51', ' 1951', '50', '501'], '52', 'no'),\n"," '51': ([' 52', '52', '51', ' 51', '��極'], '53', 'no'),\n"," '52': ([' 53', 'lyak', '53', 'lda', 'ahime'], '54', 'no'),\n"," '53': ([' 53', ' 54', 'atform', '53', '54'], '55', 'no'),\n"," '54': ([' ANC', '55', ' 55', '855', 'acket'], '56', 'no'),\n"," '55': (['abo', ' 56', '56', '855', ' ANC'], '57', 'no'),\n"," '56': ([' 57', 'Cath', '56', '57', ' Burke'], '58', 'no'),\n"," '57': (['58', 'BW', 'CV', ' 58', 'ucl'], '59', 'no'),\n"," '58': (['58', ' 1959', ' 58', 'VC', '1959'], '60', 'no'),\n"," '59': (['60', ' Doll', 'daq', 'adelphia', 'CE'], '61', 'no'),\n"," '60': (['61', ' 61', 'elf', 'ELF', '60'], '62', 'no'),\n"," '61': ([' 62', '62', '61', ' 61', 'inelli'], '63', 'no'),\n"," '62': ([' 63', '63', ' 62', '62', '64'], '64', 'yes'),\n"," '63': (['64', '63', ' 63', ' 64', '864'], '65', 'no'),\n"," '64': (['64', '665', 'vc', '65', 'otta'], '66', 'no'),\n"," '65': (['66', ' 66', '65', 'alez', 'cart'], '67', 'no'),\n"," '66': ([' BG', '67', 'angelo', 'gerald', ' 67'], '68', 'no'),\n"," '67': (['BG', ' 68', ' 67', ' BG', '67'], '69', 'no'),\n"," '68': ([' 68', 'BG', 'CG', ' 69', '68'], '70', 'no'),\n"," '69': (['BG', 'bg', ' BG', 'EG', 'CG'], '71', 'no'),\n"," '70': ([' 71', '71', 'BG', 'aban', 'bg'], '72', 'no'),\n"," '71': (['aban', ' 72', '72', ' 71', ' 73'], '73', 'yes'),\n"," '72': ([' 73', '73', ' 72', 'amiya', 'aban'], '74', 'no'),\n"," '73': ([' 74', ' 73', '74', ' Lawson', '73'], '75', 'no'),\n"," '74': (['aban', '74', 'eva', ' DAC', ' 74'], '76', 'no'),\n"," '75': ([' 76', 'aban', '76', 'rain', 'gall'], '77', 'no'),\n"," '76': ([' 77', '77', '76', ' 76', 'utan'], '78', 'no'),\n"," '77': ([' 78', ' 77', ' 978', ' 1977', ' Seraph'], '79', 'no'),\n"," '78': ([' 78', '78', ' 79', '79', 'haar'], '80', 'no'),\n"," '79': (['79', ' WC', '80', ' 79', ' 80'], '81', 'no'),\n"," '80': ([' 81', '81', 'ao', 'aul', 'aos'], '82', 'no'),\n"," '81': ([' Lawson', ' 82', '82', ' 81', ' 83'], '83', 'yes'),\n"," '82': ([' 83', ' Lawson', 'lda', ' 82', '83'], '84', 'no'),\n"," '83': ([' 84', ' DAV', ' 83', 'lda', '84'], '85', 'no'),\n"," '84': (['84', ' 84', ' DAC', '85', ' 85'], '86', 'no'),\n"," '85': ([' 86', '86', ' 1886', 'aza', '85'], '87', 'no'),\n"," '86': ([' 87', '86', ' 86', '87', ' 88'], '88', 'yes'),\n"," '87': ([' 88', ' 87', '88', ' 1888', 'aza'], '89', 'no'),\n"," '88': ([' 88', '88', ' 89', '89', '889'], '90', 'no'),\n"," '89': (['89', ' 89', 'abbage', ' 90', 'iola'], '91', 'no'),\n"," '90': ([' 91', '91', ' Hawkins', 'ao', 'abo'], '92', 'no'),\n"," '91': ([' 92', ' 91', 'aji', ' 93', ' 96'], '93', 'yes'),\n"," '92': ([' 93', ' 1893', ' 92', '93', '1993'], '94', 'no'),\n"," '93': ([' 94', ' 93', ' 1893', '94', ' 1994'], '95', 'no'),\n"," '94': ([' 94', ' Ara', '94', ' 95', 'adish'], '96', 'no'),\n"," '95': ([' 96', 'abo', ' 1896', '96', ' 95'], '97', 'no'),\n"," '96': ([' 97', ' 96', '97', 'agna', '1997'], '98', 'no'),\n"," '97': ([' 98', ' 97', 'DCS', 'abbage', 'anks'], '99', 'no'),\n"," '98': (['gado', ' 98', '98', ' 99', 'aces'], '100', 'no'),\n"," '99': (['ague', ' 99', 'orest', 'iage', 'acular'], '101', 'no'),\n"," '100': ([' 101', '101', '1001', ' 102', ' vulner'], '102', 'yes'),\n"," '101': ([' 102', '102', ' 101', ' 103', '101'], '103', 'yes'),\n"," '102': ([' 103', '103', ' 102', '102', ' 105'], '104', 'no'),\n"," '103': ([' 104', ' 103', ' 105', '103', '104'], '105', 'yes'),\n"," '104': (['105', ' 105', ' 104', '104', '106'], '106', 'yes'),\n"," '105': ([' 105', ' 106', '106', '105', ' 107'], '107', 'yes'),\n"," '106': ([' 107', ' 106', '107', '106', ' 108'], '108', 'yes'),\n"," '107': ([' 108', ' 107', '107', '108', ' 109'], '109', 'yes'),\n"," '108': ([' 109', ' 108', '109', ' 110', '108'], '110', 'yes'),\n"," '109': ([' 110', '110', ' 109', '109', ' 111'], '111', 'yes')}"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["# TBC"],"metadata":{"id":"MdI-noObEAwb"}},{"cell_type":"markdown","source":["The cells below have not been updated yet for next scores so disregard them:\n","\n","---\n","\n"],"metadata":{"id":"ZuBrs1i5BFET"}},{"cell_type":"markdown","source":["## Writing direction results with scatterplot"],"metadata":{"id":"pc6wzXDl1G5E"}},{"cell_type":"code","source":["def scatter_attention_and_contribution(\n","    model,\n","    layer_no,\n","    head_no,\n","    dataset,\n","    S1_is_first=False,\n","    return_vals=False,\n","    return_fig=False,\n","):\n","    \"\"\"\n","    Plot a scatter plot\n","    for each input sequence with the attention paid to S\n","    and the amount that is written in the S directions\n","    \"\"\"\n","\n","    n_heads = model.cfg.n_heads\n","    n_layers = model.cfg.n_layers\n","    model_unembed = model.unembed.W_U.detach().cpu()\n","    df = []\n","    cache = {}\n","    model.cache_all(cache)\n","\n","    logits = model(dataset.toks.long())\n","\n","    for i, prompt in enumerate(dataset.prompts):\n","        s_toks = []\n","        s_positions = []\n","        s_dirs = []\n","\n","        targ_tokens = [key for key in dataset.prompts[0].keys() if key != 'text']\n","        for s_id in targ_tokens:\n","            if S1_is_first and s_id == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                s_tok = model.tokenizer(prompt[\"S1\"])[\"input_ids\"][0]\n","            else:\n","                s_tok = model.tokenizer(\" \" + prompt[s_id])[\"input_ids\"][0]\n","            s_toks.append(s_tok)\n","\n","            toks = model.tokenizer(prompt[\"text\"])[\"input_ids\"]\n","            try:\n","                s_pos = toks.index(s_tok)\n","            except ValueError:\n","                print(f\"{s_tok} is not present in {toks}. Skipping...\")\n","                continue\n","\n","            s_pos = toks.index(s_tok)\n","            s_positions.append(s_pos)\n","\n","            s_dir = model_unembed[:, s_tok].detach()\n","            s_dirs.append(s_dir)\n","\n","        for dire, posses, tok_type in zip(s_dirs, s_positions, targ_tokens):\n","            prob = sum(\n","                [\n","                    cache[f\"blocks.{layer_no}.attn.hook_attn\"][\n","                        i, head_no, dataset.word_idx[\"end\"][i], pos\n","                    ]\n","                    .detach()\n","                    .cpu()\n","                    for pos in [posses]\n","                ]\n","            )\n","            resid = (\n","                cache[f\"blocks.{layer_no}.attn.hook_result\"][\n","                    i, dataset.word_idx[\"end\"][i], head_no, :\n","                ]\n","                .detach()\n","                .cpu()\n","            )\n","            dot = torch.einsum(\"a,a->\", resid, dire)\n","            df.append([prob, dot, tok_type, prompt[\"text\"]])\n","\n","    viz_df = pd.DataFrame(\n","        df, columns=[f\"Attn Prob on Number\", f\"Dot w Number Embed\", \"Seq Position\", \"text\"]\n","    )\n","    fig = px.scatter(\n","        viz_df,\n","        x=f\"Attn Prob on Number\",\n","        y=f\"Dot w Number Embed\",\n","        color=\"Seq Position\",\n","        hover_data=[\"text\"],\n","        title=f\"How Strong {layer_no}.{head_no} Writes in the Number Embed Direction Relative to Attn Prob\",\n","    )\n","\n","    if return_vals:\n","        return viz_df\n","    if return_fig:\n","        return fig\n","    else:\n","        fig.show()"],"metadata":{"id":"Ah8DWLlUL_3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scatter_attention_and_contribution(\n","    model=model, layer_no=9, head_no=1, dataset=dataset, S1_is_first=False\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AwhyxrN3R8xL","executionInfo":{"status":"ok","timestamp":1694632979636,"user_tz":240,"elapsed":1177,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cc1d5211-5323-4818-8b10-a819b0ee9b6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"8fbbb245-1888-4018-990a-cfc0755633c8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8fbbb245-1888-4018-990a-cfc0755633c8\")) {                    Plotly.newPlot(                        \"8fbbb245-1888-4018-990a-cfc0755633c8\",                        [{\"customdata\":[[\"1 2 3 4\"],[\"2 3 4 5\"],[\"3 4 5 6\"],[\"4 5 6 7\"],[\"5 6 7 8\"],[\"6 7 8 9\"],[\"7 8 9 10\"],[\"8 9 10 11\"],[\"9 10 11 12\"],[\"10 11 12 13\"],[\"11 12 13 14\"],[\"12 13 14 15\"],[\"13 14 15 16\"],[\"14 15 16 17\"],[\"15 16 17 18\"],[\"16 17 18 19\"],[\"17 18 19 20\"],[\"18 19 20 21\"],[\"19 20 21 22\"],[\"20 21 22 23\"],[\"21 22 23 24\"],[\"22 23 24 25\"],[\"23 24 25 26\"],[\"24 25 26 27\"],[\"25 26 27 28\"],[\"26 27 28 29\"],[\"27 28 29 30\"],[\"28 29 30 31\"],[\"29 30 31 32\"],[\"30 31 32 33\"],[\"31 32 33 34\"],[\"32 33 34 35\"],[\"33 34 35 36\"],[\"34 35 36 37\"],[\"35 36 37 38\"],[\"36 37 38 39\"],[\"37 38 39 40\"],[\"38 39 40 41\"],[\"39 40 41 42\"],[\"40 41 42 43\"],[\"41 42 43 44\"],[\"42 43 44 45\"],[\"43 44 45 46\"],[\"44 45 46 47\"],[\"45 46 47 48\"],[\"46 47 48 49\"],[\"47 48 49 50\"],[\"48 49 50 51\"],[\"49 50 51 52\"],[\"50 51 52 53\"],[\"51 52 53 54\"],[\"52 53 54 55\"],[\"53 54 55 56\"],[\"54 55 56 57\"],[\"55 56 57 58\"],[\"56 57 58 59\"],[\"57 58 59 60\"],[\"58 59 60 61\"],[\"59 60 61 62\"],[\"60 61 62 63\"],[\"61 62 63 64\"],[\"62 63 64 65\"],[\"63 64 65 66\"],[\"64 65 66 67\"],[\"65 66 67 68\"],[\"66 67 68 69\"],[\"67 68 69 70\"],[\"68 69 70 71\"],[\"69 70 71 72\"],[\"70 71 72 73\"],[\"71 72 73 74\"],[\"72 73 74 75\"],[\"73 74 75 76\"],[\"74 75 76 77\"],[\"75 76 77 78\"],[\"76 77 78 79\"],[\"77 78 79 80\"],[\"78 79 80 81\"],[\"79 80 81 82\"],[\"80 81 82 83\"],[\"81 82 83 84\"],[\"82 83 84 85\"],[\"83 84 85 86\"],[\"84 85 86 87\"],[\"85 86 87 88\"],[\"86 87 88 89\"],[\"87 88 89 90\"],[\"88 89 90 91\"],[\"89 90 91 92\"],[\"90 91 92 93\"],[\"91 92 93 94\"],[\"92 93 94 95\"],[\"93 94 95 96\"],[\"94 95 96 97\"],[\"95 96 97 98\"],[\"96 97 98 99\"],[\"97 98 99 100\"]],\"hovertemplate\":\"Seq Position=S1\\u003cbr\\u003eAttn Prob on Number=%{x}\\u003cbr\\u003eDot w Number Embed=%{y}\\u003cbr\\u003etext=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"S1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"S1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.058369945734739304,0.04661092534661293,0.07315342873334885,0.058079153299331665,0.05081278085708618,0.11673405766487122,0.06410746276378632,0.0717870220541954,0.04567893594503403,0.039213333278894424,0.05854412168264389,0.08031466603279114,0.054261840879917145,0.03899860009551048,0.09240742027759552,0.10611087083816528,0.06065232679247856,0.09251639991998672,0.07898331433534622,0.06903322041034698,0.08081388473510742,0.03569205477833748,0.0320107564330101,0.052279721945524216,0.0491148978471756,0.10003545135259628,0.05480573698878288,0.037898071110248566,0.025862809270620346,0.02583528682589531,0.04233095794916153,0.04851434752345085,0.052419427782297134,0.043383657932281494,0.056453391909599304,0.10494264215230942,0.0704803317785263,0.07697003334760666,0.026571912690997124,0.04317902773618698,0.048181820660829544,0.04515625908970833,0.0502261258661747,0.05129455402493477,0.06540762633085251,0.043131373822689056,0.03432781994342804,0.04845472425222397,0.05043718218803406,0.09936491400003433,0.06495700776576996,0.0515628308057785,0.038121163845062256,0.05191275104880333,0.09533122181892395,0.18170194327831268,0.15029354393482208,0.07766516506671906,0.04286946728825569,0.07886725664138794,0.018000103533267975,0.03606640547513962,0.016660964116454124,0.03724221885204315,0.06365592032670975,0.09405805170536041,0.07084433734416962,0.039876729249954224,0.06363839656114578,0.0882931649684906,0.10085336863994598,0.0943865180015564,0.06805043667554855,0.07206793129444122,0.11166490614414215,0.14845938980579376,0.10762715339660645,0.06946232169866562,0.07483921200037003,0.08999312669038773,0.06697554886341095,0.08379931002855301,0.0921439528465271,0.07796071469783783,0.10680976510047913,0.15941160917282104,0.15456072986125946,0.05010952055454254,0.04620009660720825,0.07470546662807465,0.09162824600934982,0.046728648245334625,0.059796884655952454,0.049967095255851746,0.060601476579904556,0.08569534868001938,0.04282321408390999],\"xaxis\":\"x\",\"y\":[0.3768908977508545,3.3398380279541016,5.914272308349609,5.945875644683838,1.6907645463943481,4.424149990081787,1.477386713027954,0.6497106552124023,2.3197755813598633,6.406676292419434,5.48015022277832,6.003168106079102,0.720984935760498,1.2707651853561401,4.938894748687744,6.343235492706299,1.5373859405517578,4.801419734954834,4.203042030334473,5.588527202606201,3.682209014892578,1.9755555391311646,0.12904953956604004,5.508935451507568,7.615622520446777,8.455768585205078,3.4924588203430176,2.0864410400390625,-2.171081781387329,5.194705486297607,4.130370140075684,8.424798965454102,8.715641975402832,6.989168643951416,10.629358291625977,11.059053421020508,6.775578498840332,6.899392127990723,4.59255838394165,9.405509948730469,3.806697368621826,3.736084461212158,2.9992051124572754,9.411905288696289,8.190970420837402,8.266132354736328,4.445199489593506,9.726814270019531,2.243312358856201,6.002878665924072,4.194843769073486,5.120826721191406,2.9950122833251953,6.381253719329834,6.219289779663086,10.017535209655762,8.869142532348633,11.829793930053711,6.019057273864746,13.980338096618652,8.61738395690918,13.62224006652832,11.977130889892578,20.62066650390625,20.179189682006836,19.001258850097656,15.598492622375488,10.774189949035645,10.798739433288574,17.576465606689453,11.124285697937012,6.998898983001709,10.250484466552734,13.603531837463379,11.921567916870117,11.731103897094727,10.72551155090332,12.362720489501953,7.078793048858643,11.89607048034668,7.730164051055908,8.697235107421875,11.613883972167969,16.410633087158203,15.140596389770508,11.522521018981934,8.523770332336426,7.170890808105469,4.3135271072387695,12.49072265625,6.126021862030029,8.614516258239746,5.327075958251953,5.030712604522705,6.076260566711426,11.192577362060547,7.133969783782959],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"1 2 3 4\"],[\"2 3 4 5\"],[\"3 4 5 6\"],[\"4 5 6 7\"],[\"5 6 7 8\"],[\"6 7 8 9\"],[\"7 8 9 10\"],[\"8 9 10 11\"],[\"9 10 11 12\"],[\"10 11 12 13\"],[\"11 12 13 14\"],[\"12 13 14 15\"],[\"13 14 15 16\"],[\"14 15 16 17\"],[\"15 16 17 18\"],[\"16 17 18 19\"],[\"17 18 19 20\"],[\"18 19 20 21\"],[\"19 20 21 22\"],[\"20 21 22 23\"],[\"21 22 23 24\"],[\"22 23 24 25\"],[\"23 24 25 26\"],[\"24 25 26 27\"],[\"25 26 27 28\"],[\"26 27 28 29\"],[\"27 28 29 30\"],[\"28 29 30 31\"],[\"29 30 31 32\"],[\"30 31 32 33\"],[\"31 32 33 34\"],[\"32 33 34 35\"],[\"33 34 35 36\"],[\"34 35 36 37\"],[\"35 36 37 38\"],[\"36 37 38 39\"],[\"37 38 39 40\"],[\"38 39 40 41\"],[\"39 40 41 42\"],[\"40 41 42 43\"],[\"41 42 43 44\"],[\"42 43 44 45\"],[\"43 44 45 46\"],[\"44 45 46 47\"],[\"45 46 47 48\"],[\"46 47 48 49\"],[\"47 48 49 50\"],[\"48 49 50 51\"],[\"49 50 51 52\"],[\"50 51 52 53\"],[\"51 52 53 54\"],[\"52 53 54 55\"],[\"53 54 55 56\"],[\"54 55 56 57\"],[\"55 56 57 58\"],[\"56 57 58 59\"],[\"57 58 59 60\"],[\"58 59 60 61\"],[\"59 60 61 62\"],[\"60 61 62 63\"],[\"61 62 63 64\"],[\"62 63 64 65\"],[\"63 64 65 66\"],[\"64 65 66 67\"],[\"65 66 67 68\"],[\"66 67 68 69\"],[\"67 68 69 70\"],[\"68 69 70 71\"],[\"69 70 71 72\"],[\"70 71 72 73\"],[\"71 72 73 74\"],[\"72 73 74 75\"],[\"73 74 75 76\"],[\"74 75 76 77\"],[\"75 76 77 78\"],[\"76 77 78 79\"],[\"77 78 79 80\"],[\"78 79 80 81\"],[\"79 80 81 82\"],[\"80 81 82 83\"],[\"81 82 83 84\"],[\"82 83 84 85\"],[\"83 84 85 86\"],[\"84 85 86 87\"],[\"85 86 87 88\"],[\"86 87 88 89\"],[\"87 88 89 90\"],[\"88 89 90 91\"],[\"89 90 91 92\"],[\"90 91 92 93\"],[\"91 92 93 94\"],[\"92 93 94 95\"],[\"93 94 95 96\"],[\"94 95 96 97\"],[\"95 96 97 98\"],[\"96 97 98 99\"],[\"97 98 99 100\"]],\"hovertemplate\":\"Seq Position=S2\\u003cbr\\u003eAttn Prob on Number=%{x}\\u003cbr\\u003eDot w Number Embed=%{y}\\u003cbr\\u003etext=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"S2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"S2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.07873581349849701,0.10731079429388046,0.10352344065904617,0.10348695516586304,0.09009633958339691,0.17240649461746216,0.16490213572978973,0.18446506559848785,0.14336895942687988,0.24087612330913544,0.182461678981781,0.19920727610588074,0.09732425957918167,0.1466037929058075,0.10900336503982544,0.15884412825107574,0.10182013362646103,0.0984463021159172,0.14454971253871918,0.2178499698638916,0.10851317644119263,0.06955814361572266,0.09981309622526169,0.18695871531963348,0.15416507422924042,0.25141441822052,0.17032141983509064,0.23803485929965973,0.23469752073287964,0.19960874319076538,0.13889314234256744,0.18100903928279877,0.13108067214488983,0.1346719115972519,0.2294027954339981,0.2769585847854614,0.21540307998657227,0.1264331191778183,0.18459691107273102,0.2718696594238281,0.08831173181533813,0.12850745022296906,0.022050144150853157,0.23000450432300568,0.1336274892091751,0.19128450751304626,0.12012644112110138,0.160847008228302,0.24395577609539032,0.21175669133663177,0.08537479490041733,0.15026815235614777,0.06762507557868958,0.21020962297916412,0.15785855054855347,0.2238786667585373,0.17623955011367798,0.0939125269651413,0.24846507608890533,0.21350106596946716,0.12897664308547974,0.17042005062103271,0.09476252645254135,0.2975817918777466,0.18166472017765045,0.2168208509683609,0.2620604336261749,0.14649614691734314,0.2772209048271179,0.2603856325149536,0.06746089458465576,0.1280277967453003,0.06588388979434967,0.32126760482788086,0.19367793202400208,0.21325281262397766,0.2291259914636612,0.1752614974975586,0.2687947750091553,0.18869233131408691,0.06370814889669418,0.13363142311573029,0.10865548253059387,0.23996925354003906,0.11224327981472015,0.19627171754837036,0.11984694749116898,0.08880522102117538,0.17390993237495422,0.341315895318985,0.052027735859155655,0.19120226800441742,0.10694733262062073,0.29526934027671814,0.10370046645402908,0.2415165901184082,0.12343640625476837],\"xaxis\":\"x\",\"y\":[4.1631855964660645,6.472253799438477,10.069902420043945,9.48226547241211,4.522725582122803,9.023200988769531,5.206195831298828,6.425249099731445,10.334317207336426,10.548896789550781,9.432479858398438,9.347149848937988,-0.15148711204528809,7.17771053314209,10.721237182617188,12.275752067565918,6.181478977203369,6.658269882202148,7.191499710083008,7.280240058898926,6.802263259887695,0.8435254096984863,2.020536422729492,10.580190658569336,11.837533950805664,11.788631439208984,6.903816223144531,5.243185520172119,10.290245056152344,4.429510116577148,7.7107696533203125,10.592881202697754,9.325336456298828,13.958169937133789,15.456231117248535,15.39303207397461,8.862814903259277,9.275005340576172,12.556304931640625,12.248733520507812,5.328000068664551,5.854595184326172,5.520805358886719,13.168643951416016,12.006406784057617,9.444724082946777,11.69625473022461,9.949085235595703,11.072637557983398,8.609986305236816,6.194457054138184,6.449173927307129,5.27100944519043,9.831345558166504,12.451213836669922,16.25992774963379,13.84737777709961,8.685918807983398,19.16136360168457,13.913079261779785,10.883277893066406,15.875748634338379,17.705974578857422,24.480701446533203,23.895715713500977,21.258220672607422,19.54884910583496,13.221105575561523,21.57874870300293,17.378948211669922,11.94927978515625,9.35794448852539,9.462553977966309,18.19504165649414,18.095977783203125,17.566240310668945,15.30600643157959,11.817265510559082,18.014394760131836,12.620097160339355,9.518659591674805,11.118144035339355,14.920135498046875,18.530553817749023,18.200014114379883,15.725235939025879,11.857934951782227,6.286099433898926,13.536431312561035,14.199226379394531,7.505089282989502,8.802331924438477,5.904911994934082,11.108528137207031,12.944974899291992,13.797220230102539,12.843158721923828],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"1 2 3 4\"],[\"2 3 4 5\"],[\"3 4 5 6\"],[\"4 5 6 7\"],[\"5 6 7 8\"],[\"6 7 8 9\"],[\"7 8 9 10\"],[\"8 9 10 11\"],[\"9 10 11 12\"],[\"10 11 12 13\"],[\"11 12 13 14\"],[\"12 13 14 15\"],[\"13 14 15 16\"],[\"14 15 16 17\"],[\"15 16 17 18\"],[\"16 17 18 19\"],[\"17 18 19 20\"],[\"18 19 20 21\"],[\"19 20 21 22\"],[\"20 21 22 23\"],[\"21 22 23 24\"],[\"22 23 24 25\"],[\"23 24 25 26\"],[\"24 25 26 27\"],[\"25 26 27 28\"],[\"26 27 28 29\"],[\"27 28 29 30\"],[\"28 29 30 31\"],[\"29 30 31 32\"],[\"30 31 32 33\"],[\"31 32 33 34\"],[\"32 33 34 35\"],[\"33 34 35 36\"],[\"34 35 36 37\"],[\"35 36 37 38\"],[\"36 37 38 39\"],[\"37 38 39 40\"],[\"38 39 40 41\"],[\"39 40 41 42\"],[\"40 41 42 43\"],[\"41 42 43 44\"],[\"42 43 44 45\"],[\"43 44 45 46\"],[\"44 45 46 47\"],[\"45 46 47 48\"],[\"46 47 48 49\"],[\"47 48 49 50\"],[\"48 49 50 51\"],[\"49 50 51 52\"],[\"50 51 52 53\"],[\"51 52 53 54\"],[\"52 53 54 55\"],[\"53 54 55 56\"],[\"54 55 56 57\"],[\"55 56 57 58\"],[\"56 57 58 59\"],[\"57 58 59 60\"],[\"58 59 60 61\"],[\"59 60 61 62\"],[\"60 61 62 63\"],[\"61 62 63 64\"],[\"62 63 64 65\"],[\"63 64 65 66\"],[\"64 65 66 67\"],[\"65 66 67 68\"],[\"66 67 68 69\"],[\"67 68 69 70\"],[\"68 69 70 71\"],[\"69 70 71 72\"],[\"70 71 72 73\"],[\"71 72 73 74\"],[\"72 73 74 75\"],[\"73 74 75 76\"],[\"74 75 76 77\"],[\"75 76 77 78\"],[\"76 77 78 79\"],[\"77 78 79 80\"],[\"78 79 80 81\"],[\"79 80 81 82\"],[\"80 81 82 83\"],[\"81 82 83 84\"],[\"82 83 84 85\"],[\"83 84 85 86\"],[\"84 85 86 87\"],[\"85 86 87 88\"],[\"86 87 88 89\"],[\"87 88 89 90\"],[\"88 89 90 91\"],[\"89 90 91 92\"],[\"90 91 92 93\"],[\"91 92 93 94\"],[\"92 93 94 95\"],[\"93 94 95 96\"],[\"94 95 96 97\"],[\"95 96 97 98\"],[\"96 97 98 99\"],[\"97 98 99 100\"]],\"hovertemplate\":\"Seq Position=S3\\u003cbr\\u003eAttn Prob on Number=%{x}\\u003cbr\\u003eDot w Number Embed=%{y}\\u003cbr\\u003etext=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"S3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"S3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.38946297764778137,0.37382620573043823,0.5445653796195984,0.5873011350631714,0.6124491095542908,0.397522509098053,0.46057817339897156,0.5816602110862732,0.7233459949493408,0.561494767665863,0.5010247230529785,0.3783726096153259,0.6811946630477905,0.6343109011650085,0.4994603395462036,0.41700202226638794,0.4430747330188751,0.34226030111312866,0.4833616614341736,0.4016396999359131,0.45893749594688416,0.26862290501594543,0.7119778394699097,0.5942502021789551,0.7101374268531799,0.34007135033607483,0.5160886645317078,0.5420438051223755,0.622459888458252,0.5403756499290466,0.6339163184165955,0.4517670273780823,0.6667737364768982,0.5987311601638794,0.5561732649803162,0.32260245084762573,0.4124804437160492,0.45787790417671204,0.6902357935905457,0.4744815528392792,0.27137213945388794,0.12063872069120407,0.3179478943347931,0.3557927906513214,0.5589047074317932,0.37555402517318726,0.5430991649627686,0.5086029171943665,0.3838893175125122,0.20224280655384064,0.374148964881897,0.2173835188150406,0.6890817880630493,0.5074136257171631,0.49200162291526794,0.23679013550281525,0.3220005929470062,0.5996852517127991,0.5071170926094055,0.411350816488266,0.7441537380218506,0.6392216682434082,0.8503198027610779,0.5755881667137146,0.7143958210945129,0.532026469707489,0.5054248571395874,0.6927767395973206,0.5825222730636597,0.4524264931678772,0.4920650124549866,0.2463674247264862,0.7562913298606873,0.5156777501106262,0.5396316647529602,0.3013964891433716,0.4093829095363617,0.6560327410697937,0.5013412833213806,0.42048829793930054,0.4314861595630646,0.2643550634384155,0.569412887096405,0.5263566970825195,0.5945475697517395,0.34578022360801697,0.3729628920555115,0.5503026843070984,0.6892929077148438,0.2883768081665039,0.3511953353881836,0.36202675104141235,0.6100982427597046,0.43849602341651917,0.5833069682121277,0.34992823004722595,0.526486337184906],\"xaxis\":\"x\",\"y\":[12.447421073913574,13.613210678100586,17.158004760742188,16.135162353515625,16.329130172729492,17.162500381469727,16.630779266357422,17.891876220703125,20.57801628112793,23.519577026367188,17.507633209228516,16.295719146728516,16.704254150390625,22.568382263183594,23.67190170288086,20.760732650756836,14.956415176391602,12.795332908630371,15.640835762023926,15.359661102294922,12.076026916503906,7.631960391998291,13.745466232299805,22.7564754486084,18.740827560424805,20.53984260559082,16.082292556762695,18.486141204833984,18.364315032958984,18.26580810546875,16.654682159423828,16.70530891418457,25.042682647705078,22.529394149780273,23.14608383178711,21.81957244873047,19.180065155029297,19.255163192749023,21.96364974975586,22.984477996826172,9.772001266479492,8.532469749450684,9.987065315246582,21.58183479309082,18.209150314331055,19.79071807861328,21.804344177246094,21.090972900390625,17.67377471923828,14.527408599853516,10.519058227539062,9.946208953857422,15.60071086883545,21.280200958251953,21.524864196777344,23.242956161499023,16.262754440307617,24.29739761352539,24.445802688598633,22.74281120300293,19.158519744873047,24.693239212036133,28.34479522705078,33.100955963134766,30.637954711914062,30.770729064941406,27.62471580505371,27.055255889892578,28.002464294433594,25.078624725341797,16.45003318786621,12.271041870117188,19.986167907714844,28.967994689941406,27.74812126159668,22.945314407348633,20.96504020690918,25.568683624267578,22.646953582763672,20.60736656188965,14.405777931213379,14.783975601196289,20.904903411865234,27.401769638061523,22.7513427734375,22.822553634643555,16.972503662109375,16.097524642944336,20.77608871459961,21.83434295654297,11.071155548095703,13.375248908996582,15.888277053833008,23.22979736328125,18.275089263916016,20.7680721282959,19.848251342773438],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Attn Prob on Number\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Dot w Number Embed\"}},\"legend\":{\"title\":{\"text\":\"Seq Position\"},\"tracegroupgap\":0},\"title\":{\"text\":\"How Strong 9.1 Writes in the Number Embed Direction Relative to Attn Prob\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('8fbbb245-1888-4018-990a-cfc0755633c8');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Correlation vals"],"metadata":{"id":"U_srDb5pnjnp"}},{"cell_type":"code","source":["def get_prob_dot(  # same as scatterplot, but output x and y vals instead of plotting\n","    model,\n","    layer_no,\n","    head_no,\n","    dataset,\n","    S1_is_first=False,\n","    return_vals=False,\n","    return_fig=False,\n","):\n","    \"\"\"\n","    Plot a scatter plot\n","    for each input sequence with the attention paid to S\n","    and the amount that is written in the S directions\n","    \"\"\"\n","\n","    n_heads = model.cfg.n_heads\n","    n_layers = model.cfg.n_layers\n","    model_unembed = model.unembed.W_U.detach().cpu()\n","    # df = []\n","    all_prob = []\n","    all_dot = []\n","    cache = {}\n","    model.cache_all(cache)\n","\n","    logits = model(dataset.toks.long())\n","\n","    for i, prompt in enumerate(dataset.prompts):\n","        s_toks = []\n","        s_positions = []\n","        s_dirs = []\n","\n","        targ_tokens = [key for key in dataset.prompts[0].keys() if key != 'text']\n","        for s_id in targ_tokens:\n","            if S1_is_first and s_id == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                s_tok = model.tokenizer(prompt[\"S1\"])[\"input_ids\"][0]\n","            else:\n","                s_tok = model.tokenizer(\" \" + prompt[s_id])[\"input_ids\"][0]\n","            s_toks.append(s_tok)\n","\n","            toks = model.tokenizer(prompt[\"text\"])[\"input_ids\"]\n","            try:\n","                s_pos = toks.index(s_tok)\n","            except ValueError:\n","                print(f\"{s_tok} is not present in {toks}. Skipping...\")\n","                continue\n","\n","            s_pos = toks.index(s_tok)\n","            s_positions.append(s_pos)\n","\n","            s_dir = model_unembed[:, s_tok].detach()\n","            s_dirs.append(s_dir)\n","\n","        for dire, posses, tok_type in zip(s_dirs, s_positions, targ_tokens):\n","            prob = sum(\n","                [\n","                    cache[f\"blocks.{layer_no}.attn.hook_attn\"][\n","                        i, head_no, dataset.word_idx[\"end\"][i], pos\n","                    ]\n","                    .detach()\n","                    .cpu()\n","                    for pos in [posses]\n","                ]\n","            )\n","            resid = (\n","                cache[f\"blocks.{layer_no}.attn.hook_result\"][\n","                    i, dataset.word_idx[\"end\"][i], head_no, :\n","                ]\n","                .detach()\n","                .cpu()\n","            )\n","            dot = torch.einsum(\"a,a->\", resid, dire)\n","            #df.append([prob, dot, tok_type, prompt[\"text\"]])\n","            all_prob.append(prob)\n","            all_dot.append(dot)\n","\n","    return all_prob, all_dot\n"],"metadata":{"id":"Ma2Q7hVqQsEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=9, head_no=9, dataset=dataset, S1_is_first=False\n",")"],"metadata":{"id":"KA_YUND7xyFo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694632980363,"user_tz":240,"elapsed":741,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e5906611-ba10-4cb5-a8c2-93634cb6c5ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n"]}]},{"cell_type":"code","source":["import scipy.stats as stats\n","\n","# X and Y should be arrays, lists, or pandas Series\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0o3quHUwuCm","executionInfo":{"status":"ok","timestamp":1694632980740,"user_tz":240,"elapsed":382,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6ea3ad1b-f8f4-4a87-a6e9-c6d9b5ff25c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correlation: 0.8127540109290008\n","p-value: 8.99435128806603e-70\n"]}]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=9, head_no=1, dataset=dataset, S1_is_first=False\n",")\n","\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p867xJ4xz5cB","executionInfo":{"status":"ok","timestamp":1694632981120,"user_tz":240,"elapsed":384,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"43c36bd5-48fa-4a2c-82cf-2dbb3da7d888"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n","Correlation: 0.769961536938402\n","p-value: 2.4806569665462174e-58\n"]}]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=7, head_no=10, dataset=dataset, S1_is_first=False\n",")\n","\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELKt-ytFz8be","executionInfo":{"status":"ok","timestamp":1694632981580,"user_tz":240,"elapsed":466,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8ebd7c49-b5dc-4f29-e369-e6ff3b01b568"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n","Correlation: 0.8862134495731306\n","p-value: 1.4163996031691708e-98\n"]}]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=5, head_no=1, dataset=dataset, S1_is_first=False\n",")\n","\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RE4IdlYA_gQL","executionInfo":{"status":"ok","timestamp":1694632981910,"user_tz":240,"elapsed":338,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ede711f5-f06d-46ff-e166-9f223d7ce5cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n","Correlation: 0.6409386764009012\n","p-value: 4.6226971665520223e-35\n"]}]},{"cell_type":"code","source":["all_prob, all_dot = get_prob_dot(\n","    model=model, layer_no=0, head_no=3, dataset=dataset, S1_is_first=False\n",")\n","\n","correlation, p_value = stats.pearsonr(all_prob, all_dot)\n","print(\"Correlation:\", correlation)\n","print(\"p-value:\", p_value)"],"metadata":{"id":"f7NUqjvX_3-K","executionInfo":{"status":"ok","timestamp":1694633011204,"user_tz":240,"elapsed":724,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"93796647-3ad5-4b00-9194-c2ea061be0cf","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["352 is not present in [16, 362, 513, 604]. Skipping...\n","362 is not present in [17, 513, 604, 642]. Skipping...\n","513 is not present in [18, 604, 642, 718]. Skipping...\n","604 is not present in [19, 642, 718, 767]. Skipping...\n","642 is not present in [20, 718, 767, 807]. Skipping...\n","718 is not present in [21, 767, 807, 860]. Skipping...\n","767 is not present in [22, 807, 860, 838]. Skipping...\n","807 is not present in [23, 860, 838, 1367]. Skipping...\n","860 is not present in [24, 838, 1367, 1105]. Skipping...\n","838 is not present in [940, 1367, 1105, 1511]. Skipping...\n","1367 is not present in [1157, 1105, 1511, 1478]. Skipping...\n","1105 is not present in [1065, 1511, 1478, 1315]. Skipping...\n","1511 is not present in [1485, 1478, 1315, 1467]. Skipping...\n","1478 is not present in [1415, 1315, 1467, 1596]. Skipping...\n","1315 is not present in [1314, 1467, 1596, 1248]. Skipping...\n","1467 is not present in [1433, 1596, 1248, 678]. Skipping...\n","1596 is not present in [1558, 1248, 678, 1160]. Skipping...\n","1248 is not present in [1507, 678, 1160, 2310]. Skipping...\n","678 is not present in [1129, 1160, 2310, 2534]. Skipping...\n","1160 is not present in [1238, 2310, 2534, 2242]. Skipping...\n","2310 is not present in [2481, 2534, 2242, 1987]. Skipping...\n","2534 is not present in [1828, 2242, 1987, 1679]. Skipping...\n","2242 is not present in [1954, 1987, 1679, 2608]. Skipping...\n","1987 is not present in [1731, 1679, 2608, 2681]. Skipping...\n","1679 is not present in [1495, 2608, 2681, 2579]. Skipping...\n","2608 is not present in [2075, 2681, 2579, 2808]. Skipping...\n","2681 is not present in [1983, 2579, 2808, 1542]. Skipping...\n","2579 is not present in [2078, 2808, 1542, 3261]. Skipping...\n","2808 is not present in [1959, 1542, 3261, 3933]. Skipping...\n","1542 is not present in [1270, 3261, 3933, 4747]. Skipping...\n","3261 is not present in [3132, 3933, 4747, 4974]. Skipping...\n","3933 is not present in [2624, 4747, 4974, 3439]. Skipping...\n","4747 is not present in [2091, 4974, 3439, 4570]. Skipping...\n","4974 is not present in [2682, 3439, 4570, 5214]. Skipping...\n","3439 is not present in [2327, 4570, 5214, 4353]. Skipping...\n","4570 is not present in [2623, 5214, 4353, 5014]. Skipping...\n","5214 is not present in [2718, 4353, 5014, 2319]. Skipping...\n","4353 is not present in [2548, 5014, 2319, 6073]. Skipping...\n","5014 is not present in [2670, 2319, 6073, 5433]. Skipping...\n","2319 is not present in [1821, 6073, 5433, 5946]. Skipping...\n","6073 is not present in [3901, 5433, 5946, 5846]. Skipping...\n","5433 is not present in [3682, 5946, 5846, 4153]. Skipping...\n","5946 is not present in [3559, 5846, 4153, 6337]. Skipping...\n","5846 is not present in [2598, 4153, 6337, 6298]. Skipping...\n","4153 is not present in [2231, 6337, 6298, 4764]. Skipping...\n","6337 is not present in [3510, 6298, 4764, 5125]. Skipping...\n","6298 is not present in [2857, 4764, 5125, 2026]. Skipping...\n","4764 is not present in [2780, 5125, 2026, 6885]. Skipping...\n","5125 is not present in [2920, 2026, 6885, 6740]. Skipping...\n","2026 is not present in [1120, 6885, 6740, 7192]. Skipping...\n","6885 is not present in [4349, 6740, 7192, 7175]. Skipping...\n","6740 is not present in [4309, 7192, 7175, 5996]. Skipping...\n","7192 is not present in [4310, 7175, 5996, 7265]. Skipping...\n","7175 is not present in [4051, 5996, 7265, 7632]. Skipping...\n","5996 is not present in [2816, 7265, 7632, 7618]. Skipping...\n","7265 is not present in [3980, 7632, 7618, 7863]. Skipping...\n","7632 is not present in [3553, 7618, 7863, 3126]. Skipping...\n","7618 is not present in [3365, 7863, 3126, 8454]. Skipping...\n","7863 is not present in [3270, 3126, 8454, 8190]. Skipping...\n","3126 is not present in [1899, 8454, 8190, 8093]. Skipping...\n","8454 is not present in [5333, 8190, 8093, 5598]. Skipping...\n","8190 is not present in [5237, 8093, 5598, 6135]. Skipping...\n","8093 is not present in [5066, 5598, 6135, 7930]. Skipping...\n","5598 is not present in [2414, 6135, 7930, 8275]. Skipping...\n","6135 is not present in [2996, 7930, 8275, 8257]. Skipping...\n","7930 is not present in [2791, 8275, 8257, 8644]. Skipping...\n","8275 is not present in [3134, 8257, 8644, 4317]. Skipping...\n","8257 is not present in [3104, 8644, 4317, 9166]. Skipping...\n","8644 is not present in [3388, 4317, 9166, 7724]. Skipping...\n","4317 is not present in [2154, 9166, 7724, 8854]. Skipping...\n","9166 is not present in [4869, 7724, 8854, 8915]. Skipping...\n","7724 is not present in [4761, 8854, 8915, 5441]. Skipping...\n","8854 is not present in [4790, 8915, 5441, 8684]. Skipping...\n","8915 is not present in [4524, 5441, 8684, 8541]. Skipping...\n","5441 is not present in [2425, 8684, 8541, 8699]. Skipping...\n","8684 is not present in [4304, 8541, 8699, 9225]. Skipping...\n","8541 is not present in [3324, 8699, 9225, 4019]. Skipping...\n","8699 is not present in [3695, 9225, 4019, 9773]. Skipping...\n","9225 is not present in [3720, 4019, 9773, 9415]. Skipping...\n","4019 is not present in [1795, 9773, 9415, 9698]. Skipping...\n","9773 is not present in [6659, 9415, 9698, 9508]. Skipping...\n","9415 is not present in [6469, 9698, 9508, 7600]. Skipping...\n","9698 is not present in [5999, 9508, 7600, 9849]. Skipping...\n","9508 is not present in [5705, 7600, 9849, 10083]. Skipping...\n","7600 is not present in [5332, 9849, 10083, 9193]. Skipping...\n","9849 is not present in [4521, 10083, 9193, 9919]. Skipping...\n","10083 is not present in [5774, 9193, 9919, 4101]. Skipping...\n","9193 is not present in [3459, 9919, 4101, 10495]. Skipping...\n","9919 is not present in [4531, 4101, 10495, 10190]. Skipping...\n","4101 is not present in [3829, 10495, 10190, 10261]. Skipping...\n","10495 is not present in [6420, 10190, 10261, 10048]. Skipping...\n","10190 is not present in [5892, 10261, 10048, 6957]. Skipping...\n","10261 is not present in [6052, 10048, 6957, 9907]. Skipping...\n","10048 is not present in [5824, 6957, 9907, 10111]. Skipping...\n","6957 is not present in [3865, 9907, 10111, 9661]. Skipping...\n","9907 is not present in [4846, 10111, 9661, 7388]. Skipping...\n","10111 is not present in [5607, 9661, 7388, 1802]. Skipping...\n","Correlation: 0.08677588286283226\n","p-value: 0.1397557257474165\n"]}]}]}