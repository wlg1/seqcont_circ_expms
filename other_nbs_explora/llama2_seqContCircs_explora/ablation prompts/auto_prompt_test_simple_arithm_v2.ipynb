{"cells":[{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSKP_OsTDki6"},"outputs":[],"source":["save_files = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1wsEy0MqHU0"},"outputs":[],"source":["%%capture\n","%pip install git+https://github.com/neelnanda-io/TransformerLens.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6b1n2tvIAiD"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","# import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML\n","\n","import pickle\n","from google.colab import files\n","\n","import matplotlib.pyplot as plt\n","import statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuhzYxbsIAiE"},"outputs":[],"source":["import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer #, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFMTUcQiIAiF"},"outputs":[],"source":["torch.set_grad_enabled(False)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQr6WtEppHgy"},"outputs":[],"source":["import pdb"]},{"cell_type":"markdown","metadata":{"id":"Z4iJEGh6b56v"},"source":["## Import functions from repo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4826,"status":"ok","timestamp":1718291389563,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"F8TXMRL3CoPd","outputId":"a642a6e6-5752-48b3-b2a3-8214ca90f30e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'seqcont_circuits'...\n","remote: Enumerating objects: 1022, done.\u001b[K\n","remote: Counting objects: 100% (488/488), done.\u001b[K\n","remote: Compressing objects: 100% (287/287), done.\u001b[K\n","remote: Total 1022 (delta 296), reused 378 (delta 190), pack-reused 534\u001b[K\n","Receiving objects: 100% (1022/1022), 18.76 MiB | 14.06 MiB/s, done.\n","Resolving deltas: 100% (659/659), done.\n","/content/seqcont_circuits/src/iter_node_pruning\n"]}],"source":["!git clone https://github.com/apartresearch/seqcont_circuits.git\n","%cd /content/seqcont_circuits/src/iter_node_pruning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22TI4zjMDMfQ"},"outputs":[],"source":["## comment this out when debugging functions in colab to use funcs defined in colab\n","\n","# don't improt this\n","# # from dataset import Dataset\n","\n","from metrics import *\n","from head_ablation_fns import *\n","from mlp_ablation_fns import *\n","from node_ablation_fns import *\n","from loop_node_ablation_fns import *"]},{"cell_type":"markdown","metadata":{"id":"9R_g1Ghv7cGE"},"source":["## fns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsJmCq-C2Zu6"},"outputs":[],"source":["import random\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPjHv-Xny4R"},"outputs":[],"source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer):  # , S1_is_first=False\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.corr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.incorr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a dict whose values are tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        # for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","        for targ in [key for key in pos_dict]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = self.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZKVG778QYyn"},"outputs":[],"source":["def generate_prompts_list_longer(text, tokens):\n","    prompts_list = []\n","    prompt_dict = {\n","        'corr': str(1),\n","        'incorr': str(2),\n","        'text': text\n","        # 'text': model.to_string(tokens)[0]\n","        }\n","    tokens_as_strs = model.tokenizer.tokenize(text)\n","    # tokens_as_strs = model.to_string(tokens)[0].split()\n","    # for i in range(tokens.shape[1]):\n","    for i, tok in enumerate(tokens_as_strs):\n","        prompt_dict['S'+str(i)] = tok\n","    # for i, tok in enumerate(tokens):\n","    #     prompt_dict['S'+str(i)] = model.to_string(tok)\n","\n","    # prompt_dict = {\n","    #     'corr': '4',\n","    #     'incorr': '3',\n","    #     'text': model.to_string(tokens)[0]\n","    # }\n","    # # list_tokens = tokenizer.tokenize('1 2 3 ')\n","    # tokens_as_strs = model.to_string(tokens)[0].split()\n","    # for i, tok_as_str in enumerate(tokens_as_strs):\n","    #     if tok_as_str == '▁':\n","    #         prompt_dict['S'+str(i)] = ' '\n","    #     else:\n","    #         prompt_dict['S'+str(i)] = tok_as_str\n","    prompts_list.append(prompt_dict)\n","    return prompts_list"]},{"cell_type":"markdown","metadata":{"id":"PDP2cpaiZpPX"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGCiZUPpJUsD"},"outputs":[],"source":["from transformers import LlamaForCausalLM, LlamaTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15677,"status":"ok","timestamp":1718291415617,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"-CocJpgjsf_M","outputId":"e89975eb-ed44-4f06-c7c9-08d3025b4c48"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46380,"status":"ok","timestamp":1718291461888,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"xLgpia0tI6O8","outputId":"3c896b79-f5ba-40bc-f181-ea6b6829272c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c77db85c9ef64ff1b55a54b604de4822","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8301f6476fe548469248aff71d360ece","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87e9d04d06ec4e0ebd60a2c4c509e554","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40ebb98d8e344ea78dcf084bf2d31dfc","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de435dc74c8545b7a2a57bff55b0c1ef","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e77870b7cf1945dc9d545b116d6d949c","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a507fdf33bf4df0a82f15a5695a47d1","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"687993a1eef9460ba806fe46d35febc1","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8454ec884d8f4d34b9917e4049681dfd","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d9ccae72bfe4c8cb48816068bf1d60b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c6f8435e3be4a2b859618c1a2cf0dff","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["LLAMA_2_7B_CHAT_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","tokenizer = LlamaTokenizer.from_pretrained(LLAMA_2_7B_CHAT_PATH)\n","# tokenizer = LlamaTokenizer.from_pretrained(LLAMA_2_7B_CHAT_PATH, use_fast= False, add_prefix_space= False)\n","hf_model = LlamaForCausalLM.from_pretrained(LLAMA_2_7B_CHAT_PATH, low_cpu_mem_usage=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rtZ2e3sMY5S"},"outputs":[],"source":["import transformer_lens.utils as utils\n","from transformer_lens.hook_points import HookPoint\n","from transformer_lens import HookedTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31692,"status":"ok","timestamp":1718291493565,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"sUnSHvA-Myx8","outputId":"c9bdaffb-0763-4028-bc79-a4fb83a9298f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n","Moving model to device:  cuda\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    LLAMA_2_7B_CHAT_PATH,\n","    hf_model = hf_model,\n","    tokenizer = tokenizer,\n","    device = \"cpu\",\n","    fold_ln = False,\n","    center_writing_weights = False,\n","    center_unembed = False,\n",")\n","\n","del hf_model\n","\n","model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"dsdvChbcvgp5"},"source":["# new ablation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KlWYoEy72Cf"},"outputs":[],"source":["def get_heads_actv_mean(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Output: The mean activations of a head's output\n","    '''\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"z\"),\n","    )\n","    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n","\n","    # for layer in range(model.cfg.n_layers):\n","    #     z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n","    #     for template_group in means_dataset.groups:\n","    #         z_for_this_template = z_for_this_layer[template_group]\n","    #         z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n","    #         if z_means_for_this_template.shape[0] == 5:\n","    #             pdb.set_trace()\n","    #         means[layer, template_group] = z_means_for_this_template\n","\n","    del(means_cache)\n","\n","    return means"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFDQMOt9CyVw"},"outputs":[],"source":["# def mask_circ_heads(\n","#     means_dataset: Dataset,\n","#     model: HookedTransformer,\n","#     circuit: Dict[str, List[Tuple[int, int]]],\n","#     seq_pos_to_keep: Dict[str, str],\n","# ) -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","#     '''\n","#     Output: for each layer, a mask of circuit components that should not be ablated\n","#     '''\n","#     heads_and_posns_to_keep = {}\n","#     batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","\n","#     for layer in range(model.cfg.n_layers):\n","\n","#         mask = t.zeros(size=(batch, seq, n_heads))\n","\n","#         for (head_type, head_list) in circuit.items():\n","#             seq_pos = seq_pos_to_keep[head_type]\n","#             # if seq_pos == 'S7':\n","#             #     pdb.set_trace()\n","#             indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","#             for (layer_idx, head_idx) in head_list:\n","#                 if layer_idx == layer:\n","#                     # if indices.item() == 7:\n","#                     #     pdb.set_trace()\n","#                     mask[:, indices, head_idx] = 1\n","#                     # mask[:, :, head_idx] = 1  # keep L.H at all pos\n","\n","#         heads_and_posns_to_keep[layer] = mask.bool()\n","#     # pdb.set_trace()\n","#     return heads_and_posns_to_keep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1boH1469_HI"},"outputs":[],"source":["def mask_circ_heads(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","    '''\n","    Output: for each layer, a mask of circuit components that should not be ablated\n","    '''\n","    heads_and_posns_to_keep = {}\n","    # batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","    batch, seq, n_heads = len(means_dataset), len(circuit.keys()), model.cfg.n_heads\n","    # print(seq)\n","\n","    for layer in range(model.cfg.n_layers):\n","\n","        mask = t.zeros(size=(batch, seq, n_heads))\n","\n","        for (head_type, head_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[head_type]\n","            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","            for (layer_idx, head_idx) in head_list:\n","                if layer_idx == layer:\n","                    # mask[:, indices, head_idx] = 1\n","                    mask[:, :, head_idx] = 1\n","\n","        heads_and_posns_to_keep[layer] = mask.bool()\n","\n","    return heads_and_posns_to_keep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdxeNJ5C9tHx"},"outputs":[],"source":["def hook_func_mask_head(\n","    z: Float[Tensor, \"batch seq head d_head\"],\n","    hook: HookPoint,\n","    # components_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n","    # means: Float[Tensor, \"layer batch seq head d_head\"],\n","    circuit: Dict[str, List[Tuple[int, int]]],\n",") -> Float[Tensor, \"batch seq head d_head\"]:\n","    '''\n","    Use this to not mask components\n","    '''\n","    # mask_for_this_layer = components_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n","    # z = t.where(mask_for_this_layer, z, means[hook.layer()])\n","\n","    ###\n","    # heads_and_posns_to_keep = {}\n","    # batch, seq, n_heads = z.shape[0], z.shape[1], model.cfg.n_heads  # components_to_keep[0].shape[0] is batch\n","\n","    # for layer in range(model.cfg.n_layers):\n","\n","    #     mask = t.zeros(size=(batch, seq, n_heads))\n","\n","    #     for (head_type, head_list) in circuit.items():\n","    #         # seq_pos = seq_pos_to_keep[head_type]\n","    #         # indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","    #         for (layer_idx, head_idx) in head_list:\n","    #             if layer_idx == layer:\n","    #                 # mask[:, indices, head_idx] = 1\n","    #                 mask[:, :, head_idx] = 1\n","\n","    #     heads_and_posns_to_keep[layer] = mask.bool()\n","    ###\n","    mask_for_this_layer = t.zeros(size=(z.shape[0], z.shape[1], z.shape[2]))\n","    for (head_type, head_list) in circuit.items():\n","        # seq_pos = seq_pos_to_keep[head_type]\n","        # indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","        for (layer_idx, head_idx) in head_list:\n","            if layer_idx == hook.layer():\n","                # mask[:, indices, head_idx] = 1\n","                mask_for_this_layer[:, :, head_idx] = 1\n","\n","    mask_for_this_layer = mask_for_this_layer.bool()\n","    mask_for_this_layer = mask_for_this_layer.unsqueeze(-1).to(z.device)  # d_model is 1; then is broadcast in where\n","\n","    z = t.where(mask_for_this_layer, z, 0)\n","\n","    return z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dg3XuWScAVvG"},"outputs":[],"source":["def add_ablation_hook_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Ablate the model, except as components and positions to keep\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","    means = get_heads_actv_mean(means_dataset, model)\n","    components_to_keep = mask_circ_heads(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    hook_fn = partial(\n","        hook_func_mask_head,\n","        # components_to_keep=components_to_keep,\n","        # means=means,\n","        circuit=circuit,\n","    )\n","\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ILjxwH9YUYP"},"outputs":[],"source":["# from dataset import Dataset\n","from transformer_lens import HookedTransformer, utils\n","from transformer_lens.hook_points import HookPoint\n","import einops\n","from functools import partial\n","import torch as t\n","from torch import Tensor\n","from typing import Dict, Tuple, List\n","from jaxtyping import Float, Bool\n","\n","# from head_ablation_fns import *\n","# from mlp_ablation_fns import *\n","\n","def add_ablation_hook_MLP_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    heads_lst, mlp_lst,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    # for i in range(len(model.tokenizer.tokenize(means_dataset.prompts[0]['text']))):\n","    num_pos = len(model.tokenizer(means_dataset.prompts[0]['text']).input_ids)\n","    for i in range(num_pos ):\n","        CIRCUIT['S'+str(i)] = heads_lst\n","        # if i == len(model.tokenizer.tokenize(means_dataset.prompts[0]['text'])) - 1:\n","        # if i == num_pos - 1:\n","        #     SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        # else:\n","        SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = get_heads_actv_mean(means_dataset, model)\n","    # Convert this into a boolean map\n","    components_to_keep = mask_circ_heads(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_func_mask_head,\n","        # components_to_keep=components_to_keep,\n","        # means=means,\n","        circuit=CIRCUIT,\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    # if all_entries_true(components_to_keep) == False:\n","    #     pdb.set_trace()\n","    ########################\n","    # CIRCUIT = {}\n","    # SEQ_POS_TO_KEEP = {}\n","    # # for i in range(len(model.tokenizer.tokenize(means_dataset.prompts[0]['text']))):\n","    # num_pos = len(model.tokenizer(means_dataset.prompts[0]['text']).input_ids)\n","    # for i in range(num_pos ):\n","    #     CIRCUIT['S'+str(i)] = mlp_lst\n","    #     # if i == len(model.tokenizer.tokenize(means_dataset.prompts[0]['text'])) - 1:\n","    #     # if i == num_pos - 1:\n","    #     #     SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","    #     # else:\n","    #     SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    # # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    # means = get_MLPs_actv_mean(means_dataset, model)\n","\n","    # # Convert this into a boolean map\n","    # components_to_keep = mask_circ_MLPs(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # # Get a hook function which will patch in the mean z values for each head, at\n","    # # all positions which aren't important for the circuit\n","    # hook_fn = partial(\n","    #     hook_func_mask_mlp_out,\n","    #     components_to_keep=components_to_keep,\n","    #     means=means\n","    # )\n","\n","    # model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-YuOEDieLgE"},"outputs":[],"source":["def all_entries_true(tensor_dict):\n","    for key, tensor in tensor_dict.items():\n","        if not torch.all(tensor).item():\n","            return False\n","    return True"]},{"cell_type":"markdown","metadata":{"id":"jtaV1q3SBHow"},"source":["# ablation fns mult tok answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgbtY5fFPb71"},"outputs":[],"source":["def clean_gen(model, clean_text, corr_ans):\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    tokens = model.to_tokens(clean_text).to(device)\n","    # tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","\n","    total_score = 0\n","    corr_ans_tokLen = 0\n","    ans_so_far = ''\n","    # while True:\n","    for i in range(5):\n","        # print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","        logits = model(tokens)\n","        next_token = logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","        next_char = model.to_string(next_token)\n","\n","        corr_logits = logits[:, -1, next_token]\n","        total_score += corr_logits\n","        # print(f\"logit diff of new char: {corr_logits}\")\n","\n","        ans_so_far += next_char\n","        corr_ans_tokLen += 1\n","        # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","        if ans_so_far == corr_ans:\n","            # print('\\nTotal logit diff: ', total_score.item())\n","            break\n","\n","        # Define new input sequence, by appending the previously generated token\n","        tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","        # if next_char == '':\n","        #     next_char = ' '\n","        # clean_text = clean_text + next_char\n","        # tokens = model.to_tokens(clean_text).to(device)\n","    return corr_ans_tokLen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lp4MyZ52cUTK"},"outputs":[],"source":["def ablate_then_gen(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, corr_ans_tokLen):\n","    tokens = model.to_tokens(clean_text).to(device)\n","    prompts_list = generate_prompts_list_longer(clean_text, tokens)\n","\n","    corr_tokens = model.to_tokens(corr_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    pos_dict = {}\n","    num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","    for i in range(num_pos ):\n","        pos_dict['S'+str(i)] = i\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    logits = model(tokens)\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    next_char = model.to_string(next_token)\n","\n","    total_score = 0\n","\n","    for i in range(corr_ans_tokLen):\n","        if next_char == '':\n","            next_char = ' '\n","\n","        clean_text = clean_text + next_char\n","        if i == corr_ans_tokLen - 1:\n","            print(model.to_string(tokens))\n","            # print(f\"Sequence so far: {clean_text}\")\n","            # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","        tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","\n","        # get new ablation dataset\n","        # model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","        # corr_text = corr_text + next_char\n","        # corr_tokens = torch.cat([corr_tokens, next_token[None, None]], dim=-1)\n","        # prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","        # pos_dict = {}\n","        # num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","        # for i in range(num_pos ):\n","        #     pos_dict['S'+str(i)] = i\n","\n","        # dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, corr_tokens)\n","\n","        # model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","        logits = model(tokens)\n","        next_token = logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","        next_char = model.to_string(next_token)\n","\n","        # new_score = get_logit_diff(logits, dataset)\n","        # total_score += new_score\n","        # print(f\"corr logit of new char: {new_score}\")\n","    # print('\\n Total corr logit: ', total_score.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8KYb2BBSm-G"},"outputs":[],"source":["# Function to randomly choose 50 pairs ensuring less than 10 overlap with heads_of_circ\n","def choose_heads_to_remove(filtered_pairs, heads_of_circ, num_pairs=50, max_overlap=10):\n","    while True:\n","        head_to_remove = random.sample(filtered_pairs, num_pairs)\n","        overlap_count = len([head for head in head_to_remove if head in heads_of_circ])\n","        if overlap_count < max_overlap:\n","            return head_to_remove"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWyYbvi3Mout"},"outputs":[],"source":["def ablate_auto_score(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans_tokLen):  # correct_ans\n","    tokens = model.to_tokens(clean_text).to(device)\n","    prompts_list = generate_prompts_list_longer(clean_text, tokens)\n","\n","    corr_tokens = model.to_tokens(corr_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    pos_dict = {}\n","    num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","    for i in range(num_pos ):\n","        pos_dict['S'+str(i)] = i\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    # logits = model(tokens)\n","    # next_token = logits[0, -1].argmax(dim=-1)\n","    # next_char = model.to_string(next_token)\n","\n","    total_score = 0\n","    ans_so_far = ''\n","    for i in range(correct_ans_tokLen):\n","        # if next_char == '':\n","        #     next_char = ' '\n","\n","        # clean_text = clean_text + next_char\n","        # if i == correct_ans_tokLen - 1:\n","        #     print(model.to_string(tokens))\n","        #     # print(f\"Sequence so far: {clean_text}\")\n","        #     # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","        # tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","\n","        # get new ablation dataset\n","        # model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","        # corr_text = corr_text + next_char\n","        # corr_tokens = torch.cat([corr_tokens, next_token[None, None]], dim=-1)\n","        # prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","        # pos_dict = {}\n","        # num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","        # for i in range(num_pos ):\n","        #     pos_dict['S'+str(i)] = i\n","\n","        # dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, corr_tokens)\n","\n","        # model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","        logits = model(tokens)\n","        next_token = logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","        next_char = model.to_string(next_token)\n","\n","        if next_char == '':\n","            next_char = ' '\n","\n","        clean_text = clean_text + next_char\n","        # if i == correct_ans_tokLen - 1:\n","            # print(model.to_string(tokens))\n","            # print(f\"Sequence so far: {clean_text}\")\n","            # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","        tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","\n","        ans_so_far += next_char\n","        correct_ans_tokLen += 1\n","        # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","    # if ans_so_far == corr_ans:\n","        # print('\\nTotal logit diff: ', total_score.item())\n","    return ans_so_far\n","\n","        # new_score = get_logit_diff(logits, dataset)\n","        # total_score += new_score\n","        # print(f\"corr logit of new char: {new_score}\")\n","    # print('\\n Total corr logit: ', total_score.item())"]},{"cell_type":"markdown","metadata":{"id":"JPKiYdKTAMni"},"source":["# Define circs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1718291493566,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"YId1M9rIroEe","outputId":"658da91b-9de1-42a7-ad7a-f4b789e13136"},"outputs":[{"data":{"text/plain":["84"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# from Llama2_numerals_1to10.ipynb\n","nums_1to9 = [(0, 2), (0, 5), (0, 6), (0, 15), (1, 15), (1, 28), (2, 13), (2, 24), (3, 24), (4, 3), (4, 16), (5, 11), (5, 13), (5, 15), (5, 16), (5, 23), (5, 25), (5, 27), (6, 11), (6, 14), (6, 20), (6, 23), (6, 24), (6, 26), (6, 28), (6, 30), (6, 31), (7, 0), (7, 13), (7, 21), (7, 30), (8, 0), (8, 2), (8, 12), (8, 15), (8, 26), (8, 27), (8, 30), (8, 31), (9, 15), (9, 16), (9, 23), (9, 26), (9, 27), (9, 29), (9, 31), (10, 1), (10, 13), (10, 18), (10, 23), (10, 29), (11, 7), (11, 8), (11, 9), (11, 17), (11, 18), (11, 25), (11, 28), (12, 18), (12, 19), (12, 23), (12, 27), (13, 6), (13, 11), (13, 20), (14, 18), (14, 19), (14, 20), (14, 21), (16, 0), (18, 19), (18, 21), (18, 25), (18, 26), (18, 31), (19, 28), (20, 17), (21, 0), (21, 2), (22, 18), (22, 20), (22, 25), (23, 27), (26, 2)]\n","len(nums_1to9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1718291493566,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"wzTeD-OvAOwC","outputId":"bee7affb-ff3c-4744-a891-a6fceba9dbc8"},"outputs":[{"data":{"text/plain":["82"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# nw_circ = [(0, 1), (0, 4), (0, 6), (0, 7), (0, 8), (0, 10), (0, 11), (0, 12), (1, 16), (1, 24), (1, 27), (1, 28), (2, 2), (2, 5), (2, 8), (2, 24), (2, 30), (3, 7), (3, 14), (3, 19), (3, 23), (4, 3), (5, 16), (5, 25), (6, 11), (6, 14), (7, 0), (7, 30), (8, 0), (8, 2), (8, 3), (8, 4), (8, 6), (8, 21), (8, 31), (9, 1), (9, 3), (9, 7), (9, 11), (9, 29), (9, 31), (10, 13), (10, 18), (10, 23), (10, 24), (10, 25), (10, 27), (11, 18), (11, 28), (12, 18), (12, 26), (13, 11), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 23), (14, 7), (14, 14), (15, 25), (15, 28), (16, 0), (16, 12), (16, 14), (16, 15), (16, 16), (16, 19), (16, 24), (16, 29), (17, 17), (17, 23), (17, 31), (18, 31), (19, 12), (20, 17), (27, 20), (27, 25), (27, 27), (27, 31), (28, 5), (29, 5)]\n","# in order from most impt to least based on how much changes perf when ablated\n","nw_circ = [(20, 17), (5, 25), (16, 0), (29, 5), (3, 19), (6, 11), (15, 25), (8, 0), (16, 24), (8, 4), (7, 0), (6, 14), (16, 29), (5, 16), (12, 26), (4, 3), (3, 7), (7, 30), (11, 28), (28, 5), (17, 31), (13, 11), (13, 20), (12, 18), (1, 27), (10, 13), (18, 31), (8, 6), (9, 1), (0, 4), (2, 2), (9, 11), (19, 12), (1, 16), (13, 17), (9, 7), (11, 18), (2, 24), (10, 18), (9, 31), (9, 29), (2, 30), (2, 5), (1, 24), (2, 8), (15, 28), (27, 31), (16, 14), (3, 23), (3, 14), (10, 23), (27, 20), (8, 3), (14, 7), (14, 14), (16, 15), (8, 2), (17, 17), (0, 1), (10, 27), (16, 19), (0, 8), (0, 12), (1, 28), (0, 11), (17, 23), (0, 10), (0, 6), (13, 19), (8, 31), (10, 24), (16, 12), (13, 23), (13, 21), (27, 27), (9, 3), (27, 25), (16, 16), (8, 21), (0, 7), (13, 18), (10, 25)]\n","len(nw_circ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1718291493566,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"a8zrblGeHiND","outputId":"b088a2fb-06c9-48a7-8830-b296d11c5cf5"},"outputs":[{"data":{"text/plain":["64"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# impt_months_heads = ([(23, 17), (17, 11), (16, 0), (26, 14), (18, 9), (5, 25), (22, 20), (6, 24), (26, 9), (12, 18), (13, 20), (19, 12), (27, 29), (13, 14), (16, 14), (12, 26), (19, 30), (16, 18), (31, 27), (26, 28), (16, 1), (18, 1), (19, 28), (18, 31), (29, 4), (17, 0), (14, 1), (17, 12), (12, 15), (28, 16), (10, 1), (16, 19), (9, 27), (30, 1), (19, 27), (0, 3), (15, 11), (21, 3), (11, 19), (12, 0), (23, 11), (8, 14), (16, 8), (22, 13), (13, 3), (4, 19), (14, 15), (12, 20), (19, 16), (18, 5)])\n","months_circ = [(20, 17), (6, 11), (16, 0), (5, 15), (17, 11), (23, 16), (5, 25), (7, 0), (26, 14), (6, 14), (12, 22), (8, 4), (12, 15), (16, 29), (15, 25), (5, 16), (18, 31), (14, 7), (11, 18), (4, 12), (3, 19), (12, 2), (11, 28), (4, 3), (18, 9), (8, 14), (12, 3), (11, 2), (10, 13), (4, 16), (1, 22), (11, 16), (3, 15), (13, 31), (2, 4), (2, 16), (8, 13), (0, 13), (8, 15), (12, 28), (1, 5), (0, 4), (0, 25), (3, 24), (13, 11), (1, 24), (8, 16), (13, 8), (3, 26), (0, 6), (3, 23), (1, 3), (14, 3), (8, 19), (8, 12), (14, 2), (8, 5), (1, 28), (8, 20), (2, 30), (8, 6), (10, 1), (13, 20), (19, 27)]\n","len(months_circ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1718291493566,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"e2XFugMsd3BY","outputId":"c221078b-b4f5-4f40-cb4e-ed9d047e8ed0"},"outputs":[{"data":{"text/plain":["16"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["intersect_all = list(set(nums_1to9) & set(nw_circ) & set(months_circ))\n","len(intersect_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1718291493566,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"i2CL3XqQ4E7R","outputId":"b6c01d0d-e3a3-4d74-ce10-804582fdbd41"},"outputs":[{"data":{"text/plain":["172"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["union_all = list(set(nums_1to9) | set(nw_circ) | set(months_circ))\n","len(union_all)"]},{"cell_type":"markdown","metadata":{"id":"qCVySrfhk-YH"},"source":["# auto measure fns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Njf4fDdk_k9"},"outputs":[],"source":["def ablate_circ_autoScore(model, circuit, sequences_as_str, next_members):\n","    corr_text = \"5 3 9\"\n","    list_outputs = []\n","    score = 0\n","    for clean_text, correct_ans in zip(sequences_as_str, next_members):\n","        correct_ans_tokLen = clean_gen(model, clean_text, correct_ans)\n","\n","        heads_not_ablate = [(layer, head) for layer in range(32) for head in range(32)]  # unablated\n","        head_to_remove = circuit\n","        heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","\n","        mlps_not_ablate = [layer for layer in range(32)]\n","\n","        output_after_ablate = ablate_auto_score(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans_tokLen)\n","        list_outputs.append(output_after_ablate)\n","        print(correct_ans, output_after_ablate)\n","        if correct_ans == output_after_ablate:\n","            score += 1\n","    perc_score = score / len(next_members)\n","    return perc_score, list_outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CswxAMn4oRfW"},"outputs":[],"source":["def ablate_randcirc_autoScore(model, sequences_as_str, next_members, num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap):\n","    corr_text = \"5 3 9\"\n","    list_outputs = []\n","    all_scores = []\n","    for clean_text, correct_ans in zip(sequences_as_str, next_members):\n","        prompt_score = 0\n","        correct_ans_tokLen = clean_gen(model, clean_text, correct_ans)\n","        for j in range(num_rand_runs):\n","            all_possible_pairs =  [(layer, head) for layer in range(32) for head in range(32)]\n","            filtered_pairs = [pair for pair in all_possible_pairs if pair not in heads_not_overlap] # Filter out heads_not_overlap from all_possible_pairs\n","\n","            # Randomly choose num_heads_rand pairs ensuring less than num_not_overlap overlaps with heads_not_overlap\n","            head_to_remove = choose_heads_to_remove(filtered_pairs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","\n","            heads_not_ablate = [x for x in all_possible_pairs if x not in head_to_remove]\n","\n","            mlps_not_ablate = [layer for layer in range(32)]\n","\n","            output_after_ablate = ablate_auto_score(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans_tokLen)\n","            # list_outputs.append(output_after_ablate)\n","            # print(correct_ans, output_after_ablate)\n","            if correct_ans == output_after_ablate:\n","                prompt_score += 1\n","        print(prompt_score / num_rand_runs)\n","        all_scores.append(prompt_score / num_rand_runs)\n","\n","    perc_score = sum(all_scores) / len(next_members)\n","    return perc_score, list_outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHNPYffBKwNe"},"outputs":[],"source":["# import random\n","\n","# def gen_single_addition_prompts(num_prompts):\n","#     sequences = []\n","#     next_members = []\n","\n","#     single_digit_additions = [(random.randint(0, 9), random.randint(0, 9)) for _ in range(num_prompts)]\n","#     # double_digit_additions = [(random.randint(10, 99), random.randint(10, 99)) for _ in range(num_prompts)]\n","\n","#     for a, b in single_digit_additions:\n","#         prompt = f\"{a} + {b} = \"\n","#         answer = str(a + b)\n","#         sequences.append(prompt)\n","#         next_members.append(answer)\n","\n","#     # for a, b in double_digit_additions:\n","#     #     prompt = f\"{a} + {b} = \"\n","#     #     answer = str(a + b)\n","#     #     sequences.append(prompt)\n","#     #     next_members.append(answer)\n","\n","#     print(\"Sequences:\")\n","#     print(sequences)\n","#     print(\"\\nNext Members:\")\n","#     print(next_members)\n","#     return sequences, next_members\n","\n","import random\n","\n","def gen_single_addition_prompts(num_prompts):\n","    sequences = []\n","    next_members = []\n","    seen_pairs = set()\n","\n","    while len(sequences) < num_prompts:\n","        a = random.randint(0, 9)\n","        b = random.randint(0, 9)\n","        if (a, b) not in seen_pairs:\n","            seen_pairs.add((a, b))\n","            prompt = f\"{a} + {b} = \"\n","            answer = str(a + b)\n","            sequences.append(prompt)\n","            next_members.append(answer)\n","\n","    print(\"Sequences:\")\n","    print(sequences)\n","    print(\"\\nNext Members:\")\n","    print(next_members)\n","    return sequences, next_members"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQdQbd5NYyso"},"outputs":[],"source":["# import random\n","\n","# def gen_double_addition_prompts(num_prompts):\n","#     sequences = []\n","#     next_members = []\n","\n","#     # single_digit_additions = [(random.randint(0, 9), random.randint(0, 9)) for _ in range(num_prompts)]\n","#     double_digit_additions = [(random.randint(10, 99), random.randint(10, 99)) for _ in range(num_prompts)]\n","\n","#     # for a, b in single_digit_additions:\n","#     #     prompt = f\"{a} + {b} = \"\n","#     #     answer = str(a + b)\n","#     #     sequences.append(prompt)\n","#     #     next_members.append(answer)\n","\n","#     for a, b in double_digit_additions:\n","#         prompt = f\"{a} + {b} = \"\n","#         answer = str(a + b)\n","#         sequences.append(prompt)\n","#         next_members.append(answer)\n","\n","#     print(\"Sequences:\")\n","#     print(sequences)\n","#     print(\"\\nNext Members:\")\n","#     print(next_members)\n","#     return sequences, next_members\n","\n","import random\n","\n","def gen_double_addition_prompts(num_prompts):\n","    sequences = []\n","    next_members = []\n","    seen_pairs = set()\n","\n","    while len(sequences) < num_prompts:\n","        a = random.randint(10, 99)\n","        b = random.randint(10, 99)\n","        if (a, b) not in seen_pairs:\n","            seen_pairs.add((a, b))\n","            prompt = f\"{a} + {b} = \"\n","            answer = str(a + b)\n","            sequences.append(prompt)\n","            next_members.append(answer)\n","\n","    print(\"Sequences:\")\n","    print(sequences)\n","    print(\"\\nNext Members:\")\n","    print(next_members)\n","    return sequences, next_members"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9caYhdQf2H3s"},"outputs":[],"source":["# import random\n","\n","# def gen_s_subtraction_prompts(num_prompts):\n","#     sequences = []\n","#     next_members = []\n","\n","#     single_digit_subtractions = [(random.randint(0, 9), random.randint(0, 9)) for _ in range(num_prompts)]\n","#     # double_digit_subtractions = [(random.randint(10, 99), random.randint(10, 99)) for _ in range(num_prompts)]\n","\n","#     for a, b in single_digit_subtractions:\n","#         # Ensure a is greater than or equal to b to avoid negative results\n","#         a, b = max(a, b), min(a, b)\n","#         prompt = f\"{a} - {b} = \"\n","#         answer = str(a - b)\n","#         sequences.append(prompt)\n","#         next_members.append(answer)\n","\n","#     # for a, b in double_digit_subtractions:\n","#     #     # Ensure a is greater than or equal to b to avoid negative results\n","#     #     a, b = max(a, b), min(a, b)\n","#     #     prompt = f\"{a} - {b} = \"\n","#     #     answer = str(a - b)\n","#     #     sequences.append(prompt)\n","#     #     next_members.append(answer)\n","\n","#     print(\"Sequences:\")\n","#     print(sequences)\n","#     print(\"\\nNext Members:\")\n","#     print(next_members)\n","#     return sequences, next_members\n","\n","import random\n","\n","def gen_s_subtraction_prompts(num_prompts):\n","    sequences = []\n","    next_members = []\n","    seen_pairs = set()\n","\n","    while len(sequences) < num_prompts:\n","        a = random.randint(0, 9)\n","        b = random.randint(0, 9)\n","        a, b = max(a, b), min(a, b)  # Ensure a is greater than or equal to b to avoid negative results\n","        if (a, b) not in seen_pairs:\n","            seen_pairs.add((a, b))\n","            prompt = f\"{a} - {b} = \"\n","            answer = str(a - b)\n","            sequences.append(prompt)\n","            next_members.append(answer)\n","\n","    print(\"Sequences:\")\n","    print(sequences)\n","    print(\"\\nNext Members:\")\n","    print(next_members)\n","    return sequences, next_members"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWtlECocO_QX"},"outputs":[],"source":["# import random\n","\n","# def gen_d_subtraction_prompts(num_prompts):\n","#     sequences = []\n","#     next_members = []\n","\n","#     # single_digit_subtractions = [(random.randint(0, 9), random.randint(0, 9)) for _ in range(num_prompts)]\n","#     double_digit_subtractions = [(random.randint(10, 99), random.randint(10, 99)) for _ in range(num_prompts)]\n","\n","#     # for a, b in single_digit_subtractions:\n","#     #     # Ensure a is greater than or equal to b to avoid negative results\n","#     #     a, b = max(a, b), min(a, b)\n","#     #     prompt = f\"{a} - {b} = \"\n","#     #     answer = str(a - b)\n","#     #     sequences.append(prompt)\n","#     #     next_members.append(answer)\n","\n","#     for a, b in double_digit_subtractions:\n","#         # Ensure a is greater than or equal to b to avoid negative results\n","#         a, b = max(a, b), min(a, b)\n","#         prompt = f\"{a} - {b} = \"\n","#         answer = str(a - b)\n","#         sequences.append(prompt)\n","#         next_members.append(answer)\n","\n","#     print(\"Sequences:\")\n","#     print(sequences)\n","#     print(\"\\nNext Members:\")\n","#     print(next_members)\n","#     return sequences, next_members\n","\n","import random\n","\n","def gen_d_subtraction_prompts(num_prompts):\n","    sequences = []\n","    next_members = []\n","    seen_pairs = set()\n","\n","    while len(sequences) < num_prompts:\n","        a = random.randint(10, 99)\n","        b = random.randint(10, 99)\n","        a, b = max(a, b), min(a, b)  # Ensure a is greater than or equal to b to avoid negative results\n","        if (a, b) not in seen_pairs:\n","            seen_pairs.add((a, b))\n","            prompt = f\"{a} - {b} = \"\n","            answer = str(a - b)\n","            sequences.append(prompt)\n","            next_members.append(answer)\n","\n","    print(\"Sequences:\")\n","    print(sequences)\n","    print(\"\\nNext Members:\")\n","    print(next_members)\n","    return sequences, next_members\n"]},{"cell_type":"markdown","metadata":{"id":"sav9M8Rzw25Y"},"source":["# addition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1718291508999,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"djv1stYSw25Z","outputId":"36216998-7a62-462c-b184-a17bdab76932"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequences:\n","['1 + 1 = ', '9 + 9 = ', '7 + 7 = ', '1 + 9 = ', '4 + 1 = ', '6 + 0 = ', '4 + 0 = ', '0 + 4 = ', '3 + 3 = ', '7 + 7 = ', '1 + 9 = ', '8 + 3 = ', '3 + 2 = ', '5 + 1 = ', '2 + 4 = ', '2 + 5 = ', '1 + 6 = ', '8 + 2 = ', '8 + 0 = ', '9 + 9 = ', '9 + 8 = ', '8 + 9 = ', '2 + 9 = ', '7 + 1 = ', '2 + 9 = ', '5 + 1 = ', '5 + 5 = ', '6 + 0 = ', '5 + 5 = ', '5 + 0 = ', '5 + 4 = ', '9 + 4 = ', '4 + 8 = ', '8 + 2 = ', '0 + 4 = ', '8 + 7 = ', '3 + 7 = ', '7 + 5 = ', '8 + 0 = ', '3 + 8 = ', '1 + 9 = ', '7 + 2 = ', '4 + 9 = ', '6 + 4 = ', '5 + 6 = ', '0 + 9 = ', '5 + 6 = ', '7 + 2 = ', '5 + 8 = ', '2 + 4 = ', '78 + 98 = ', '71 + 87 = ', '71 + 59 = ', '15 + 82 = ', '74 + 16 = ', '28 + 78 = ', '21 + 13 = ', '20 + 66 = ', '23 + 36 = ', '49 + 50 = ', '48 + 71 = ', '33 + 84 = ', '76 + 15 = ', '46 + 59 = ', '25 + 41 = ', '71 + 29 = ', '50 + 92 = ', '26 + 18 = ', '94 + 99 = ', '96 + 25 = ', '13 + 48 = ', '52 + 10 = ', '85 + 61 = ', '75 + 50 = ', '53 + 42 = ', '78 + 53 = ', '82 + 51 = ', '89 + 99 = ', '70 + 75 = ', '40 + 56 = ', '48 + 60 = ', '57 + 68 = ', '55 + 96 = ', '84 + 56 = ', '35 + 70 = ', '66 + 73 = ', '98 + 26 = ', '17 + 60 = ', '71 + 20 = ', '60 + 92 = ', '41 + 42 = ', '71 + 27 = ', '25 + 29 = ', '33 + 96 = ', '73 + 62 = ', '26 + 30 = ', '44 + 94 = ', '61 + 16 = ', '37 + 20 = ', '96 + 66 = ']\n","\n","Next Members:\n","['2', '18', '14', '10', '5', '6', '4', '4', '6', '14', '10', '11', '5', '6', '6', '7', '7', '10', '8', '18', '17', '17', '11', '8', '11', '6', '10', '6', '10', '5', '9', '13', '12', '10', '4', '15', '10', '12', '8', '11', '10', '9', '13', '10', '11', '9', '11', '9', '13', '6', '176', '158', '130', '97', '90', '106', '34', '86', '59', '99', '119', '117', '91', '105', '66', '100', '142', '44', '193', '121', '61', '62', '146', '125', '95', '131', '133', '188', '145', '96', '108', '125', '151', '140', '105', '139', '124', '77', '91', '152', '83', '98', '54', '129', '135', '56', '138', '77', '57', '162']\n"]}],"source":["num_prompts = 50\n","sequences_as_str, next_members = gen_addition_prompts(num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8pavLs_djnU"},"outputs":[],"source":["# all_heads = [(layer, head) for layer in range(32) for head in range(32)]\n","# input the circuit to ablate, not what to keep\n","# perc_score, list_outputs = ablate_circ_autoScore(model, [], sequences_as_str, next_members)\n","# perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":62870,"status":"error","timestamp":1718291572410,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"V7Dg9Pc9w25Z","outputId":"ffabe315-89e3-48a2-d1fd-0afd3a62f4b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["2 2\n","18 18\n","14 14\n","10 10\n","5 5\n","6 6\n","4 4\n","4 4\n","6 6\n","14 14\n","10 10\n","11 11\n","5 5\n","6 6\n","6 6\n","7 7\n","7 7\n","10 10\n","8 8<0x0A><0x0A>8+\n","18 18\n","17 17\n","17 17\n","11 11\n","8 9<0x0A><0x0A>Yourturn\n","11 11\n","6 6\n","10 10\n","6 6\n","10 10\n","5 5<0x0A><0x0A>5+\n","9 9\n","13 13\n","12 12\n","10 10\n","4 4\n","15 15\n","10 10\n","12 12\n","8 8<0x0A><0x0A>8+\n","11 11\n","10 10\n","9 9\n","13 13\n","10 10\n","11 11\n","9 9\n","11 11\n","9 9\n","13 13\n","6 6\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-2697c1838d57>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mablate_circ_autoScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintersect_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences_as_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mperc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-45538775c0c0>\u001b[0m in \u001b[0;36mablate_circ_autoScore\u001b[0;34m(model, circuit, sequences_as_str, next_members)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmlps_not_ablate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput_after_ablate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mablate_auto_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads_not_ablate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlps_not_ablate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_ans_tokLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlist_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_after_ablate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_ans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_after_ablate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-32a1f3357b8c>\u001b[0m in \u001b[0;36mablate_auto_score\u001b[0;34m(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans_tokLen)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpos_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdataset_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts_list_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_ablation_hook_MLP_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads_not_ablate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlps_not_ablate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# logits = model(tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-ab1aa0185647>\u001b[0m in \u001b[0;36madd_ablation_hook_MLP_head\u001b[0;34m(model, means_dataset, heads_lst, mlp_lst, is_permanent)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Apply hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"z\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_permanent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_permanent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# if all_entries_true(components_to_keep) == False:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36madd_hook\u001b[0;34m(self, name, hook, dir, is_permanent, level, prepend)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook_point_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_point_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     self.check_and_add_hook(\n\u001b[0m\u001b[1;32m    317\u001b[0m                         \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                         \u001b[0mhook_point_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mcheck_and_add_hook\u001b[0;34m(self, hook_point, hook_point_name, hook, dir, is_permanent, level, prepend)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mprepend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         )\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mhook_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_permanent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_permanent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     def check_hooks_to_add(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36madd_hook\u001b[0;34m(self, hook, dir, is_permanent, level, prepend)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         full_hook.__name__ = (\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         )  # annotate the `full_hook` with the string representation of the `hook` function\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137898,"status":"ok","timestamp":1718291715129,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"ys884NHJw25Z","outputId":"89176dcc-a444-4f5d-9e26-442487585881"},"outputs":[{"name":"stdout","output_type":"stream","text":["2 1\n","18 18\n","14 14\n","10 12\n","5 5\n","6 6\n","4 4\n","4 4\n","6 3\n","14 14\n","10 12\n","11 12\n","5 3\n","6 6\n","6 1\n","7 1\n","7 1\n","10 10\n","8 8<0x0A><0x0A><0x0A>8\n","18 18\n","17 18\n","17 16\n","11 12\n","8 14<0x0A><0x0A><0x0A>\n","11 12\n","6 6\n","10 10\n","6 6\n","10 10\n","5 5<0x0A><0x0A><0x0A>5\n","9 1\n","13 10\n","12 12\n","10 10\n","4 4\n","15 15\n","10 33\n","12 12\n","8 8<0x0A><0x0A><0x0A>8\n","11 33\n","10 12\n","9 1\n","13 18\n","10 10\n","11 11\n","9 0\n","11 11\n","9 1\n","13 10\n","6 1\n","176 88<0x0A>\n","158 711\n","130 71<0x0A>\n","97 11\n","90 90\n","106 30<0x0A>\n","34 14\n","86 20\n","59 36\n","99 49\n","119 491\n","117 333\n","91 76\n","105 55<0x0A>\n","66 25\n","100 711\n","142 500\n","44 42\n","193 94<0x0A>\n","121 97<0x0A><0x0A> \n","61 11\n","62 62\n","146 85<0x0A>\n","125 755\n","95 53\n","131 788\n","133 82<0x0A>\n","188 99<0x0A>\n","145 75<0x0A>\n","96 40\n","108 60<0x0A>\n","125 588\n","151 56<0x0A>\n","140 140\n","105 335\n","139 70<0x0A>\n","124 1000<0x0A>\n","77 11\n","91 71\n","152 600\n","83 42\n","98 71\n","54 54\n","129 333\n","135 733\n","56 26\n","138 449\n","77 61\n","57 33\n","162 100<0x0A><0x0A>\n"]},{"data":{"text/plain":["0.26"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137674,"status":"ok","timestamp":1718291852685,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"BV5nlp2Bw25Z","outputId":"8fc33cb5-37ba-409c-9e34-e6443e7502e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["2 2\n","18 18\n","14 14\n","10 1+\n","5 5\n","6 6\n","4 4\n","4 4\n","6 6\n","14 14\n","10 1+\n","11 10\n","5 5\n","6 6\n","6 1\n","7 1\n","7 7\n","10 10\n","8 8<0x0A><0x0A>Hinweis:\n","18 18\n","17 18\n","17 10\n","11 10\n","8 8<0x0A><0x0A><0x0A>So\n","11 10\n","6 6\n","10 10\n","6 6\n","10 10\n","5 5<0x0A><0x0A><0x0A>Answer\n","9 1\n","13 14\n","12 18\n","10 10\n","4 4\n","15 15\n","10 10\n","12 12\n","8 8<0x0A><0x0A>Hinweis:\n","11 10\n","10 1+\n","9 1\n","13 10\n","10 10\n","11 11\n","9 9\n","11 11\n","9 1\n","13 10\n","6 1\n","176 80<0x0A>\n","158 50<0x0A>\n","130 71+\n","97 12\n","90 <0x0A><0x0A>\n","106 156\n","34 5<0x0A>\n","86 12\n","59 10\n","99 9<0x0A>\n","119 <0x0A><0x0A> \n","117 110\n","91 12\n","105 <0x0A><0x0A> \n","66 10\n","100 79<0x0A>\n","142 50+\n","44 10\n","193 94+\n","121 10<0x0A><0x0A> \n","61 10\n","62 52\n","146 85+\n","125 120\n","95 10\n","131 115\n","133 85<0x0A>\n","188 170\n","145 70+\n","96 40\n","108 8<0x0A><0x0A>\n","125 125\n","151 10<0x0A>\n","140 84+\n","105 10<0x0A>\n","139 10<0x0A>\n","124 10<0x0A><0x0A> \n","77 7<0x0A>\n","91 71\n","152 60+\n","83 14\n","98 19\n","54 17\n","129 <0x0A><0x0A><0x0A>\n","135 10<0x0A>\n","56 6<0x0A>\n","138 164\n","77 63\n","57 7<0x0A>\n","162 10<0x0A><0x0A> \n"]},{"data":{"text/plain":["0.28"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":65769,"status":"error","timestamp":1718291918318,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"8ZEdPNHVw25Z","outputId":"67b54cc6-14de-4aa8-ac2f-e2042447901f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2 1\n","18 18\n","14 49\n","10 10\n","5 1\n","6 6\n","4 0\n","4 0\n","6 3\n","14 49\n","10 10\n","11 10\n","5 1\n","6 1\n","6 2\n","7 1\n","7 1\n","10 10\n","8 8<0x0A><0x0A><0x0A><0x0A>\n","18 18\n","17 10\n","17 10\n","11 10\n","8 10<0x0A><0x0A><0x0A>\n","11 10\n","6 1\n","10 10\n","6 6\n","10 10\n","5 5<0x0A><0x0A><0x0A><0x0A>\n","9 1\n","13 10\n","12 16\n","10 10\n","4 0\n","15 10\n","10 3+\n","12 15\n","8 8<0x0A><0x0A><0x0A><0x0A>\n","11 3+\n","10 10\n","9 1\n","13 10\n","10 16\n","11 10\n","9 0\n","11 10\n","9 1\n","13 50\n","6 2\n","176 70<0x0A>\n","158 71+\n","130 71+\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-ef0dcd4b0924>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mablate_circ_autoScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonths_circ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences_as_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_members\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mperc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-45538775c0c0>\u001b[0m in \u001b[0;36mablate_circ_autoScore\u001b[0;34m(model, circuit, sequences_as_str, next_members)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmlps_not_ablate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput_after_ablate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mablate_auto_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads_not_ablate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlps_not_ablate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_ans_tokLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlist_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_after_ablate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_ans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_after_ablate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-32a1f3357b8c>\u001b[0m in \u001b[0;36mablate_auto_score\u001b[0;34m(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans_tokLen)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mnext_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get the predicted token at the end of our sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    547\u001b[0m                     )\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 residual = block(\n\u001b[0m\u001b[1;32m    550\u001b[0m                     \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;31m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components/transformer_block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# queries, keys and values, independently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             self.attn(\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mquery_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components/abstract_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_z_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, pos, head_index, d_head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_attn_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components/abstract_attention.py\u001b[0m in \u001b[0;36mcalculate_z_scores\u001b[0;34m(self, v, pattern)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mpattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch head_index query_pos key_pos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     ) -> Float[torch.Tensor, \"batch query_pos head_index d_head\"]:\n\u001b[0;32m--> 438\u001b[0;31m         z = self.hook_z(\n\u001b[0m\u001b[1;32m    439\u001b[0m             einsum(\n\u001b[1;32m    440\u001b[0m                 \u001b[0;31m\"\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0mkey_pos\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mfull_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m    107\u001b[0m             ):  # For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\n\u001b[1;32m    108\u001b[0m                 \u001b[0mmodule_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         full_hook.__name__ = (\n","\u001b[0;32m<ipython-input-20-a6a25ce3bbf3>\u001b[0m in \u001b[0;36mhook_func_mask_head\u001b[0;34m(z, hook, circuit)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# mask[:, indices, head_idx] = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mmask_for_this_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mlayer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0msplit_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NUtXGLgw25a"},"outputs":[],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score, list_outputs = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]},{"cell_type":"markdown","metadata":{"id":"SP_tJVupLvSe"},"source":["# single digit addition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1718293914482,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"qK0cG6WoLvSq","outputId":"6fcef93c-461a-487b-b737-e5b5893351f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequences:\n","['2 + 7 = ', '0 + 5 = ', '2 + 4 = ', '6 + 8 = ', '0 + 2 = ', '3 + 7 = ', '6 + 1 = ', '7 + 9 = ', '8 + 6 = ', '5 + 6 = ', '9 + 0 = ', '2 + 5 = ', '4 + 0 = ', '0 + 9 = ', '6 + 4 = ', '0 + 1 = ', '0 + 3 = ', '1 + 8 = ', '1 + 5 = ', '1 + 6 = ', '7 + 5 = ', '3 + 6 = ', '3 + 0 = ', '9 + 5 = ', '5 + 3 = ', '7 + 0 = ', '9 + 8 = ', '5 + 4 = ', '1 + 9 = ', '1 + 3 = ', '0 + 6 = ', '4 + 7 = ', '1 + 1 = ', '3 + 3 = ', '1 + 4 = ', '2 + 0 = ', '0 + 0 = ', '4 + 1 = ', '4 + 2 = ', '7 + 7 = ', '7 + 8 = ', '4 + 8 = ', '5 + 9 = ', '2 + 2 = ', '8 + 7 = ', '8 + 5 = ', '0 + 4 = ', '1 + 7 = ', '8 + 0 = ', '9 + 4 = ']\n","\n","Next Members:\n","['9', '5', '6', '14', '2', '10', '7', '16', '14', '11', '9', '7', '4', '9', '10', '1', '3', '9', '6', '7', '12', '9', '3', '14', '8', '7', '17', '9', '10', '4', '6', '11', '2', '6', '5', '2', '0', '5', '6', '14', '15', '12', '14', '4', '15', '13', '4', '8', '8', '13']\n"]}],"source":["num_prompts = 50\n","sequences_as_str, next_members = gen_single_addition_prompts(num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERUVlNGdLvSq"},"outputs":[],"source":["# all_heads = [(layer, head) for layer in range(32) for head in range(32)]\n","# input the circuit to ablate, not what to keep\n","# perc_score, list_outputs = ablate_circ_autoScore(model, [], sequences_as_str, next_members)\n","# perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55045,"status":"ok","timestamp":1718293970017,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"2WhTlsIZLvSq","outputId":"75169cd4-6f0f-4a79-de86-b877ed28230b"},"outputs":[{"name":"stdout","output_type":"stream","text":["9 9\n","5 5\n","6 6\n","14 14\n","2 2\n","10 10\n","7 7\n","16 16\n","14 14\n","11 11\n","9 9<0x0A><0x0A>9+\n","7 7\n","4 4\n","9 9\n","10 10\n","1 1\n","3 3\n","9 9\n","6 6\n","7 7\n","12 12\n","9 9\n","3 3\n","14 14\n","8 8\n","7 7<0x0A><0x0A>7+\n","17 17\n","9 9\n","10 10\n","4 2\n","6 6\n","11 11\n","2 2\n","6 6\n","5 5\n","2 2\n","0 0\n","5 5\n","6 <0x0A>\n","14 14\n","15 15\n","12 12\n","14 14\n","4 4\n","15 15\n","13 13\n","4 4\n","8 8\n","8 8<0x0A><0x0A>8+\n","13 13\n"]},{"data":{"text/plain":["0.9"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53683,"status":"ok","timestamp":1718294023237,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"RZJ80XAPLvSq","outputId":"e94cba51-d4fe-4445-a9bf-5ea2b203f6a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["9 1\n","5 5\n","6 1\n","14 6+\n","2 0\n","10 33\n","7 1\n","16 0<0x0A>\n","14 14\n","11 11\n","9 9<0x0A><0x0A><0x0A>9\n","7 1\n","4 4\n","9 0\n","10 10\n","1 1\n","3 0\n","9 1\n","6 1\n","7 1\n","12 12\n","9 3\n","3 3\n","14 14\n","8 1\n","7 7<0x0A><0x0A><0x0A>7\n","17 18\n","9 1\n","10 12\n","4 1\n","6 6\n","11 22\n","2 1\n","6 3\n","5 1\n","2 2\n","0 0\n","5 5\n","6 8\n","14 14\n","15 15\n","12 12\n","14 10\n","4 1\n","15 15\n","13 13\n","4 4\n","8 1\n","8 8<0x0A><0x0A><0x0A>8\n","13 10\n"]},{"data":{"text/plain":["0.38"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53653,"status":"ok","timestamp":1718294076724,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"92JlsGtbLvSq","outputId":"2202243e-c42d-44d3-caac-79bd353f4c89"},"outputs":[{"name":"stdout","output_type":"stream","text":["9 1\n","5 5\n","6 1\n","14 12\n","2 2\n","10 10\n","7 7\n","16 19\n","14 14\n","11 11\n","9 9<0x0A><0x0A><0x0A>Answer\n","7 1\n","4 4\n","9 9\n","10 10\n","1 1\n","3 3\n","9 9\n","6 1\n","7 7\n","12 12\n","9 1\n","3 3\n","14 14\n","8 8\n","7 7<0x0A><0x0A><0x0A>Answer\n","17 18\n","9 1\n","10 1+\n","4 3\n","6 6\n","11 12\n","2 2\n","6 6\n","5 3\n","2 2\n","0 0\n","5 5\n","6 8\n","14 14\n","15 14\n","12 18\n","14 10\n","4 1\n","15 15\n","13 13\n","4 4\n","8 9\n","8 8<0x0A><0x0A>Hinweis:\n","13 14\n"]},{"data":{"text/plain":["0.54"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54387,"status":"ok","timestamp":1718294130936,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"Tj4SIDdMLvSr","outputId":"82ae9244-eccb-40f0-8d6c-601003a4097a"},"outputs":[{"name":"stdout","output_type":"stream","text":["9 1\n","5 0\n","6 2\n","14 6+\n","2 0\n","10 3+\n","7 1\n","16 10\n","14 12\n","11 10\n","9 9<0x0A><0x0A><0x0A><0x0A>\n","7 1\n","4 0\n","9 0\n","10 16\n","1 0\n","3 0\n","9 1\n","6 1\n","7 1\n","12 15\n","9 1\n","3 3\n","14 15\n","8 1\n","7 7<0x0A><0x0A><0x0A><0x0A>\n","17 10\n","9 1\n","10 10\n","4 1\n","6 0\n","11 10\n","2 1\n","6 3\n","5 1\n","2 0\n","0 0\n","5 1\n","6 1\n","14 49\n","15 15\n","12 16\n","14 5+\n","4 2\n","15 10\n","13 10\n","4 0\n","8 1\n","8 8<0x0A><0x0A><0x0A><0x0A>\n","13 10\n"]},{"data":{"text/plain":["0.08"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"jYc96XuOLvSr","outputId":"5f7a901f-8945-474e-9ef3-0cfa050c7b66"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n","0.8\n","0.9\n","0.8\n","1.0\n","0.9\n","1.0\n","1.0\n","0.7\n","0.9\n","0.0\n","1.0\n","0.7\n","0.7\n","1.0\n","0.8\n","0.9\n","1.0\n","1.0\n","1.0\n","1.0\n","1.0\n","0.9\n","1.0\n","1.0\n","0.0\n","0.9\n","0.9\n","0.9\n","0.9\n","0.8\n","1.0\n","1.0\n","1.0\n","0.9\n","0.8\n","0.7\n","1.0\n","0.9\n","1.0\n","0.8\n","0.9\n","1.0\n","0.9\n","1.0\n","0.8\n","0.5\n","1.0\n","0.0\n","0.6\n"]},{"data":{"text/plain":["0.8439999999999998"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score, list_outputs = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]},{"cell_type":"markdown","metadata":{"id":"bxkfYWssLfcQ"},"source":["# double digit addition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ICaarO1SLfcZ","outputId":"2f784a4a-c17a-443f-9925-b6daf6d27d32"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequences:\n","['62 + 33 = ', '71 + 40 = ', '72 + 91 = ', '89 + 88 = ', '50 + 46 = ', '19 + 25 = ', '92 + 97 = ', '99 + 12 = ', '64 + 72 = ', '80 + 11 = ', '22 + 37 = ', '11 + 42 = ', '82 + 17 = ', '56 + 95 = ', '36 + 71 = ', '48 + 56 = ', '17 + 66 = ', '83 + 15 = ', '98 + 87 = ', '22 + 39 = ', '65 + 10 = ', '94 + 76 = ', '60 + 33 = ', '65 + 27 = ', '73 + 57 = ', '74 + 87 = ', '58 + 55 = ', '61 + 16 = ', '44 + 11 = ', '32 + 10 = ', '13 + 83 = ', '68 + 84 = ', '74 + 73 = ', '84 + 67 = ', '11 + 37 = ', '29 + 94 = ', '11 + 55 = ', '59 + 24 = ', '49 + 20 = ', '68 + 14 = ', '56 + 41 = ', '31 + 85 = ', '70 + 84 = ', '99 + 78 = ', '33 + 14 = ', '62 + 47 = ', '29 + 29 = ', '70 + 65 = ', '75 + 49 = ', '20 + 28 = ']\n","\n","Next Members:\n","['95', '111', '163', '177', '96', '44', '189', '111', '136', '91', '59', '53', '99', '151', '107', '104', '83', '98', '185', '61', '75', '170', '93', '92', '130', '161', '113', '77', '55', '42', '96', '152', '147', '151', '48', '123', '66', '83', '69', '82', '97', '116', '154', '177', '47', '109', '58', '135', '124', '48']\n"]}],"source":["num_prompts = 50\n","sequences_as_str, next_members = gen_double_addition_prompts(num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"i1qUcXmfLfca"},"outputs":[],"source":["# all_heads = [(layer, head) for layer in range(32) for head in range(32)]\n","# input the circuit to ablate, not what to keep\n","# perc_score, list_outputs = ablate_circ_autoScore(model, [], sequences_as_str, next_members)\n","# perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HQbbQ_yjLfca","outputId":"00898d3b-6361-473c-eb93-125d9d68e1f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["95 95\n","111 111\n","163 <0x0A><0x0A>Please\n","177 97<0x0A>\n","96 <0x0A><0x0A>\n","44 <0x0A><0x0A>\n","189 <0x0A><0x0A>Pleaseprovidethe\n","111 <0x0A><0x0A>Pleaseenteryour\n","136 <0x0A><0x0A>What\n","91 91\n","59 <0x0A><0x0A>\n","53 53\n","99 99\n","151 <0x0A><0x0A><0x0A>\n","107 71+\n","104 <0x0A><0x0A>Please\n","83 83\n","98 98\n","185 <0x0A><0x0A>Please\n","61 61\n","75 75\n","170 <0x0A><0x0A>Pleaseprovidethe\n","93 <0x0A><0x0A>\n","92 92\n","130 120\n","161 <0x0A><0x0A>Please\n","113 <0x0A><0x0A>What\n","77 77\n","55 55\n","42 42\n","96 98\n","152 <0x0A><0x0A><0x0A>\n","147 147\n","151 <0x0A><0x0A><0x0A>\n","48 48\n","123 123\n","66 66\n","83 83\n","69 <0x0A><0x0A>Pleaseenteryour\n","82 82\n","97 97\n","116 116\n","154 <0x0A><0x0A>Please\n","177 <0x0A><0x0A>Pleaseenteryour\n","47 <0x0A><0x0A>\n","109 <0x0A><0x0A><0x0A>\n","58 58\n","135 <0x0A><0x0A>Please\n","124 <0x0A><0x0A>Pleaseenteryour\n","48 48\n"]},{"data":{"text/plain":["0.46"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"s9vOzrTCLfca","outputId":"dec74e97-5c72-4455-ecfb-610d2b57e575"},"outputs":[{"name":"stdout","output_type":"stream","text":["95 65\n","111 711\n","163 72<0x0A>\n","177 90<0x0A>\n","96 50\n","44 12\n","189 97<0x0A><0x0A><0x0A>\n","111 100<0x0A><0x0A>\n","136 70<0x0A>\n","91 80\n","59 22\n","53 11\n","99 10\n","151 56<0x0A>\n","107 336\n","104 56<0x0A>\n","83 12\n","98 83\n","185 100\n","61 22\n","75 65\n","170 94<0x0A><0x0A><0x0A>\n","93 66\n","92 66\n","130 777\n","161 74<0x0A>\n","113 558\n","77 61\n","55 55\n","42 33\n","96 11\n","152 700\n","147 74<0x0A>\n","151 157\n","48 11\n","123 30<0x0A>\n","66 11\n","83 60\n","69 49+ 2\n","82 70\n","97 56\n","116 331\n","154 704\n","177 100<0x0A><0x0A>\n","47 33\n","109 62+\n","58 59\n","135 705\n","124 76<0x0A><0x0A><0x0A>\n","48 20\n"]},{"data":{"text/plain":["0.02"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"j0vqtQB_Lfca","outputId":"45695d90-2c0c-4255-dd28-208162982502"},"outputs":[{"name":"stdout","output_type":"stream","text":["95 65\n","111 71<0x0A>\n","163 754\n","177 170\n","96 50\n","44 14\n","189 <0x0A><0x0A> 92\n","111 10<0x0A><0x0A><0x0A>\n","136 10<0x0A>\n","91 81\n","59 10\n","53 5<0x0A>\n","99 85\n","151 <0x0A><0x0A> \n","107 <0x0A><0x0A><0x0A>\n","104 <0x0A><0x0A> \n","83 13\n","98 88\n","185 <0x0A><0x0A> \n","61 10\n","75 6<0x0A>\n","170 <0x0A><0x0A> 94\n","93 63\n","92 <0x0A><0x0A>\n","130 35<0x0A>\n","161 560\n","113 135\n","77 63\n","55 5<0x0A>\n","42 32\n","96 13\n","152 140\n","147 14+\n","151 <0x0A><0x0A><0x0A>\n","48 1+\n","123 140\n","66 1+\n","83 14\n","69 9<0x0A><0x0A><0x0A>What\n","82 10\n","97 10\n","116 31+\n","154 70+\n","177 <0x0A><0x0A> 9+\n","47 10\n","109 62+\n","58 18\n","135 70+\n","124 <0x0A><0x0A> 10\n","48 10\n"]},{"data":{"text/plain":["0.0"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CcQms0IMLfca","outputId":"3038579c-f86d-4ab1-ade8-a9ee885286a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["95 6+\n","111 71+\n","163 91<0x0A>\n","177 8+ \n","96 50\n","44 19\n","189 10<0x0A><0x0A><0x0A>\n","111 10<0x0A><0x0A><0x0A>\n","136 12+\n","91 80\n","59 22\n","53 1+\n","99 17\n","151 56<0x0A>\n","107 36<0x0A>\n","104 48+\n","83 17\n","98 15\n","185 10<0x0A>\n","61 22\n","75 10\n","170 124<0x0A><0x0A>\n","93 60\n","92 12\n","130 35<0x0A>\n","161 74+\n","113 10<0x0A>\n","77 61\n","55 44\n","42 32\n","96 13\n","152 16+\n","147 73+\n","151 8<0x0A><0x0A>\n","48 10\n","123 29+\n","66 10\n","83 59\n","69 49+ 2\n","82 16\n","97 56\n","116 31+\n","154 70+\n","177 10<0x0A><0x0A><0x0A>\n","47 10\n","109 62+\n","58 59\n","135 70+\n","124 15+ 4\n","48 20\n"]},{"data":{"text/plain":["0.0"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xeop3Pt5Lfcb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718295498796,"user_tz":-60,"elapsed":551583,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4231d2a6-0d14-41be-deeb-93caef7fb052"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n","0.5\n","0.7\n","0.8\n","1.0\n","1.0\n","0.0\n","0.0\n","0.9\n","0.8\n","0.4\n","0.8\n","0.7\n","0.4\n","0.7\n","0.9\n","0.8\n","0.9\n","0.7\n","0.9\n","1.0\n","0.0\n","0.8\n","0.5\n","0.3\n","0.9\n","0.7\n","0.8\n","0.9\n","0.9\n","0.9\n","0.5\n","0.8\n","0.4\n","0.8\n","1.0\n","0.5\n","1.0\n","0.0\n","0.6\n","0.8\n","0.6\n","1.0\n","0.0\n","0.7\n","0.8\n","1.0\n","1.0\n","0.0\n","0.5\n"]},{"output_type":"execute_result","data":{"text/plain":["0.672"]},"metadata":{},"execution_count":109}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score, list_outputs = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]},{"cell_type":"markdown","metadata":{"id":"VSRhWq7m1gKr"},"source":["# s subtraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CooL76BL1gKs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718295498820,"user_tz":-60,"elapsed":27,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"753a6bbf-18e7-4611-9025-0725cc89aa43"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequences:\n","['2 - 1 = ', '5 - 1 = ', '9 - 0 = ', '6 - 2 = ', '6 - 3 = ', '8 - 4 = ', '9 - 4 = ', '8 - 2 = ', '6 - 4 = ', '7 - 5 = ', '3 - 2 = ', '3 - 0 = ', '2 - 2 = ', '8 - 6 = ', '9 - 7 = ', '6 - 5 = ', '8 - 5 = ', '5 - 3 = ', '6 - 6 = ', '9 - 2 = ', '9 - 9 = ', '9 - 1 = ', '5 - 2 = ', '3 - 3 = ', '9 - 5 = ', '2 - 0 = ', '8 - 1 = ', '7 - 2 = ', '7 - 1 = ', '8 - 3 = ', '8 - 0 = ', '8 - 7 = ', '9 - 3 = ', '1 - 0 = ', '7 - 0 = ', '4 - 0 = ', '4 - 3 = ', '6 - 1 = ', '9 - 8 = ', '8 - 8 = ', '7 - 6 = ', '0 - 0 = ', '1 - 1 = ', '9 - 6 = ', '3 - 1 = ', '7 - 4 = ', '5 - 0 = ', '4 - 1 = ', '7 - 7 = ', '5 - 4 = ']\n","\n","Next Members:\n","['1', '4', '9', '4', '3', '4', '5', '6', '2', '2', '1', '3', '0', '2', '2', '1', '3', '2', '0', '7', '0', '8', '3', '0', '4', '2', '7', '5', '6', '5', '8', '1', '6', '1', '7', '4', '1', '5', '1', '0', '1', '0', '0', '3', '2', '3', '5', '3', '0', '1']\n"]}],"source":["num_prompts = 50\n","sequences_as_str, next_members = gen_s_subtraction_prompts(num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9EN7QOg1gKt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718295539418,"user_tz":-60,"elapsed":40624,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"67abeb4d-940c-4222-d329-98cad87e9e0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 1\n","4 4\n","9 9\n","4 4\n","3 3\n","4 4\n","5 5\n","6 6\n","2 2\n","2 2\n","1 1\n","3 3\n","0 0\n","2 2\n","2 2\n","1 1\n","3 3\n","2 2\n","0 0\n","7 7\n","0 0\n","8 8\n","3 3\n","0 0\n","4 4\n","2 2\n","7 7\n","5 5\n","6 6\n","5 5\n","8 8\n","1 1\n","6 6\n","1 1\n","7 7\n","4 4\n","1 1\n","5 5\n","1 1\n","0 0\n","1 1\n","0 0\n","0 0\n","3 3\n","2 2\n","3 3\n","5 5\n","3 3\n","0 0\n","1 1\n"]},{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":111}],"source":["# all_heads = [(layer, head) for layer in range(32) for head in range(32)]\n","# input the circuit to ablate, not what to keep\n","perc_score, list_outputs = ablate_circ_autoScore(model, [], sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iwwl28va1gKt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718295578858,"user_tz":-60,"elapsed":39442,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3ef4d5f4-f411-4147-c5eb-6b19e61c33f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 2\n","4 4\n","9 9\n","4 4\n","3 6\n","4 <0x0A>\n","5 9\n","6 6\n","2 2\n","2 </s>\n","1 3\n","3 3\n","0 2\n","2 <0x0A>\n","2 </s>\n","1 6\n","3 <0x0A>\n","2 2\n","0 0\n","7 9\n","0 0\n","8 8\n","3 3\n","0 0\n","4 4\n","2 2\n","7 7\n","5 5\n","6 6\n","5 5\n","8 8\n","1 </s>\n","6 6\n","1 1\n","7 7\n","4 4\n","1 <0x0A>\n","5 5\n","1 9\n","0 0\n","1 </s>\n","0 0\n","0 1\n","3 9\n","2 3\n","3 3\n","5 5\n","3 4\n","0 0\n","1 <0x0A>\n"]},{"output_type":"execute_result","data":{"text/plain":["0.58"]},"metadata":{},"execution_count":112}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DT8yyaRA1gKt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718295617991,"user_tz":-60,"elapsed":39135,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"fff13447-8dcb-47af-f2c1-0d81e477aed5"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 2\n","4 5\n","9 9\n","4 3\n","3 2\n","4 2\n","5 1\n","6 4\n","2 1\n","2 3\n","1 1\n","3 3\n","0 2\n","2 2\n","2 9\n","1 1\n","3 1\n","2 1\n","0 1\n","7 4\n","0 9\n","8 9\n","3 2\n","0 1\n","4 4\n","2 2\n","7 8\n","5 3\n","6 7\n","5 2\n","8 8\n","1 8\n","6 3\n","1 1\n","7 7\n","4 4\n","1 1\n","5 6\n","1 9\n","0 8\n","1 4\n","0 0\n","0 1\n","3 1\n","2 3\n","3 2\n","5 5\n","3 4\n","0 4\n","1 2\n"]},{"output_type":"execute_result","data":{"text/plain":["0.28"]},"metadata":{},"execution_count":113}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk5FpHx11gKt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718295657376,"user_tz":-60,"elapsed":39387,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4228f1f8-b424-402b-b8c6-79872bfafd7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 2\n","4 4\n","9 9\n","4 4\n","3 3\n","4 1\n","5 1\n","6 5\n","2 2\n","2 1\n","1 3\n","3 3\n","0 2\n","2 1\n","2 4\n","1 1\n","3 1\n","2 2\n","0 2\n","7 6\n","0 0\n","8 9\n","3 3\n","0 3\n","4 4\n","2 2\n","7 5\n","5 5\n","6 7\n","5 5\n","8 8\n","1 1\n","6 6\n","1 1\n","7 7\n","4 4\n","1 1\n","5 6\n","1 6\n","0 0\n","1 1\n","0 0\n","0 1\n","3 4\n","2 3\n","3 1\n","5 5\n","3 4\n","0 0\n","1 1\n"]},{"output_type":"execute_result","data":{"text/plain":["0.54"]},"metadata":{},"execution_count":114}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esCo_56b1gKt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718295696668,"user_tz":-60,"elapsed":39293,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"692d6edf-d240-45aa-84b3-e9d1feaeda09"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 1\n","4 1\n","9 9\n","4 1\n","3 1\n","4 1\n","5 1\n","6 1\n","2 6\n","2 1\n","1 1\n","3 0\n","0 2\n","2 1\n","2 1\n","1 1\n","3 1\n","2 1\n","0 6\n","7 1\n","0 9\n","8 1\n","3 1\n","0 1\n","4 4\n","2 0\n","7 1\n","5 1\n","6 1\n","5 1\n","8 8\n","1 1\n","6 1\n","1 0\n","7 7\n","4 0\n","1 1\n","5 1\n","1 8\n","0 8\n","1 1\n","0 0\n","0 0\n","3 1\n","2 1\n","3 1\n","5 0\n","3 1\n","0 7\n","1 1\n"]},{"output_type":"execute_result","data":{"text/plain":["0.26"]},"metadata":{},"execution_count":115}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kqZyTJq1gKu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718296016222,"user_tz":-60,"elapsed":319555,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"751fa58d-388c-4bbc-8654-5f0fd04d4e67"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n","0.9\n","1.0\n","1.0\n","1.0\n","0.9\n","0.9\n","1.0\n","0.9\n","0.9\n","0.8\n","0.6\n","0.8\n","0.9\n","0.9\n","1.0\n","0.9\n","1.0\n","0.8\n","0.9\n","0.7\n","0.9\n","0.7\n","0.7\n","0.9\n","0.7\n","1.0\n","1.0\n","0.9\n","0.9\n","1.0\n","0.8\n","1.0\n","0.4\n","0.9\n","0.3\n","0.9\n","1.0\n","1.0\n","0.8\n","0.8\n","1.0\n","0.9\n","1.0\n","1.0\n","0.9\n","0.9\n","0.9\n","0.8\n","1.0\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8759999999999997"]},"metadata":{},"execution_count":116}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score, list_outputs = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]},{"cell_type":"markdown","metadata":{"id":"aSbOSynhPHB6"},"source":["# d subtraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m67S7N77PHB7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718296016474,"user_tz":-60,"elapsed":258,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f16cfc5c-b2da-449e-dbe5-b862f4690106"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequences:\n","['81 - 29 = ', '93 - 39 = ', '45 - 18 = ', '55 - 22 = ', '89 - 88 = ', '52 - 49 = ', '64 - 18 = ', '49 - 35 = ', '61 - 46 = ', '95 - 57 = ', '75 - 29 = ', '98 - 86 = ', '69 - 11 = ', '91 - 40 = ', '98 - 34 = ', '28 - 12 = ', '83 - 76 = ', '97 - 90 = ', '81 - 35 = ', '52 - 20 = ', '97 - 44 = ', '94 - 20 = ', '60 - 51 = ', '44 - 32 = ', '99 - 92 = ', '76 - 52 = ', '91 - 79 = ', '75 - 25 = ', '70 - 25 = ', '80 - 75 = ', '52 - 18 = ', '38 - 22 = ', '93 - 82 = ', '39 - 36 = ', '55 - 21 = ', '71 - 39 = ', '93 - 20 = ', '60 - 28 = ', '36 - 23 = ', '83 - 18 = ', '57 - 25 = ', '78 - 27 = ', '50 - 40 = ', '71 - 26 = ', '82 - 52 = ', '94 - 88 = ', '95 - 54 = ', '44 - 37 = ', '73 - 63 = ', '37 - 29 = ']\n","\n","Next Members:\n","['52', '54', '27', '33', '1', '3', '46', '14', '15', '38', '46', '12', '58', '51', '64', '16', '7', '7', '46', '32', '53', '74', '9', '12', '7', '24', '12', '50', '45', '5', '34', '16', '11', '3', '34', '32', '73', '32', '13', '65', '32', '51', '10', '45', '30', '6', '41', '7', '10', '8']\n"]}],"source":["num_prompts = 50\n","sequences_as_str, next_members = gen_d_subtraction_prompts(num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FCEltWUPHB8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718296075890,"user_tz":-60,"elapsed":59672,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"92283362-5b60-4a04-b763-16621a8c4f73"},"outputs":[{"output_type":"stream","name":"stdout","text":["52 52\n","54 54\n","27 27\n","33 33\n","1 1\n","3 3\n","46 46\n","14 14\n","15 15\n","38 38\n","46 46\n","12 12\n","58 58\n","51 51\n","64 64\n","16 16\n","7 7\n","7 7\n","46 46\n","32 32\n","53 53\n","74 74\n","9 9\n","12 12\n","7 7\n","24 24\n","12 12\n","50 50\n","45 45\n","5 5\n","34 34\n","16 16\n","11 11\n","3 3\n","34 34\n","32 32\n","73 73\n","32 32\n","13 13\n","65 65\n","32 32\n","51 51\n","10 10\n","45 45\n","30 30\n","6 6\n","41 41\n","7 7\n","10 10\n","8 8\n"]},{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":118}],"source":["# all_heads = [(layer, head) for layer in range(32) for head in range(32)]\n","# input the circuit to ablate, not what to keep\n","perc_score, list_outputs = ablate_circ_autoScore(model, [], sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHwTX8HlPHB8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718296133873,"user_tz":-60,"elapsed":57984,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"dbbb771a-49b2-49a2-8fbd-9b843c2589c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["52 52\n","54 54\n","27 27\n","33 33\n","1 0\n","3 7\n","46 46\n","14 14\n","15 15\n","38 38\n","46 46\n","12 12\n","58 59\n","51 51\n","64 64\n","16 14\n","7 8\n","7 8\n","46 46\n","32 32\n","53 53\n","74 74\n","9 8\n","12 12\n","7 1\n","24 24\n","12 12\n","50 50\n","45 45\n","5 <0x0A>\n","34 34\n","16 18\n","11 11\n","3 6\n","34 34\n","32 32\n","73 73\n","32 32\n","13 13\n","65 65\n","32 32\n","51 51\n","10 10\n","45 45\n","30 30\n","6 <0x0A>\n","41 41\n","7 <0x0A>\n","10 10\n","8 9\n"]},{"output_type":"execute_result","data":{"text/plain":["0.72"]},"metadata":{},"execution_count":119}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6iQ8awHPHB8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718296191451,"user_tz":-60,"elapsed":57581,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f0ec6222-50da-444a-ab1e-ffa74b027c72"},"outputs":[{"output_type":"stream","name":"stdout","text":["52 81\n","54 93\n","27 45\n","33 27\n","1 8\n","3 1\n","46 64\n","14 15\n","15 13\n","38 95\n","46 75\n","12 10\n","58 6<0x0A>\n","51 91\n","64 32\n","16 28\n","7 8\n","7 1\n","46 81\n","32 26\n","53 97\n","74 94\n","9 1\n","12 44\n","7 1\n","24 14\n","12 91\n","50 30\n","45 70\n","5 1\n","34 52\n","16 16\n","11 10\n","3 3\n","34 25\n","32 71\n","73 93\n","32 24\n","13 10\n","65 83\n","32 25\n","51 78\n","10 12\n","45 71\n","30 79\n","6 8\n","41 95\n","7 1\n","10 73\n","8 3\n"]},{"output_type":"execute_result","data":{"text/plain":["0.04"]},"metadata":{},"execution_count":120}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2T8Jm2avPHB8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718296247837,"user_tz":-60,"elapsed":56392,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2c26e229-9521-4072-ff0d-9029c06a3357"},"outputs":[{"output_type":"stream","name":"stdout","text":["52 73\n","54 24\n","27 20\n","33 3<0x0A>\n","1 7\n","3 3\n","46 24\n","14 14\n","15 56\n","38 4<0x0A>\n","46 7<0x0A>\n","12 12\n","58 6<0x0A>\n","51 91\n","64 24\n","16 6<0x0A>\n","7 5\n","7 1\n","46 26\n","32 52\n","53 23\n","74 94\n","9 5\n","12 2<0x0A>\n","7 9\n","24 4<0x0A>\n","12 53\n","50 25\n","45 65\n","5 7\n","34 42\n","16 5<0x0A>\n","11 13\n","3 5\n","34 4<0x0A>\n","32 32\n","73 93\n","32 52\n","13 3<0x0A>\n","65 53\n","32 4<0x0A>\n","51 5<0x0A>\n","10 50\n","45 56\n","30 82\n","6 5\n","41 95\n","7 4\n","10 21\n","8 5\n"]},{"output_type":"execute_result","data":{"text/plain":["0.08"]},"metadata":{},"execution_count":121}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBGc_fxBPHB8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718296305403,"user_tz":-60,"elapsed":57567,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3b551338-e47d-4672-b1b7-5c484a65c384"},"outputs":[{"output_type":"stream","name":"stdout","text":["52 81\n","54 39\n","27 45\n","33 10\n","1 8\n","3 2\n","46 6<0x0A>\n","14 49\n","15 46\n","38 35\n","46 29\n","12 86\n","58 11\n","51 40\n","64 34\n","16 28\n","7 3\n","7 7\n","46 81\n","32 10\n","53 44\n","74 10\n","9 6\n","12 44\n","7 9\n","24 36\n","12 79\n","50 5<0x0A>\n","45 70\n","5 8\n","34 18\n","16 18\n","11 32\n","3 3\n","34 15\n","32 39\n","73 10\n","32 60\n","13 12\n","65 18\n","32 25\n","51 7<0x0A>\n","10 10\n","45 71\n","30 82\n","6 4\n","41 54\n","7 4\n","10 33\n","8 3\n"]},{"output_type":"execute_result","data":{"text/plain":["0.06"]},"metadata":{},"execution_count":122}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuFng1zIPHB9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718296759534,"user_tz":-60,"elapsed":454133,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9a911ecf-ebde-4e9e-f0eb-598c4e499cd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.9\n","0.7\n","0.6\n","0.9\n","0.9\n","0.8\n","0.9\n","1.0\n","1.0\n","0.6\n","1.0\n","0.8\n","0.7\n","0.9\n","0.6\n","0.7\n","0.6\n","0.8\n","1.0\n","0.7\n","0.9\n","0.8\n","0.6\n","0.9\n","0.9\n","0.7\n","0.9\n","1.0\n","0.9\n","0.7\n","1.0\n","0.8\n","0.9\n","0.9\n","0.7\n","0.6\n","1.0\n","0.4\n","1.0\n","0.6\n","0.9\n","0.9\n","0.9\n","0.8\n","0.8\n","0.7\n","0.7\n","0.6\n","0.8\n","0.6\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7999999999999997"]},"metadata":{},"execution_count":123}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score, list_outputs = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["DcZG9rm2IAiA","PDP2cpaiZpPX"],"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyO5KguZQcHqFh7sc6MiChLk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}