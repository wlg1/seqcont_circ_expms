{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8Yp3T2aXovnWy5IlEIHRQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wxS8oMwNS4dM"},"source":["# Initial Setup Code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116631,"status":"ok","timestamp":1696539257906,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"eIThWRFlvryT","outputId":"eaeae2d9-e531-4ca7-f4aa-30c8bfdbd1a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\u001b[1m\u001b[31m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[m\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\n","  \u001b[1m\u001b[33m                         \u001b[4mSCRIPT DEPRECATION WARNING\u001b[m                    \u001b[m\n","\n","  \n","  This script, located at \u001b[1mhttps://deb.nodesource.com/setup_X\u001b[m, used to\n","  install Node.js is deprecated now and will eventually be made inactive.\n","\n","  Please visit the NodeSource \u001b[1mdistributions\u001b[m Github and follow the\n","  instructions to migrate your repo.\n","  \u001b[4m\u001b[32m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n","\n","  The \u001b[1mNodeSource\u001b[m Node.js Linux distributions GitHub repository contains\n","  information about which versions of Node.js and which Linux distributions\n","  are supported and how to install it.\n","  \u001b[4m\u001b[32m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n","\n","\n","                          \u001b[4m\u001b[1m\u001b[33mSCRIPT DEPRECATION WARNING\u001b[m\n","\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\u001b[1m\u001b[31m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[m\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\n","\u001b[36m\u001b[1mTO AVOID THIS WAIT MIGRATE THE SCRIPT\u001b[m\n","Continuing in 60 seconds (press Ctrl-C to abort) ...\n","\n","\n","## Installing the NodeSource Node.js 16.x repo...\n","\n","\n","## Populating apt-get cache...\n","\n","+ apt-get update\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n","Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [44.5 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:11 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,197 kB]\n","Get:12 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,128 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,002 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,252 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,226 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,342 kB]\n","Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,081 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,266 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\n","Fetched 10.9 MB in 2s (6,160 kB/s)\n","Reading package lists... Done\n","\n","## Confirming \"jammy\" is supported...\n","\n","+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_16.x/dists/jammy/Release'\n","\n","## Adding the NodeSource signing key to your keyring...\n","\n","+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | tee /usr/share/keyrings/nodesource.gpg >/dev/null\n","\n","## Creating apt sources list file for the NodeSource Node.js 16.x repo...\n","\n","+ echo 'deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x jammy main' > /etc/apt/sources.list.d/nodesource.list\n","+ echo 'deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x jammy main' >> /etc/apt/sources.list.d/nodesource.list\n","\n","## Running `apt-get update` for you...\n","\n","+ apt-get update\n","Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:5 https://deb.nodesource.com/node_16.x jammy InRelease [4,583 B]\n","Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Get:12 https://deb.nodesource.com/node_16.x jammy/main amd64 Packages [776 B]\n","Fetched 116 kB in 1s (132 kB/s)\n","Reading package lists... Done\n","\n","## Run `\u001b[1msudo apt-get install -y nodejs\u001b[m` to install Node.js 16.x and npm\n","## You may also need development tools to build native addons:\n","     sudo apt-get install gcc g++ make\n","## To install the Yarn package manager, run:\n","     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/yarnkey.gpg >/dev/null\n","     echo \"deb [signed-by=/usr/share/keyrings/yarnkey.gpg] https://dl.yarnpkg.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n","     sudo apt-get update && sudo apt-get install yarn\n","\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  nodejs\n","0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n","Need to get 27.2 MB of archives.\n","After this operation, 128 MB of additional disk space will be used.\n","Get:1 https://deb.nodesource.com/node_16.x jammy/main amd64 nodejs amd64 16.20.2-deb-1nodesource1 [27.2 MB]\n","Fetched 27.2 MB in 0s (83.5 MB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package nodejs.\n","(Reading database ... 120879 files and directories currently installed.)\n","Preparing to unpack .../nodejs_16.20.2-deb-1nodesource1_amd64.deb ...\n","Unpacking nodejs (16.20.2-deb-1nodesource1) ...\n","Setting up nodejs (16.20.2-deb-1nodesource1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0\n","Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.4.1\n","Cloning into 'svd_directions'...\n","remote: Enumerating objects: 69, done.\u001b[K\n","remote: Counting objects: 100% (9/9), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 69 (delta 3), reused 0 (delta 0), pack-reused 60\u001b[K\n","Receiving objects: 100% (69/69), 8.26 MiB | 9.88 MiB/s, done.\n","Resolving deltas: 100% (33/33), done.\n","/content/svd_directions\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.34.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.14.5)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.12.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (0.14.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (9.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (0.70.15)\n","Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.8.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.43.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.16.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2023.7.22)\n","Cloning into 'PySvelte'...\n","remote: Enumerating objects: 148, done.\u001b[K\n","remote: Counting objects: 100% (22/22), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 148 (delta 15), reused 13 (delta 13), pack-reused 126\u001b[K\n","Receiving objects: 100% (148/148), 1.85 MiB | 18.80 MiB/s, done.\n","Resolving deltas: 100% (72/72), done.\n","Obtaining file:///content/svd_directions/PySvelte\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pysvelte==1.0.1) (2.0.1+cu118)\n","Collecting einops (from pysvelte==1.0.1)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pysvelte==1.0.1) (1.23.5)\n","Collecting typeguard (from pysvelte==1.0.1)\n","  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pysvelte==1.0.1) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pysvelte==1.0.1) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pysvelte==1.0.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pysvelte==1.0.1) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pysvelte==1.0.1) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pysvelte==1.0.1) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pysvelte==1.0.1) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pysvelte==1.0.1) (17.0.2)\n","Collecting typing-extensions (from torch->pysvelte==1.0.1)\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pysvelte==1.0.1) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pysvelte==1.0.1) (1.3.0)\n","Installing collected packages: typing-extensions, einops, typeguard, pysvelte\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Running setup.py develop for pysvelte\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed einops-0.7.0 pysvelte-1.0.1 typeguard-4.1.5 typing-extensions-4.8.0\n"]}],"source":["# get everything set up\n","# more rapidly install node\n","!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","# install other dependencies\n","!pip install transformers\n","!pip install datasets\n","# install repo with the data\n","!git clone https://github.com/BerenMillidge/svd_directions\n","%cd svd_directions\n","\n","!bash setup.sh\n","\n","import torch\n","from collections import Counter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import gc\n","from copy import deepcopy\n","from tqdm.auto import tqdm, trange\n","import re\n","from collections import defaultdict\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","# utils\n","import json\n","from torch import nn\n","import torch.nn.functional as F\n","from datasets import load_dataset\n","from copy import deepcopy\n","from torch.nn import functional as F\n","from tabulate import tabulate\n","from tqdm import tqdm, trange\n","import functools\n","import math\n","\n","# this resets up the site so you don't have to restart the runtime to use pysvelte\n","import site\n","site.main()\n","# import pysvelte\n","\n","\n","# sns.set_palette('colorblind')\n","# cmap = sns.color_palette('colorblind')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-Fci9FTwq9b"},"outputs":[],"source":["def keep_k(x, k=100, absolute=True, dim=-1):\n","    shape = x.shape\n","    x_ = x\n","    if absolute:\n","        x_ = abs(x)\n","    values, indices = torch.topk(x_, k=k, dim=dim)\n","    res = torch.zeros_like(x)\n","    res.scatter_(dim, indices, x.gather(dim, indices))\n","    return res\n","\n","def get_max_token_length(tokens):\n","  maxlen = 0\n","  for t in tokens:\n","    l = len(t)\n","    if l > maxlen:\n","      maxlen = l\n","  return maxlen\n","\n","def pad_with_space(t, maxlen):\n","  spaces_to_add = maxlen - len(t)\n","  for i in range(spaces_to_add):\n","    t += \" \"\n","  return t\n","\n","def convert_to_tokens(indices, tokenizer, extended, extra_values_pos, strip=True, pad_to_maxlen=False):\n","    if extended:\n","        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else\n","               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\")\n","               for idx in indices]\n","    else:\n","        res = tokenizer.convert_ids_to_tokens(indices)\n","    if strip:\n","        res = list(map(lambda x: x[1:] if x[0] == 'Ġ' else \"#\" + x, res))\n","    if pad_to_maxlen:\n","      maxlen = get_max_token_length(res)\n","      res = list(map(lambda t: pad_with_space(t, maxlen), res))\n","    return res\n","\n","\n","def top_tokens(v_tok, k=100, tokenizer=None, only_english=False, only_ascii=True, with_values=False,\n","               exclude_brackets=False, extended=True, extra_values=None, pad_to_maxlen=False):\n","    if tokenizer is None:\n","        tokenizer = my_tokenizer\n","    v_tok = deepcopy(v_tok)\n","    ignored_indices = []\n","    if only_ascii:\n","        ignored_indices = [key for val, key in tokenizer.vocab.items() if not val.strip('Ġ').isascii()]\n","    if only_english:\n","        ignored_indices =[key for val, key in tokenizer.vocab.items() if not (val.strip('Ġ').isascii() and val.strip('Ġ[]').isalnum())]\n","    if exclude_brackets:\n","        ignored_indices = set(ignored_indices).intersection(\n","            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n","        ignored_indices = list(ignored_indices)\n","    v_tok[ignored_indices] = -np.inf\n","    extra_values_pos = len(v_tok)\n","    if extra_values is not None:\n","        v_tok = torch.cat([v_tok, extra_values])\n","    values, indices = torch.topk(v_tok, k=k)\n","    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos,pad_to_maxlen = pad_to_maxlen)\n","    if with_values:\n","        res = list(zip(res, values.cpu().numpy()))\n","    return res\n","\n","\n","def top_matrix_tokens(mat, k=100, tokenizer=None, rel_thresh=None, thresh=None,\n","                      sample_entries=10000, alphabetical=True, only_english=False,\n","                      exclude_brackets=False, with_values=True, extended=True):\n","    if tokenizer is None:\n","        tokenizer = my_tokenizer\n","    mat = deepcopy(mat)\n","    ignored_indices = []\n","    if only_english:\n","        ignored_indices = [key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.strip('[]').isalnum())]\n","    if exclude_brackets:\n","        ignored_indices = set(ignored_indices).intersection(\n","            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n","        ignored_indices = list(ignored_indices)\n","    mat[ignored_indices, :] = -np.inf\n","    mat[:, ignored_indices] = -np.inf\n","    cond = torch.ones_like(mat).bool()\n","    if rel_thresh:\n","        cond &= (mat > torch.max(mat) * rel_thresh)\n","    if thresh:\n","        cond &= (mat > thresh)\n","    entries = torch.nonzero(cond)\n","    if sample_entries:\n","        entries = entries[np.random.randint(len(torch.nonzero(cond)), size=sample_entries)]\n","    res_indices = sorted(entries,\n","                         key=lambda x: x[0] if alphabetical else -mat[x[0], x[1]])\n","    res = [*map(partial(convert_to_tokens, extended=extended, tokenizer=tokenizer), res_indices)]\n","\n","    if with_values:\n","        res_ = []\n","        for (x1, x2), (i1, i2) in zip(res, res_indices):\n","            res_.append((x1, x2, mat[i1][i2].item()))\n","        res = res_\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpD59bpfxndh"},"outputs":[],"source":["def rgetattr(obj, attr, *args):\n","    def _getattr(obj, attr):\n","        return getattr(obj, attr, *args)\n","    return functools.reduce(_getattr, [obj] + attr.split('.'))\n","\n","def rsetattr(obj, attr, val):\n","    pre, _, post = attr.rpartition('.')\n","    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n","\n","def get_model_tokenizer_embedding(model_name=\"gpt2\"):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device == 'cpu':\n","    print(\"WARNING: you should probably restart on a GPU runtime\")\n","\n","  model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  emb = model.get_output_embeddings().weight.data.T.detach()\n","  return model, tokenizer, emb, device\n","\n","\n","def get_model_info(model):\n","  num_layers = model.config.n_layer\n","  num_heads = model.config.n_head\n","  hidden_dim = model.config.n_embd\n","  head_size = hidden_dim // num_heads\n","  return num_layers, num_heads, hidden_dim, head_size\n","\n","def get_mlp_weights(model,num_layers, hidden_dim):\n","  Ks = []\n","  Vs = []\n","  for j in range(num_layers):\n","    K = model.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T.detach()\n","    # fuse the layernorm\n","    ln_2_weight = model.get_parameter(f\"transformer.h.{j}.ln_2.weight\").detach()\n","    K = torch.einsum(\"oi,i -> oi\", K, ln_2_weight)\n","\n","    V = model.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n","    Ks.append(K)\n","    Vs.append(V)\n","\n","  Ks =  torch.cat(Ks)\n","  Vs = torch.cat(Vs)\n","  K_heads = Ks.reshape(num_layers, -1, hidden_dim)\n","  V_heads = Vs.reshape(num_layers, -1, hidden_dim)\n","  return K_heads, V_heads\n","\n","def get_attention_heads(model, num_layers, hidden_dim, num_heads, head_size):\n","  qkvs = []\n","  for j in range(num_layers):\n","    qkv = model.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\").detach().T\n","    ln_weight_1 = model.get_parameter(f\"transformer.h.{j}.ln_1.weight\").detach()\n","\n","    qkv = qkv - torch.mean(qkv, dim=0)\n","    qkv = torch.einsum(\"oi,i -> oi\", qkv, ln_weight_1)\n","    qkvs.append(qkv.T)\n","\n","  W_Q, W_K, W_V = torch.cat(qkvs).chunk(3, dim=-1)\n","  W_O = torch.cat([model.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\") for j in range(num_layers)]).detach()\n","  W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n","  W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)\n","  W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n","  W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n","  return W_Q_heads, W_K_heads, W_V_heads, W_O_heads\n","\n","def top_singular_vectors(mat, emb, all_tokens, k = 20, N_singular_vectors = 10, with_negative = False,use_visualization=True, filter=\"topk\"):\n","  U,S,V = torch.linalg.svd(mat)\n","  Vs = []\n","  for i in range(N_singular_vectors):\n","      acts = V[i,:].float() @ emb\n","      Vs.append(acts)\n","  if use_visualization:\n","    Vs = torch.stack(Vs, dim=1).unsqueeze(1) # n_tokens, n_layers (1), n_directions\n","    pysvelte.TopKTable(tokens=all_tokens, activations=Vs, obj_type=\"SVD direction\", k=k, filter=filter).show()\n","  else:\n","    Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n","    print(tabulate([*zip(*Vs)]))\n","  if with_negative:\n","    Vs = []\n","    for i in range(N_singular_vectors):\n","      acts = -V[i,:].float() @ emb\n","      Vs.append(acts)\n","    if use_visualization:\n","      Vs = torch.stack(Vs, dim=1).unsqueeze(1) # n_tokens, n_layers (1), n_directions\n","      pysvelte.TopKTable(tokens=all_tokens, activations=Vs, obj_type=\"SVD direction\", k=k, filter=filter).show()\n","    else:\n","      Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n","      print(tabulate([*zip(*Vs)]))\n","\n","def plot_MLP_singular_vectors(K,layer_idx, max_rank=None):\n","  W_matrix = K[layer_idx, :,:]\n","  U,S,V = torch.linalg.svd(W_matrix,full_matrices=False)\n","  if not max_rank:\n","    max_rank = len(S)\n","  if max_rank > len(S):\n","    max_rank = len(S) -1\n","  plt.plot(S[0:max_rank].detach().cpu().numpy())\n","  plt.yscale('log')\n","  plt.ylabel(\"Singular value\")\n","  plt.xlabel(\"Rank\")\n","  plt.title(\"Distribution of the singular vectors\")\n","  plt.show()\n","\n","def cosine_sim(x,y):\n","    return torch.dot(x,y) / (torch.norm(x) * torch.norm(y))\n","\n","\n","def normalize_and_entropy(V, eps=1e-6):\n","    absV = torch.abs(V)\n","    normV = absV / torch.sum(absV)\n","    entropy = torch.sum(normV * torch.log(normV + eps)).item()\n","    return -entropy\n"]},{"cell_type":"markdown","source":["## load model and get weights"],"metadata":{"id":"0rl4Hr0u8Ai1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBppCQTf57ne","executionInfo":{"status":"ok","timestamp":1696539283224,"user_tz":240,"elapsed":25332,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"361d6dad-a61a-425a-8c0d-afe48725d03e","colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["3074ab41b7d84cfbb497a4e07987cd26","2f78125cd14d4cd5a5227cbd947d237c","5efa7d90647a41a3956503a9745a3c44","57e56e927472442b8c58aa8f680c1874","9e5644510bbe4097816b1511798ab259","46463a17c01547999b6678d50c8164db","c19f37413971493fa3f560e05668253c","d475232c359a4248b52fea289ded95ae","9b551c6ddd1a44a3acfc25ad967bf640","2012c284c99846a4a9cc27755aead3b7","5062f1bb4ec54f03863eb74d2515991a","8e86bd19870f41a89984a58c19b502a7","6c5352d5b10a4edead9a11a61d3f9c14","724a6b7256e242e0a641b8d213ef0233","35ec6d6ccde1448d9bdc9e0125b6cee0","4b3930053ff242d19684c9bb5d412e75","451e976b1b314956a9a7cf6317667bcf","5b9f5b8e52df45f3a4fd0a0e558ea80c","1d40dfdc28f6487d976afa21f6dec3d5","9ac3637c980642e885c898f19de6ab20","31d779ddcc3946ca9af7373e9a58df3c","807f57b035e643b79d5926cf1b6b6105","6b77d96f54fc4300909b9aac311866b2","28580dc8307a4d70a61f52f25fcea00c","5af15ef10d944964a12d93c1ac9b16a0","d85302ac828146008dd0c5aff938cd88","0beda1ab657244c8b6b9e1ea292c4045","d16c30813abd41a385c3cf086a44b616","f67f5982cf6d4c13ba3b78c9834ee198","0ccf6b0f8d86434581d1813e5bf5d710","862b35c2305d4764b7c9e44afaade837","6c5a1a8befcf41a28441e55cdee7cd66","08aad929d0484ecbb6e806345165a1a4","5d92e745a436444c9ff95ef06c2323a1","3542f2a6387b42c8bf4518e1088fade2","fd3885abe8a54263bcacb98aef730e1a","7082275e890e4bfbb3c03375871efcd2","dc3c3f1242d84b629f2c3d2b81c201b3","16f5ba1a2c7347e1b7653e3010920a17","2f55275cf91f4dd884bdabb1880d9479","c2f118de59e646578f5b275eef0bc56e","350653e2399d4b1ca22af8591c9a4de0","9f8257a1f8594f2c94c16bf1f9cbcddc","170d5aa081db402ba9cf4a369270b00d","31a1641b766243989eede377b4f086dc","71f5a4f154c847a99f19944b3a5d379f","e3067f923e5a4f28b8a3ea4b4293807f","5a159e777ccd4768bca12ce070c9b70e","dd1e9911715046dd91c23368e126c4df","ee77cd1d70494d568c08a157405d2608","117573bd40d14b32a8afaff7c7bec0e9","e68ab67a64504e1a908cb22e6a19d0cd","c90f13a38607494a8ef90b46ed6b2c7e","29d64dac2c4644af8a50de8fe5afc7f3","e7a414d4aa6c4c4fa7407947215fb883","66eded8bbcd14c06b280c4938002894b","1645fbb195c7463e9b7f23190c2bb2e9","a6d9ff7d082a4264bfd3620d21377132","0d3f3345c92f4791820a6e891ea888f5","7bdfc4eaeecd448ea99f1b7ad5e19907","e13ababda3cd47dfbf40eda04e0bf21c","c3b4b7e3ae4d47e59c6e7e73daf7e7f2","2b3cca112aaa4f80ad14451082c5527c","8974ead801d846449b35acdbedfda6ad","8e07b3678765403da0736d6fd8ca5401","f80ff71695b04b0cb879539d62f49117"]}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3074ab41b7d84cfbb497a4e07987cd26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e86bd19870f41a89984a58c19b502a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b77d96f54fc4300909b9aac311866b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d92e745a436444c9ff95ef06c2323a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31a1641b766243989eede377b4f086dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66eded8bbcd14c06b280c4938002894b"}},"metadata":{}}],"source":["# Load up the model and get all the key weight matrices.\n","model, tokenizer, emb, device = get_model_tokenizer_embedding()\n","my_tokenizer = tokenizer\n","num_layers, num_heads, hidden_dim, head_size = get_model_info(model)\n","all_tokens = [tokenizer.decode([i]) for i in range(tokenizer.vocab_size)]\n","\n","K,V = get_mlp_weights(model, num_layers = num_layers, hidden_dim = hidden_dim)\n","W_Q_heads, W_K_heads, W_V_heads, W_O_heads = get_attention_heads(model, num_layers=num_layers, hidden_dim=hidden_dim, num_heads=num_heads, head_size = head_size)\n"]},{"cell_type":"markdown","metadata":{"id":"ekeOLatgdxhi"},"source":["# Visualizing the SVD directions"]},{"cell_type":"markdown","metadata":{"id":"ELdYODbHfBUd"},"source":["The way to read these tables is that the columns each represent a singular vector, ordered from that of the highest singular vector down to the lowest. The rows are the top-k token activations when the singular vector dimension is projected to token space, ordered by their value from top (greatest) to bottom (lowest). The colors represent the strength of the embedding.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGEAEwKAa24V"},"outputs":[],"source":["def OV_top_singular_vectors(W_V_heads, W_O_heads, emb, layer_idx, head_idx, all_tokens, k=20, N_singular_vectors=10, use_visualization=True, with_negative=False, filter=\"topk\", return_OV=False):\n","  W_V_tmp, W_O_tmp = W_V_heads[layer_idx, head_idx, :], W_O_heads[layer_idx, head_idx]\n","  OV = W_V_tmp @ W_O_tmp\n","  U,S,V = torch.linalg.svd(OV)\n","  Vs = []\n","  for i in range(N_singular_vectors):\n","      acts = V[i,:].float() @ emb\n","      Vs.append(acts)\n","  if use_visualization:\n","    Vs = torch.stack(Vs, dim=1).unsqueeze(1) # n_tokens, n_layers (1), n_directions\n","    pysvelte.TopKTable(tokens=all_tokens, activations=Vs, obj_type=\"SVD direction\", k=k, filter=filter).show()\n","  else:\n","    Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n","    print(tabulate([*zip(*Vs)]))\n","  if with_negative:\n","    Vs = []\n","    for i in range(N_singular_vectors):\n","      acts = -V[i,:].float() @ emb\n","      Vs.append(acts)\n","    if use_visualization:\n","      Vs = torch.stack(Vs, dim=1).unsqueeze(1) # n_tokens, n_layers (1), n_directions\n","      pysvelte.TopKTable(tokens=all_tokens, activations=Vs, obj_type=\"SVD direction\", k=k, filter=filter).show()\n","    else:\n","      Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n","      print(tabulate([*zip(*Vs)]))\n","  if return_OV:\n","    return OV"]}]}