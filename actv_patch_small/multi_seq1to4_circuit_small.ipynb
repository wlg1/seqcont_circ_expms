{"cells":[{"cell_type":"markdown","metadata":{"id":"b13177b7"},"source":["\u003ca href=\"https://colab.research.google.com/github/wlg100/numseqcont_circuit_expms/blob/main/nb_templates/circuit_expms_template.ipynb\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/\u003e\u003c/a\u003e\u0026nbsp;or in a local notebook."]},{"cell_type":"markdown","metadata":{"id":"fubS4n-KA3C-"},"source":["# Hypothesize and Design Expms"]},{"cell_type":"markdown","metadata":{"id":"i9sjdMMqA3C_"},"source":["\u003ch3\u003eAnswer questions based on known info to form hypotheses\u003c/h3\u003e\n","\n","- Eg) Can a 1L attn only model do this task?\n","    - Attention is really good at the primitive operations of looking nearby, or copying information. Eg) at `to` it should look for names and predict that those names came next (eg the skip trigram \" John...to -\u003e John\")\n","    - But it's much harder to tell how \u003ci\u003emany\u003c/i\u003e of each previous name there are - attending 0.3 to each copy of John will look exactly the same as attending 0.6 to a single John token\n","- Eg) How many heads to do this task then? On which tokens?\n","    - The natural place to break this symmetry is on the second \" John\" token - telling whether there is an earlier copy of the \u003ci\u003ecurrent\u003c/i\u003e token should be a much easier task.\n","    - So expect there to be a head which detects duplicate tokens on the second \" John\" token, and then another head which moves that information from the second \" John\" token to the \" to\" token.\n","\n","\u003ch3\u003eForm Hypotheses\u003c/h3\u003e\n","\n","Possible way to choose a previous token:\n","1. By MLP: Detect all preceding [token types] and move this information to \" [last token]\" and then delete any token corresponding to the duplicate token feature (vector). This feels easier done with a non-linearity, since precisely cancelling out vectors is hard, so I'd imagine an MLP layer deletes the \" John\" direction of the residual stream\n","    - if it's mostly MLPs  which attend to \" Mary\" but not to any \"John\" it's probably hypothesis 1.\n","2. By attention head: Have a head which attends to all previous token, but where the duplicate token features \u003ci\u003einhibit\u003c/i\u003e it from attending to specific names. So this only attends to Mary. And then the output of this head maps to the logits.  \n","    - if it's mostly attention heads, it's probably hypothesis 2\n","\n","\u003ch3\u003ePlan Experiment Ideas To Execute\u003c/h3\u003e\n","\n","- A test that could distinguish these two is to look at which components of the model add directly to the logits: MLPs, or heads? [See which components (head/MLP) x-points increase in Layer Attribution](https://colab.research.google.com/drive/1nLb8rq60wEoT5_QFXFoyI5rWHFbshAwJ#scrollTo=DQyELRKZIAiS\u0026line=2\u0026uniqifier=1)\n","\n","\u003cb\u003eHow to Find Head Functions\u003c/b\u003e\n","\n","Eg) To identify duplicate token heads: find heads which\n","1. Attend from \" John\" to \" John\"\n","2. Whose outputs are then moved to the \" to\" token by V-Composition with another head"]},{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup\n","(No need to change anything)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"rMcpSDdjIAiA"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running as a Colab notebook\n","Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n","  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-hyg7jnpr\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-hyg7jnpr\n","  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit e98763689463ff0fb928b0f9f48ff9e4d1a3bf01\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate\u003e=0.23.0 (from transformer-lens==0.0.0)\n","  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting beartype\u003c0.15.0,\u003e=0.14.1 (from transformer-lens==0.0.0)\n","  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\u003e=2.7.1 (from transformer-lens==0.0.0)\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops\u003e=0.6.0 (from transformer-lens==0.0.0)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fancy-einsum\u003e=0.0.3 (from transformer-lens==0.0.0)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Collecting jaxtyping\u003e=0.2.11 (from transformer-lens==0.0.0)\n","  Downloading jaxtyping-0.2.23-py3-none-any.whl (29 kB)\n","Requirement already satisfied: numpy\u003e=1.23 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.23.5)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\u001b[1m\u001b[31m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[m\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\n","  \u001b[1m\u001b[33m                         \u001b[4mSCRIPT DEPRECATION WARNING\u001b[m                    \u001b[m\n","\n","  \n","  This script, located at \u001b[1mhttps://deb.nodesource.com/setup_X\u001b[m, used to\n","  install Node.js is deprecated now and will eventually be made inactive.\n","\n","  Please visit the NodeSource \u001b[1mdistributions\u001b[m Github and follow the\n","  instructions to migrate your repo.\n","  \u001b[4m\u001b[32m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n","\n","  The \u001b[1mNodeSource\u001b[m Node.js Linux distributions GitHub repository contains\n","  information about which versions of Node.js and which Linux distributions\n","  are supported and how to install it.\n","  \u001b[4m\u001b[32m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n","\n","\n","                          \u001b[4m\u001b[1m\u001b[33mSCRIPT DEPRECATION WARNING\u001b[m\n","\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\u001b[1m\u001b[31m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[m\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\n","\u001b[36m\u001b[1mTO AVOID THIS WAIT MIGRATE THE SCRIPT\u001b[m\n","Continuing in 60 seconds (press Ctrl-C to abort) ...\n","\n"]}],"source":["# Janky code to do different setup when run in a Colab notebook vs VSCode\n","DEBUG_MODE = False\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n","    # Install another version of node that makes PySvelte work way faster\n","    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")\n","    from IPython import get_ipython\n","\n","    ipython = get_ipython()\n","    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKoTs7VBIAiD"},"outputs":[],"source":["# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n","import plotly.io as pio\n","\n","if IN_COLAB or not DEBUG_MODE:\n","    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n","    pio.renderers.default = \"colab\"\n","else:\n","    pio.renderers.default = \"png\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6b1n2tvIAiD"},"outputs":[],"source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuhzYxbsIAiE"},"outputs":[],"source":["import pysvelte\n","\n","import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFMTUcQiIAiF"},"outputs":[],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","metadata":{"id":"zyKb4C51IAiG"},"source":["Plotting helper functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFs9BrbzIAiH"},"outputs":[],"source":["def imshow(tensor, renderer=None, **kwargs):\n","    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n","\n","def line(tensor, renderer=None, **kwargs):\n","    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n","\n","def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n","    x = utils.to_numpy(x)\n","    y = utils.to_numpy(y)\n","    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"]},{"cell_type":"markdown","metadata":{"id":"OLkInsdjyHMx"},"source":["# Load Model"]},{"cell_type":"markdown","metadata":{"id":"ssJgoKr2yI8O"},"source":["Decide which model to use (eg. gpt2-small vs -medium)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLwDyosvIAiJ"},"outputs":[],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"sPm1nBFrTA5k"},"source":["# Inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7wUq-v8aIUW"},"outputs":[],"source":["prompts = [\" \".join(map(str, range(i, i+4))) for i in range(1, 21)]\n","answers = [(str(i+4), str(i+3)) for i in range(1, 21)]\n","corrupted_prompts = [\" \".join(map(str, list(range(i, i+3)) + [i+2])) for i in range(1, 21)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhJ_WdrQQIRp"},"outputs":[],"source":["answer_tokens = []\n","for answer in answers:\n","    correct_token = model.to_single_token(answer[0])\n","    incorrect_token = model.to_single_token(answer[1])\n","    answer_tokens.append((correct_token, incorrect_token))\n","answer_tokens = torch.tensor(answer_tokens).cuda()"]},{"cell_type":"markdown","metadata":{"id":"87Ox9F6ZIAiO"},"source":["# Direct Logit Attribution: Identify Impt Areas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9PpnGsMIAiN"},"outputs":[],"source":["tokens = model.to_tokens(prompts, prepend_bos=True)\n","tokens = tokens.cuda() # Move the tokens to the GPU\n","original_logits, cache = model.run_with_cache(tokens) # Run the model and cache all activations\n","\n","def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n","    # Only the final logits are relevant for the answer\n","    final_logits = logits[:, -1, :]\n","    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n","    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n","    if per_prompt:\n","        return answer_logit_diff\n","    else:\n","        return answer_logit_diff.mean()\n","\n","print(\"Per prompt logit difference:\", logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True))\n","original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)\n","print(\"Average logit difference:\", original_average_logit_diff.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lF0SLcPQIAiP"},"outputs":[],"source":["answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n","# print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n","logit_diff_directions = answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n","# print(\"Logit difference directions shape:\", logit_diff_directions.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcrN1KMkIAiP"},"outputs":[],"source":["# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n","final_residual_stream = cache[\"resid_post\", -1]\n","# print(\"Final residual stream shape:\", final_residual_stream.shape)\n","final_token_residual_stream = final_residual_stream[:, -1, :]\n","# Apply LayerNorm scaling\n","# pos_slice is the subset of the positions we take - here the final token of each prompt\n","scaled_final_token_residual_stream = cache.apply_ln_to_stack(final_token_residual_stream, layer = -1, pos_slice=-1)\n","\n","average_logit_diff = einsum(\"batch d_model, batch d_model -\u003e \", scaled_final_token_residual_stream, logit_diff_directions)/len(prompts)\n","print(\"Calculated average logit diff:\", average_logit_diff.item())\n","print(\"Original logit difference:\",original_average_logit_diff.item())"]},{"cell_type":"markdown","metadata":{"id":"wj98ZqCoA3C_"},"source":["**How to interpret**\n","\n","X = average logit difference\n","\n","This represents putting an $e^{X}$ higher probability on the correct answer."]},{"cell_type":"markdown","metadata":{"id":"BnEkxdAuIAiQ"},"source":["## Logit Lens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ax8_2nkSIAiQ"},"outputs":[],"source":["def residual_stack_to_logit_diff(residual_stack: Float[torch.Tensor, \"components batch d_model\"], cache: ActivationCache) -\u003e float:\n","    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer = -1, pos_slice=-1)\n","    return einsum(\"... batch d_model, batch d_model -\u003e ...\", scaled_residual_stack, logit_diff_directions)/len(prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYOOrypHIAiR"},"outputs":[],"source":["accumulated_residual, labels = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n","logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n","line(logit_lens_logit_diffs, x=np.arange(model.cfg.n_layers*2+1)/2, hover_name=labels, title=\"Logit Difference From Accumulate Residual Stream\")"]},{"cell_type":"markdown","metadata":{"id":"A41_6WpnIAiQ"},"source":["**How to interpret**\n","\n","Model is able to do task (first notable increase in graph) STARTING on Layer:\n","\n","Layers with very strong performance (sharp increases):\n","- L\n","\n","slightly strong performance:\n","- L\n","\n","Layers with weak performance:\n","- L\n","\n","---\n","\n","**Note:** Hover over each data point to see what residual stream position it's from!\n","\n","\u003cdetails\u003e \u003csummary\u003eDetails on `accumulated_resid`\u003c/summary\u003e\n","**Key:** `n_pre` means the residual stream at the start of layer n, `n_mid` means the residual stream after the attention part of layer n (`n_post` is the same as `n+1_pre` so is not included)\n","\n","* `layer` is the layer for which we input the residual stream (this is used to identify *which* layer norm scaling factor we want)\n","* `incl_mid` is whether to include the residual stream in the middle of a layer, ie after attention \u0026 before MLP\n","* `pos_slice` is the subset of the positions used. See `utils.Slice` for details on the syntax.\n","* return_labels is whether to return the labels for each component returned (useful for plotting)\n","\u003c/details\u003e"]},{"cell_type":"markdown","metadata":{"id":"jAa2eO4cIAiS"},"source":["## Layer Attribution"]},{"cell_type":"markdown","metadata":{"id":"QcYRawDCIAiS"},"source":["**How it works**\n","\n","(Layer L) - (Layer L-1)\n","\n","Eg) (Attn layer L) - (MLP Layer L-1)\n","\n","Eg) (MLP Layer L-1) - (Attn Layer L-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQyELRKZIAiS"},"outputs":[],"source":["per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n","per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n","line(per_layer_logit_diffs, hover_name=labels, title=\"Logit Difference From Each Layer\")"]},{"cell_type":"markdown","metadata":{"id":"gdAwEsvi-bB-"},"source":["**How to interpret**\n","\n","If there is an increase (\u003e 0), that layer at the peak matters. Layers with sharp increases:\n","- L\n","\n","If attn layer matters, information is moved around. Eg) the correct name from the input is moved up\n","\n","---\n","\u003cdetails\u003e \u003csummary\u003e To do later \u003c/summary\u003e\n","\n","[ B/c finding 'impt layers' is algorithmic, we can turn this into code.]\n","\n","[But plot is good to see holistic trends, instead of just single impt pts]\n","\u003c/details\u003e"]},{"cell_type":"markdown","metadata":{"id":"Q2ArztAMIAiT"},"source":["## Head Attribution"]},{"cell_type":"markdown","metadata":{"id":"3elRHc-D_QKW"},"source":["**How it works**\n","\n","\u003cdetails\u003e \u003csummary\u003eDecomposing attention output into sums of heads\u003c/summary\u003e\n","\n","The standard way to compute the output of an attention layer is by concatenating the mixed values of each head, and multiplying by a big output weight matrix. But as described in [A Mathematical Framework] this is equivalent to splitting the output weight matrix into a per-head output (here `model.blocks[k].attn.W_O`) and adding them up (including an overall bias term for the entire layer)\n","\u003c/details\u003e"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J48zwMMQIAiT"},"outputs":[],"source":["per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n","per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n","per_head_logit_diffs = einops.rearrange(per_head_logit_diffs, \"(layer head_index) -\u003e layer head_index\", layer=model.cfg.n_layers, head_index=model.cfg.n_heads)\n","imshow(per_head_logit_diffs, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Logit Difference From Each Head\")"]},{"cell_type":"markdown","metadata":{"id":"okhq4gxpIAiT"},"source":["**How to interpret**\n","\n","Heads that contribute a lot positively:\n","- L H\n","\n","Are these heads on same layer as impt pos layers found before, such as in the Layer attr plot?\n","\n","Heads that contribute a lot negatively:\n","- L H\n","\n","Eg) These could be negative name movers"]},{"cell_type":"markdown","metadata":{"id":"I-29LypgBGBJ"},"source":["### Print top heads (by DLA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vc2WRBT08-no"},"outputs":[],"source":["def get_top_indices(tensor, num_top_values):\n","    # Flatten the 2D tensor\n","    flattened_tensor = tensor.view(-1)\n","\n","    # Get the indices and values of the top elements\n","    top_indices = torch.topk(flattened_tensor, num_top_values)[1]\n","    top_values = torch.topk(flattened_tensor, num_top_values)[0]\n","\n","    # Convert the flattened index back to row and column indices\n","    rows = top_indices // tensor.size(1)\n","    cols = top_indices % tensor.size(1)\n","\n","    for i in range(num_top_values):\n","        row = rows[i].item()\n","        col = cols[i].item()\n","        value = top_values[i].item()\n","        print(f\"Top value {i+1}: Layer={row}, Head={col}, Value={value}\")\n","\n","    # Create a list of tuples with row and column indices\n","    top_indices_tuple = [(row.item(), col.item()) for row, col in zip(rows, cols)]\n","    return top_indices_tuple\n","\n","top_indices = get_top_indices(per_head_logit_diffs, 10)"]},{"cell_type":"markdown","metadata":{"id":"hqs8vXuRIAiT"},"source":["## Attention Analysis"]},{"cell_type":"markdown","metadata":{"id":"nD-3TTQ0IAiU"},"source":["**How it works**\n","\n","MAIN AIM: Head H moves info from which tokens tgt to which tokens src?\n","\n","(sounds \"flipped\" b/c src ATTENDS to tgt, means info from tgt goes to src via head H)\n","\n","We visualize the top 3 positive and negative heads by direct logit attribution, and show these for the first prompt.\n","\n","---\n","\u003cdetails\u003e \u003csummary\u003eToken position, not content?\u003c/summary\u003e\n","\n","Eg) the period at the end of a sentence may contain summary information for that sentence, and the head may solely move that, rather than caring about whether it ends in \".\", \"!\" or \"?\"\n","\n","Thus, attention patterns only say it's moving information FROM that *residual stream position* corresponding to that input token (whether it's \".\", \"!\", etc doesn't matter). But if not '.', and say 'is', does the content of that last token matter?\n","\u003c/details\u003e"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aIB4vwgUIAiU"},"outputs":[],"source":["def visualize_attention_patterns(\n","    heads: Union[List[int], int, Float[torch.Tensor, \"heads\"]],\n","    local_cache: Optional[ActivationCache]=None,\n","    local_tokens: Optional[torch.Tensor]=None,\n","    title: str=\"\"):\n","    # Heads are given as a list of integers or a single integer in [0, n_layers * n_heads)\n","    if isinstance(heads, int):\n","        heads = [heads]\n","    elif isinstance(heads, list) or isinstance(heads, torch.Tensor):\n","        heads = utils.to_numpy(heads)\n","    # Cache defaults to the original activation cache\n","    if local_cache is None:\n","        local_cache = cache\n","    # Tokens defaults to the tokenization of the first prompt (including the BOS token)\n","    if local_tokens is None:\n","        # The tokens of the first prompt\n","        local_tokens = tokens[0]\n","\n","    labels = []\n","    patterns = []\n","    batch_index = 0\n","    for head in heads:\n","        layer = head // model.cfg.n_heads\n","        head_index = head % model.cfg.n_heads\n","        # Get the attention patterns for the head\n","        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n","        patterns.append(local_cache[\"attn\", layer][batch_index, head_index])\n","        labels.append(f\"L{layer}H{head_index}\")\n","    str_tokens = model.to_str_tokens(local_tokens)\n","    patterns = torch.stack(patterns, dim=-1)\n","    # Plot the attention patterns\n","    attention_vis = pysvelte.AttentionMulti(attention=patterns, tokens=str_tokens, head_labels=labels)\n","    display(HTML(f\"\u003ch3\u003e{title}\u003c/h3\u003e\"))\n","    attention_vis.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVXzmMoGIAiV"},"outputs":[],"source":["top_k = 3\n","top_positive_logit_attr_heads = torch.topk(per_head_logit_diffs.flatten(), k=top_k).indices\n","visualize_attention_patterns(top_positive_logit_attr_heads, title=f\"Top {top_k} Positive Logit Attribution Heads\")\n","top_negative_logit_attr_heads = torch.topk(-per_head_logit_diffs.flatten(), k=top_k).indices\n","visualize_attention_patterns(top_negative_logit_attr_heads, title=f\"Top {top_k} Negative Logit Attribution Heads\")"]},{"cell_type":"markdown","metadata":{"id":"itQbQOGeIAiU"},"source":["**How to interpret**:\n","\u003cdetails\u003e\u003csummary\u003eInstructions for interpreting attention visualization\u003c/summary\u003e\n","\n","- Token src attends to tgt means: \"token src copies information from which PREVIOUS tokens tgt\"\n","- Attn Pat: Which heads from token src (hovered over, click selected) attend to which token tgt (highlighted)?\n","- Checking the box means token tgt sends info to which tokens src\n","- Since we're only looking at the direct effect on the logits, which is affected by the final token, we need only look at the attention patterns from the final token.\n","- eg) if BOTH pos and neg heads at final token (src) attends to IO (tgt), they're copying the info to output next. QK says which token pos to copy token from\n","    - QK says where to move information from, then OV says what values are moved. Copy score ignores QK b/c doesn't care what information is moved to.\n","\u003c/details\u003e\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x_jv6okkEKkM"},"source":["L H :\n","- dst token Y attends to src X"]},{"cell_type":"markdown","metadata":{"id":"xJXsNJMMIAiV"},"source":["# Activation Patching: Evidence for Information Movement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKMM1aqfBemP"},"outputs":[],"source":["corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n","corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n","corrupted_average_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n","print(\"Corrupted Average Logit Diff\", corrupted_average_logit_diff)\n","print(\"Clean Average Logit Diff\", original_average_logit_diff)"]},{"cell_type":"markdown","metadata":{"id":"QygolVNBIAiW"},"source":["## Residual Stream"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzmdOdeJIAiY"},"outputs":[],"source":["def patch_residual_component(\n","    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n","    hook,\n","    pos,\n","    clean_cache):\n","    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n","    return corrupted_residual_component\n","\n","def normalize_patched_logit_diff(patched_logit_diff):\n","    # Subtract corrupted logit diff to measure the improvement, divide by the total improvement from clean to corrupted to normalise\n","    # 0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, \u003e1 means actively *improved* on clean performance\n","    return (patched_logit_diff - corrupted_average_logit_diff)/(original_average_logit_diff - corrupted_average_logit_diff)\n","\n","patched_residual_stream_diff = torch.zeros(model.cfg.n_layers, tokens.shape[1], device=\"cuda\", dtype=torch.float32)\n","for layer in range(model.cfg.n_layers):\n","    for position in range(tokens.shape[1]):\n","        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n","        patched_logits = model.run_with_hooks(\n","            corrupted_tokens,\n","            fwd_hooks = [(utils.get_act_name(\"resid_pre\", layer),\n","                hook_fn)],\n","            return_type=\"logits\"\n","        )\n","        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n","\n","        patched_residual_stream_diff[layer, position] = normalize_patched_logit_diff(patched_logit_diff)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sl1MuOcIAiY"},"outputs":[],"source":["prompt_position_labels = [f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(tokens[0]))]\n","imshow(patched_residual_stream_diff, x=prompt_position_labels, title=\"Logit Difference From Patched Residual Stream\", labels={\"x\":\"Position\", \"y\":\"Layer\"})"]},{"cell_type":"markdown","metadata":{"id":"xe4wkWpkIAiY"},"source":["**How to interpret**\n","\n","Look at which tokens on x-axis the colors initially start from (from top of y-axis down). This is relevant computation, as restoring its activation outputs restores the correct answer.\n","\n","Which layers materr on which tokens?\n","- L on tokens:\n","    - T\n","\n","Then observe which layers this computation occurs at another token. This indicates that at these layers, information is moved to this token.\n","\n","Jumps suggest evidence for information movement:\n","- From L on T() to L on T()\n","\n","---\n","\u003cdetails\u003e\u003csummary\u003e Other notes \u003c/summary\u003e\n","\n","(In an abuse of notation, note that the difference here is averaged over *all* 8 prompts, while the labels only come from the *first* prompt.)\n","\n","To be easier to interpret, we normalise the logit difference, by subtracting the corrupted logit difference, and dividing by the total improvement from clean to corrupted to normalise\n","0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, \u003e1 means actively *improved* on clean performance\n","\u003c/details\u003e"]},{"cell_type":"markdown","metadata":{"id":"Drmr_Z5UIAiZ"},"source":["## Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elDd6jPqIAiZ"},"outputs":[],"source":["patched_attn_diff = torch.zeros(model.cfg.n_layers, tokens.shape[1], device=\"cuda\", dtype=torch.float32)\n","patched_mlp_diff = torch.zeros(model.cfg.n_layers, tokens.shape[1], device=\"cuda\", dtype=torch.float32)\n","for layer in range(model.cfg.n_layers):\n","    for position in range(tokens.shape[1]):\n","        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n","        patched_attn_logits = model.run_with_hooks(\n","            corrupted_tokens,\n","            fwd_hooks = [(utils.get_act_name(\"attn_out\", layer),\n","                hook_fn)],\n","            return_type=\"logits\"\n","        )\n","        patched_attn_logit_diff = logits_to_ave_logit_diff(patched_attn_logits, answer_tokens)\n","        patched_mlp_logits = model.run_with_hooks(\n","            corrupted_tokens,\n","            fwd_hooks = [(utils.get_act_name(\"mlp_out\", layer),\n","                hook_fn)],\n","            return_type=\"logits\"\n","        )\n","        patched_mlp_logit_diff = logits_to_ave_logit_diff(patched_mlp_logits, answer_tokens)\n","\n","        patched_attn_diff[layer, position] = normalize_patched_logit_diff(patched_attn_logit_diff)\n","        patched_mlp_diff[layer, position] = normalize_patched_logit_diff(patched_mlp_logit_diff)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNhyDx1XIAia"},"outputs":[],"source":["imshow(patched_attn_diff, x=prompt_position_labels, title=\"Logit Difference From Patched Attention Layer\", labels={\"x\":\"Position\", \"y\":\"Layer\"})"]},{"cell_type":"markdown","metadata":{"id":"_1f80YL2IAia"},"source":["**How to interpret**\n","\n","Which (attn heads) layers matter on which tokens?\n","- LH _ on tokens:\n","    - T\n","\n","---\n","\n","If negative (red) right after blue, those layers likely only matter for direct logit effects\n","\n","Deep blue right after the \"jump\": layers have heads that move info about (prev jump) token to (curr pos they're on) token."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTINqrXSIAib"},"outputs":[],"source":["imshow(patched_mlp_diff, x=prompt_position_labels, title=\"Logit Difference From Patched MLP Layer\", labels={\"x\":\"Position\", \"y\":\"Layer\"})"]},{"cell_type":"markdown","metadata":{"id":"vFMdeHOqIAib"},"source":["**How to interpret**\n","\n","Which MLP layers matter on which tokens?\n","- LM _ on\n","    - T\n","\n","Gray layers do not matter much.\n","\n","---\n","**How it works**\n","\n","\u003cdetails\u003e\u003csummary\u003eMLP 0 \u003c/summary\u003e\n","MLP 0 is usually blue for all cases on GPT-2 Small, so disregard it (perhaps b/c it's an extension of the embedding so when later layers want to access the input tokens they mostly read in the output of the first MLP layer, rather than the token embeddings. Within this frame, the first attention layer doesn't do much either)\n","\n","The token(s) that MLP0 matters on are token positions that later layers want access to.\n","\u003c/details\u003e"]},{"cell_type":"markdown","metadata":{"id":"XmBCzNlkIAib"},"source":["## Heads\n","\n","**How it works**\n","\n","Before, we found jumps across layers. Now, we identify which heads in those mid-layers are responsible for this.\n","\n","To go from three dimensions (head_index, position and layer) to two, patch in a head's output across all positions.\n","\n","The easiest way to do this is to patch in the activation `z`, the \"mixed value\" (QKV) of the attention head. That is, the average of all previous values weighted by the attention pattern, ie the activation that has yet to be multiplied by `W_O`, the output weights.\n","\n","NOTE: though we are interested in info from token pos X to pos Y, these are for all positions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yUslGh9oIAib"},"outputs":[],"source":["def patch_head_vector(\n","    corrupted_head_vector: Float[torch.Tensor, \"batch pos head_index d_head\"],\n","    hook,\n","    head_index,\n","    clean_cache):\n","    corrupted_head_vector[:, :, head_index, :] = clean_cache[hook.name][:, :, head_index, :]\n","    return corrupted_head_vector\n","\n","\n","patched_head_z_diff = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, device=\"cuda\", dtype=torch.float32)\n","for layer in range(model.cfg.n_layers):\n","    for head_index in range(model.cfg.n_heads):\n","        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)\n","        patched_logits = model.run_with_hooks(\n","            corrupted_tokens,\n","            fwd_hooks = [(utils.get_act_name(\"z\", layer, \"attn\"),\n","                hook_fn)],\n","            return_type=\"logits\"\n","        )\n","        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n","\n","        patched_head_z_diff[layer, head_index] = normalize_patched_logit_diff(patched_logit_diff)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojXVrsQqIAic"},"outputs":[],"source":["imshow(patched_head_z_diff, title=\"Logit Difference From Patched Head Output\", labels={\"x\":\"Head\", \"y\":\"Layer\"})"]},{"cell_type":"markdown","metadata":{"id":"JPysuMQUIAic"},"source":["**How to interpret**\n","\n","Strong pos heads:\n","- L H\n","\n","Strong neg heads:\n","- L H\n","\n","Weak pos heads:\n","- L H\n","\n","Weak neg heads:\n","- L H\n","\n","\n","See which ones above also are induction heads using\n","https://www.neelnanda.io/mosaic :\n","- L H\n","\n","weak induction:\n","- L H\n","\n","---\n","Then hypothesize how they interact:\n","\n","Eg) Early heads: reinforce findings about how these detect X-type (eg. duplicated) tokens, which are then moved by mid-later heads. To determine this:\n","- In addition to the X-type heads identified before (from direct logit attribution), look at blue heads in mid-late \"jump\" layers\n"]},{"cell_type":"markdown","metadata":{"id":"3CswaiqXABuE"},"source":["### Print top heads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9i02FBI_7ZE"},"outputs":[],"source":["top_indices = get_top_indices(patched_head_z_diff, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKFgtjkvAbh8"},"outputs":[],"source":["top_indices  # copy this into \"headFNs_expms_template.ipynb\" (Section: Get important heads)"]},{"cell_type":"markdown","metadata":{"id":"pP2k9NzekXcX"},"source":["### Print top heads found by abs()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQs0mZfPkdUf"},"outputs":[],"source":["def get_top_indices(tensor, num_top_values):\n","    # Flatten the 2D tensor\n","    flattened_tensor = tensor.view(-1)\n","    abs_flattened_tensor = torch.abs(flattened_tensor) # absolute tensor\n","\n","    # Get the indices and values of the top elements\n","    top_indices = torch.topk(abs_flattened_tensor, num_top_values)[1]\n","    top_values = flattened_tensor[top_indices] # get original top values\n","\n","    # Convert the flattened index back to row and column indices\n","    rows = top_indices // tensor.size(1)\n","    cols = top_indices % tensor.size(1)\n","\n","    for i in range(num_top_values):\n","        row = rows[i].item()\n","        col = cols[i].item()\n","        value = top_values[i].item() # get the original value\n","        print(f\"Top value {i+1}: Layer={row}, Head={col}, Value={value}\")\n","\n","    # Create a list of tuples with row and column indices\n","    top_indices_tuple = [(row.item(), col.item()) for row, col, value in zip(rows, cols, top_values)]\n","    return top_indices_tuple\n","\n","top_indices = get_top_indices(patched_head_z_diff, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mzw198CkM4N"},"outputs":[],"source":["top_indices"]},{"cell_type":"markdown","metadata":{"id":"JAbsTRepIAic"},"source":["## Decomposing Heads"]},{"cell_type":"markdown","metadata":{"id":"srBIBNLSIAic"},"source":["**How it works**\n","\n","We can disentangle if QK (*where* to move information) or OV (*what* to move) is more important by patching in just the attention pattern *or* the value vectors. (https://www.youtube.com/watch?v=KV5gbOmHbjU) for more on this decomposition.\n","\n","First let's patch in the value vectors, to measure when figuring out what to move is important. This has the same shape as z ([batch, pos, head_index, d_head]) so we can reuse the same hook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxCzX92vIAid"},"outputs":[],"source":["patched_head_v_diff = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, device=\"cuda\", dtype=torch.float32)\n","for layer in range(model.cfg.n_layers):\n","    for head_index in range(model.cfg.n_heads):\n","        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)\n","        patched_logits = model.run_with_hooks(\n","            corrupted_tokens,\n","            fwd_hooks = [(utils.get_act_name(\"v\", layer, \"attn\"),\n","                hook_fn)],\n","            return_type=\"logits\"\n","        )\n","        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n","\n","        patched_head_v_diff[layer, head_index] = normalize_patched_logit_diff(patched_logit_diff)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBV0aU6ZIAid"},"outputs":[],"source":["imshow(patched_head_v_diff, title=\"Logit Difference From Patched Head Value\", labels={\"x\":\"Head\", \"y\":\"Layer\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAIYDqr-IAid"},"outputs":[],"source":["head_labels = [f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)]\n","scatter(\n","    x=utils.to_numpy(patched_head_v_diff.flatten()),\n","    y=utils.to_numpy(patched_head_z_diff.flatten()),\n","    xaxis=\"Value Patch\",\n","    yaxis=\"Output Patch\",\n","    caxis=\"Layer\",\n","    hover_name = head_labels,\n","    color=einops.repeat(np.arange(model.cfg.n_layers), \"layer -\u003e (layer head)\", head=model.cfg.n_heads),\n","    range_x=(-0.5, 0.5),\n","    range_y=(-0.5, 0.5),\n","    title=\"Scatter plot of output patching vs value patching\")"]},{"cell_type":"markdown","metadata":{"id":"2jmOjmxQIAid"},"source":["**How to interpret**\n","\n","Scatter plot against patching head outputs: heads that are not in vertical line matter more.\n","\n","Patching in value vectors indicates these heads..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MxckLYEIAie"},"outputs":[],"source":["def patch_head_pattern(\n","    corrupted_head_pattern: Float[torch.Tensor, \"batch head_index query_pos d_head\"],\n","    hook,\n","    head_index,\n","    clean_cache):\n","    corrupted_head_pattern[:, head_index, :, :] = clean_cache[hook.name][:, head_index, :, :]\n","    return corrupted_head_pattern\n","\n","patched_head_attn_diff = torch.zeros(model.cfg.n_layers, model.cfg.n_heads, device=\"cuda\", dtype=torch.float32)\n","for layer in range(model.cfg.n_layers):\n","    for head_index in range(model.cfg.n_heads):\n","        hook_fn = partial(patch_head_pattern, head_index=head_index, clean_cache=cache)\n","        patched_logits = model.run_with_hooks(\n","            corrupted_tokens,\n","            fwd_hooks = [(utils.get_act_name(\"attn\", layer, \"attn\"),\n","                hook_fn)],\n","            return_type=\"logits\"\n","        )\n","        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n","\n","        patched_head_attn_diff[layer, head_index] = normalize_patched_logit_diff(patched_logit_diff)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGwmzMGbIAie"},"outputs":[],"source":["imshow(patched_head_attn_diff, title=\"Logit Difference From Patched Head Pattern\", labels={\"x\":\"Head\", \"y\":\"Layer\"})\n","head_labels = [f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)]\n","scatter(\n","    x=utils.to_numpy(patched_head_attn_diff.flatten()),\n","    y=utils.to_numpy(patched_head_z_diff.flatten()),\n","    hover_name = head_labels,\n","    xaxis=\"Attention Patch\",\n","    yaxis=\"Output Patch\",\n","    title=\"Scatter plot of output patching vs attention patching\")"]},{"cell_type":"markdown","metadata":{"id":"7e8EX8HxIAie"},"source":["**How to interpret**\n","\n","Patching in attention patterns indicates these heads..."]},{"cell_type":"markdown","metadata":{"id":"DIsX0K_8IAie"},"source":["# Consolidating Evidence for Head Functions into a Circuit"]},{"cell_type":"markdown","metadata":{"id":"z1ccdKFao2YJ"},"source":["**How it works**\n","\n","This takes how heads move info (QK, jumps), and what info they move (OV) and combines them with hypotheses about the task algorithms to determine possible, likely circuit layouts.\n","\n","This doesn't use info about MLPs, so that can be incorporated later into a detailed hypothesized circuit."]},{"cell_type":"markdown","metadata":{"id":"1p2zyJ_6IAif"},"source":["## Visualizing Attention Patterns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3FvuXyJv8w5"},"outputs":[],"source":["top_heads_by_output_patch = torch.topk(patched_head_z_diff.abs().flatten(), k=top_k).indices\n","top_heads_by_output_patch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjXea78dIAif"},"outputs":[],"source":["top_k = 10\n","top_heads_by_output_patch = torch.topk(patched_head_z_diff.abs().flatten(), k=top_k).indices\n","first_mid_layer = 7\n","first_late_layer = 9\n","early_heads = top_heads_by_output_patch[top_heads_by_output_patch\u003cmodel.cfg.n_heads * first_mid_layer]\n","mid_heads = top_heads_by_output_patch[torch.logical_and(model.cfg.n_heads * first_mid_layer\u003c=top_heads_by_output_patch, top_heads_by_output_patch\u003cmodel.cfg.n_heads * first_late_layer)]\n","late_heads = top_heads_by_output_patch[model.cfg.n_heads * first_late_layer\u003c=top_heads_by_output_patch]\n","visualize_attention_patterns(early_heads, title=f\"Top Early Heads\")\n","visualize_attention_patterns(mid_heads, title=f\"Top Middle Heads\")\n","visualize_attention_patterns(late_heads, title=f\"Top Late Heads\")"]},{"cell_type":"markdown","metadata":{"id":"gk2pakl7mtaA"},"source":["**How to interpret**\n","\n","Categorize heads using:\n","1. layer location\n","2. from and to which tok type\n","3. behavior determined by V or QK?\n","\n","Eg) early heads from second subject (to prev subjs / words next to them) det by V means: detect second subj is repeated token, and *which* is (eg. John)\n","    - Guess these are 'duplicate detection' heads\n","\n","Eg) middle heads from final to 2nd subj token det by QK means: compose with early head outputs copy the information about the duplicate token to final token to *inhibit* the attention paid from late (name mover) heads to the first copy of the duplicated token\n","\n","Eg) late heads from final to prev subj tokens det by QK means: compose with middle head outputs, and then attend to the correct IO name and copy that directly to the logits, using the S-Inhibition heads to inhibit attention to the first copy of the subject token.\n"]},{"cell_type":"markdown","metadata":{"id":"rkhXMj55LyAo"},"source":["## Auto guess head type and connections"]},{"cell_type":"markdown","metadata":{"id":"PtqYBOS4L9Az"},"source":["top attention heads tokens attend to what? measure \"semantically similar\" type by dot product cosine, and get value. Then input this into \"circuit type\" and run through mimimal model ablation.\n","\n","To do:\n","Output the tokens for each head"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKhNNq-cLyXw"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wHQEQybaIAif"},"source":["## Create a Circuit Diagram\n","\n","Use Visio, etc. to make a hypothesized circuit based on:  \n","1. How heads move info to layers (via jumps, attn pat)\n","2. What info is moved (attn pat, OV)\n","    - Eg) duplicate heads should:\n","        - 1. Attend from 2nd subj to 1st subj\n","        - 2. outputs are then moved to the last token by V-Composition with another head\n","3. Guesses how heads work with each other, seeing if this aligns with hypotheses about possible algorithms\n","    - Eg) if early heads are 'duplicates', what (mid/late) heads move their output info? What heads alter this info? This coincides with hypothesis that attn heads move and inhibit tokens\n","\n","![](https://pbs.twimg.com/media/FghGkTAWAAAmkhm.jpg)"]},{"cell_type":"markdown","metadata":{"id":"KAECKWO8usT5"},"source":["**Summary Example**\n","\n","\n","* Early: The duplicate token heads, previous token heads and induction heads. These serve the purpose of detecting that the second subject is duplicated and which earlier name is the duplicate.\n","    * We found a direct duplicate token head which behaves exactly as expected, L3H0. Heads L5H0 and L6H9 are induction heads, which explains why they don't attend directly to the earlier copy of John!\n","    * Note that the duplicate token heads and induction heads do not compose with each other - both directly add to the S-Inhibition heads. The diagram is somewhat misleading.\n","* Middle: They call these S-Inhibition heads - they copy the information about the duplicate token from the second subject to the to token, and their output is used to *inhibit* the attention paid from the name movers to the first subject copy. We found all these heads, and had a decent guess for what they did.\n","    * In either case they attend to the second subject, so the patch that mattered was their value vectors!\n","* Late: They call these name movers, and we found some of them. They attend from the final token to the indirect object name and copy that to the logits, using the S-Inhibition heads to inhibit attention to the first copy of the subject token.\n","    * We did find their surprising result of *negative* name movers - name movers that inhibit the correct answer!\n","    * They have an entire category of heads we missed called backup name movers - we'll get to these later."]},{"cell_type":"markdown","metadata":{"id":"yXPXpwGfokXR"},"source":["# More Questions About Head Functions"]},{"cell_type":"markdown","metadata":{"id":"967hC-FYuyG0"},"source":["Aside from the main heads involved in our hypothesized algorithm, we may notice other heads involved. Analyze them after constructing a possible \"main\" circuit. Examples include:"]},{"cell_type":"markdown","metadata":{"id":"YyMikO9nIAif"},"source":["## Why these Early Heads are Induction Heads?\n","\n","**How it works**\n","\n","Some of the early heads detecting duplicated tokens are induction heads, not just direct duplicate token heads.\n","\n","An induction head can detect and continue repeated sequences. It is the second head in a two head induction circuit, which looks for previous copies of the current token (A) and attends to the token *after* it (B), and then copies that (B) to the current position and predicts that it will come next. [See paper](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).\n","\n","![](https://pbs.twimg.com/media/FNWAzXjVEAEOGRe.jpg)\n","\n","This is surprising because it feels like overkill. The model doesn't care about *what* token comes after the first copy of the subject, just that it's duplicated. And it already has simpler duplicate token heads.\n","\n","Best guess: model just already had induction heads around and that, in addition to their main function, they *also* only activate on duplicated tokens. So it was useful to repurpose this existing machinery.\n","\n","This suggests that in larger models, components in simpler circuits (possibly existed in smaller models) get repurposed and built upon into more complicated circuits."]},{"cell_type":"markdown","metadata":{"id":"5Koj4WPJIAig"},"source":["We can verify that these are induction heads by running the model on repeated text and plotting the heads.\n","\n","---\n","\n","\u003cdetails\u003e \u003csummary\u003eCaveats\u003c/summary\u003e\n","\n","Note that this is a superficial study of whether something is an induction head - we totally ignore the question of whether it actually does boost the correct token or whether it composes with a single previous head and how. In particular, we sometimes get anti-induction heads which suppress the induction-y token (no clue why!), and this technique will find those too . But given the previous rigorous analysis, we can be pretty confident that this picks up on some true signal about induction heads.\n","\u003c/details\u003e\n","\n","\u003cdetails\u003e \u003csummary\u003eTechnical Implementation Details\u003c/summary\u003e\n","\n","We can do this again by using hooks, this time just to access the attention patterns rather than to intervene on them.\n","\n","Our hook function acts on the attention pattern activation. This has the name \"blocks.{layer}.{layer_type}.hook_{activation_name}\" in general, here it's \"blocks.{layer}.attn.hook_attn\". And it has shape [batch, head_index, query_pos, token_pos]. Our hook function takes in the attention pattern activation, calculates the score for the relevant type of head, and write it to an external cache.\n","\n","We add in hooks using `model.run_with_hooks(tokens, fwd_hooks=[(names_filter, hook_fn)])` to temporarily add in the hooks and run the model, getting the resulting output. Previously names_filter was the name of the activation, but here it's a boolean function mapping activation names to whether we want to hook them or not. Here it's just whether the name ends with hook_attn. hook_fn must take in the two inputs activation (the activation tensor) and hook (the HookPoint object, which contains the name of the activation and some metadata such as the current layer).\n","\n","Internally our hooks use the function `tensor.diagonal`, this takes the diagonal between two dimensions, and allows an arbitrary offset - offset by 1 to get previous tokens, seq_len to get duplicate tokens (the distance to earlier copies) and seq_len-1 to get induction heads (the distance to the token *after* earlier copies). Different offsets give a different length of output tensor, and we can now just average to get a score in [0, 1] for each head\n","\u003c/details\u003e"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YnN9t3n-IAig"},"outputs":[],"source":["example_text = \"Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components.\"\n","example_repeated_text = example_text + example_text\n","example_repeated_tokens = model.to_tokens(example_repeated_text, prepend_bos=True)\n","example_repeated_logits, example_repeated_cache = model.run_with_cache(example_repeated_tokens)\n","induction_head_labels = [81, 65]\n","visualize_attention_patterns(induction_head_labels, example_repeated_cache, example_repeated_tokens, title=\"Induction Heads\")"]},{"cell_type":"markdown","metadata":{"id":"TfCf_nGUx7sT"},"source":["**How to interpret**\n","\n","Eg) 'explain behaviors' -\u003e 'explain'\n","\n","Explain (src, A) attends to 'behaviors (tgt, B) so that it will output 'behaviors' next\n","\n","More examples: 'machine learning', 'learning models'\n","\n","(Every token pair, which overlaps with the next pair, in this phrase, as it's repeated multiple times, is detected as an induction pattern by these heads)"]},{"cell_type":"markdown","metadata":{"id":"ISVB45XtIAig"},"source":["Measure:\n","\n","- the average attention paid from the second copy of a token to the token AFTER the first copy.\n","\n","- the average attention paid from the second copy of a token to the first copy of the token (which is the attention that the induction head would pay if it were a duplicate token head)\n","\n","- the average attention paid to the previous token to find previous token heads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRzyJfH8IAig"},"outputs":[],"source":["seq_len = 100\n","batch_size = 2\n","\n","prev_token_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=\"cuda\")\n","def prev_token_hook(pattern, hook):\n","    layer = hook.layer()\n","    diagonal = pattern.diagonal(offset=1, dim1=-1, dim2=-2)\n","    # print(diagonal)\n","    # print(pattern)\n","    prev_token_scores[layer] = einops.reduce(diagonal, \"batch head_index diagonal -\u003e head_index\", \"mean\")\n","duplicate_token_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=\"cuda\")\n","def duplicate_token_hook(pattern, hook):\n","    layer = hook.layer()\n","    diagonal = pattern.diagonal(offset=seq_len, dim1=-1, dim2=-2)\n","    duplicate_token_scores[layer] = einops.reduce(diagonal, \"batch head_index diagonal -\u003e head_index\", \"mean\")\n","induction_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=\"cuda\")\n","def induction_hook(pattern, hook):\n","    layer = hook.layer()\n","    diagonal = pattern.diagonal(offset=seq_len-1, dim1=-1, dim2=-2)\n","    induction_scores[layer] = einops.reduce(diagonal, \"batch head_index diagonal -\u003e head_index\", \"mean\")\n","original_tokens = torch.randint(100, 20000, size=(batch_size, seq_len))\n","repeated_tokens = einops.repeat(original_tokens, \"batch seq_len -\u003e batch (2 seq_len)\").cuda()\n","\n","pattern_filter = lambda act_name: act_name.endswith(\"hook_attn\")\n","loss = model.run_with_hooks(repeated_tokens, return_type=\"loss\", fwd_hooks=[(pattern_filter, prev_token_hook), (pattern_filter, duplicate_token_hook), (pattern_filter, induction_hook)])\n","print(utils.get_corner(prev_token_scores))\n","print(utils.get_corner(duplicate_token_scores))\n","print(utils.get_corner(induction_scores))"]},{"cell_type":"markdown","metadata":{"id":"RcYhIlMmIAih"},"source":["**How to interpret**\n","\n","Check if the relevant early heads are induction heads (and?)/or duplicate token heads by seeing which heatmap they belong to:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p29rNmWhIAih"},"outputs":[],"source":["imshow(prev_token_scores, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Previous Token Scores\")\n","imshow(duplicate_token_scores, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Duplicate Token Scores\")\n","imshow(induction_scores, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Induction Head Scores\")"]},{"cell_type":"markdown","metadata":{"id":"QNeuTSquIAih"},"source":["## Backup Heads"]},{"cell_type":"markdown","metadata":{"id":"_X9J-mHCIAih"},"source":["**How it works**\n","\n","If we knock out one of the X-type heads, then there are some backup heads in later layers that *change their behaviour* and do (some of) the job of the original head's job.\n","\n","\n","---\n","\n","\n","\u003cdetails\u003e\u003csummary\u003eAblation issues\u003c/summary\u003e\n","\n","If the model is robust to ablating a head to zero after running, then naively we can be confident that the head is not doing anything important. Issues include: that the average output of the head may be far from zero and so the knockout may send it far from expected activations, breaking internals on *any* task.\n","\u003c/details\u003e\n","\n","\n","\u003cdetails\u003e \u003csummary\u003eImplementation Details\u003c/summary\u003e\n","\n","To ablate: use a custom ablation hook and then cache all new activations and compared performance\n","\n","We can just define a hook on the z activation in the relevant attention layer (recall, z is the mixed values, and comes immediately before multiplying by the output weights $W_O$). z has a head_index axis, so we can set the component for the relevant head and for position -1 to zero, and return it. (Technically we could just edit in place without returning it, but by convention we always return an edited activation).\n","\n","We now want to compare all internal activations with a hook, which is hard to do with the nice `run_with_hooks` API. So we can directly access the hook on the z activation with `model.blocks[layer].attn.hook_z` and call its `add_hook` method. This adds in the hook to the *global state* of the model. We can now use run_with_cache, and don't need to care about the global state, because run_with_cache internally adds a bunch of caching hooks, and then removes all hooks after the run, *including* the previously added ablation hook. This can be disabled with the reset_hooks_end flag, but here it's useful!\n","\u003c/details\u003e\n"]},{"cell_type":"markdown","metadata":{"id":"5yWt3qYEIAih"},"source":["**How to interpret**\n","\n","Ablate the most important X-type head on just the final token (final position to ablate the direct logit effect)\n","\n","Does removing the top X-type reduce the logit diff a lot? If it only goes down a little, that suggests there are backup X-type heads.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkFen0_6IAii"},"outputs":[],"source":["top_name_mover = per_head_logit_diffs.flatten().argmax().item()\n","top_name_mover_layer = top_name_mover//model.cfg.n_heads\n","top_name_mover_head = top_name_mover % model.cfg.n_heads\n","print(f\"Top Name Mover to ablate: L{top_name_mover_layer}H{top_name_mover_head}\")\n","def ablate_top_head_hook(z: Float[torch.Tensor, \"batch pos head_index d_head\"], hook):\n","    z[:, -1, top_name_mover_head, :] = 0\n","    return z\n","# Adds a hook into global model state\n","model.blocks[top_name_mover_layer].attn.hook_z.add_hook(ablate_top_head_hook)\n","# Runs the model, temporarily adds caching hooks and then removes *all* hooks after running, including the ablation hook.\n","ablated_logits, ablated_cache = model.run_with_cache(tokens)\n","print(f\"Original logit diff: {original_average_logit_diff}\")\n","print(f\"Post ablation logit diff: {logits_to_ave_logit_diff(ablated_logits, answer_tokens).item()}\")\n","print(f\"Direct Logit Attribution of top name mover head: {per_head_logit_diffs.flatten()[top_name_mover].item()}\")\n","print(f\"Naive prediction of post ablation logit diff: {original_average_logit_diff - per_head_logit_diffs.flatten()[top_name_mover].item()}\")"]},{"cell_type":"markdown","metadata":{"id":"8-oXPqaDIAii"},"source":["**How it works**\n","\n","Look at the direct logit attribution of each head to see what's going on. It's easiest to interpret if plotted as a scatter plot against the initial per head logit difference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyF1LfY_IAii"},"outputs":[],"source":["per_head_ablated_residual, labels = ablated_cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n","per_head_ablated_logit_diffs = residual_stack_to_logit_diff(per_head_ablated_residual, ablated_cache)\n","per_head_ablated_logit_diffs = per_head_ablated_logit_diffs.reshape(model.cfg.n_layers, model.cfg.n_heads)\n","imshow(per_head_ablated_logit_diffs, labels={\"x\":\"Head\", \"y\":\"Layer\"})\n","scatter(y=per_head_logit_diffs.flatten(), x=per_head_ablated_logit_diffs.flatten(), hover_name=head_labels, range_x=(-3, 3), range_y=(-3, 3), xaxis=\"Ablated\", yaxis=\"Original\", title=\"Original vs Post-Ablation Direct Logit Attribution of Heads\")"]},{"cell_type":"markdown","metadata":{"id":"RoyrHCFLm5K4"},"source":["**How to interpret**\n","\n","Points that stay on the x=y diagonal don't change, but those that deviate from it change a lot once ablate.\n","\n","(Hover to see head names and exact x and y values)\n","\n","Eg) negative name mover L10H7 decreases its negative effect a lot, adding +1 to the logit diff (ablated - original ~ 1)\n","\n","Eg) backup name mover L10H10 adjusts its effect to be more positive, adding +0.8 to the logit diff\n","\n","The ablated head should be down to zero"]},{"cell_type":"markdown","metadata":{"id":"NqGr02TPn77O"},"source":["### Why do Backup Heads change their behavior upon ablation?"]},{"cell_type":"markdown","metadata":{"id":"VyhSmHW3IAii"},"source":["One natural hypothesis is that this is because the final LayerNorm scaling has changed, which can scale up or down the final residual stream. This is slightly true, and we can see that the typical head is a bit off from the x=y line. But the average LN scaling ratio is 1.04, and this should uniformly change *all* heads by the same factor, so this can't be sufficient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZRY0YrSIAij"},"outputs":[],"source":["print(\"Average LN scaling ratio:\", (cache[\"ln_final.hook_scale\"][:, -1]/ablated_cache[\"ln_final.hook_scale\"][:, -1]).mean().item())\n","print(\"Ablation LN scale\", ablated_cache[\"ln_final.hook_scale\"][:, -1])\n","print(\"Original LN scale\", cache[\"ln_final.hook_scale\"][:, -1])"]},{"cell_type":"markdown","metadata":{"id":"cUo7-Oc0IAij"},"source":["**Exercise to the reader:** Can you finish off this analysis? What's going on here? Why are the backup name movers changing their behaviour? Why is one negative name mover becoming significantly less important?"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP5sbJGCgGUss+IPxcuWvGZ","collapsed_sections":["fubS4n-KA3C-","DcZG9rm2IAiA","HJu3r7dJTS8W","87Ox9F6ZIAiO"],"machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}