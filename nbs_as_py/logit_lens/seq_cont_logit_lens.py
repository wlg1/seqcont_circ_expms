# -*- coding: utf-8 -*-
"""seq_cont_logit_lens.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eCYOPQbfkYBWcyJqfZvHjPsprtTigjSF

# Setup
"""

import json
from transformers import AutoModelForCausalLM, AutoTokenizer
# from transformers import BloomTokenizerFast
import torch as t
from torch import nn
import torch.nn.functional as F
import numpy as np

model_name = 'gpt2'
device = 'cuda:0' if t.cuda.is_available() else 'cpu'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=t.float16).to(device)

"""# Functions"""

def actvs_to_logits(hidden_states):
    """
    outputs.hidden_states is a tuple for every layer
    each tuple member is an actvs tensor of size (batch_size, seq_len, d_model)
    loop thru tuple to get actv for each layer
    """
    layer_logits_list = []  # logits for each layer hidden state output actvs
    for i, h in enumerate(hidden_states):
        h_last_tok = h[:, -1, :]
        if i == len(hidden_states) - 1:
            ln_h_last_tok = h_last_tok
        else:
            ln_h_last_tok = model.transformer.ln_f(h_last_tok)  # apply layer norm as not in last
        logits = t.einsum('ab,cb->ac', model.lm_head.weight, ln_h_last_tok)
        layer_logits_list.append(logits)
    return layer_logits_list

def get_logits(input_text):
    token_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
    outputs = model(token_ids, output_hidden_states=True)
    logits = actvs_to_logits(outputs.hidden_states)
    logits = t.stack(logits).squeeze(-1)
    return logits

def get_decoded_indiv_toks(logits, k=10):
    """
    i is the layer (from before to last).
    layer_logits[i] are the scores for each token in vocab dim for the ith unembedded layer
    j is the top 5
    """
    output_list = []
    for i, layer in enumerate(logits):
        top_5_at_layer = []
        sorted_token_ids = F.softmax(layer_logits[i],dim=-1).argsort(descending=True)
        for j in range(5):  # loop to separate them in a list, rather than concat into one str
            top_5_at_layer.append(tokenizer.decode(sorted_token_ids[j]))
        output_list.append( top_5_at_layer )
    return output_list

"""# Test one samp each type"""

prompts = ["1 2 3 4", "one two three four", "January February March April"]
for test_text in prompts:
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)
    for i, tokouts in enumerate(tok_logit_lens):
        print(i-1, tokouts)
    print('\n')

"""# pure seq prompts"""

# def generate_prompts_list(x ,y):
#     prompts_list = []
#     for i in range(x, y):
#         prompt_dict = {
#             'corr': str(i+4),
#             'incorr': str(i+3),
#             'text': f"{i} {i+1} {i+2} {i+3}"
#         }
#         prompts_list.append(prompt_dict)
#     return prompts_list

# prompts_list = generate_prompts_list(1, 9)

# for pd in prompts_list:
#     test_text = pd['text']
#     layer_logits = get_logits(test_text)
#     tok_logit_lens = get_decoded_indiv_toks(layer_logits)
#     for i, tokouts in enumerate(tok_logit_lens):
#         print(i-1, tokouts)
#     print('\n')

"""# pure seq prompts nw"""

# def generate_prompts_list(x ,y):
#     words = [' one', ' two', ' three', ' four', ' five', ' six', ' seven', ' eight', ' nine', ' ten', ' eleven', ' twelve', ' thirteen', ' fourteen', ' fifteen', ' sixteen', ' seventeen', ' eighteen', ' nineteen', ' twenty']
#     prompts_list = []
#     for i in range(x, y):
#         prompt_dict = {
#             'S1': words[i],
#             'S2': words[i+1],
#             'S3': words[i+2],
#             'S4': words[i+3],
#             'corr': words[i+4],
#             'incorr': words[i+3],  # this is arbitrary
#             'text': f"{words[i]}{words[i+1]}{words[i+2]}{words[i+3]}"
#         }
#         prompts_list.append(prompt_dict)
#     return prompts_list

# prompts_list = generate_prompts_list(0, 8)

# for pd in prompts_list:
#     test_text = pd['text']
#     layer_logits = get_logits(test_text)
#     tok_logit_lens = get_decoded_indiv_toks(layer_logits)
#     for i, tokouts in enumerate(tok_logit_lens):
#         print(i-1, tokouts)
#     print('\n')

"""# pure seq prompts months"""

# def generate_prompts_list(x ,y):
#     words = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
#     prompts_list = []
#     for i in range(x, y):
#         prompt_dict = {
#             'S1': words[i],
#             'S2': words[i+1],
#             'S3': words[i+2],
#             'S4': words[i+3],
#             'corr': words[i+4],
#             'incorr': words[i+3],  # this is arbitrary
#             'text': f"{words[i]}{words[i+1]}{words[i+2]}{words[i+3]}"
#         }
#         prompts_list.append(prompt_dict)
#     return prompts_list

# prompts_list = generate_prompts_list(0, 8)

# for pd in prompts_list:
#     test_text = pd['text']
#     layer_logits = get_logits(test_text)
#     tok_logit_lens = get_decoded_indiv_toks(layer_logits)
#     for i, tokouts in enumerate(tok_logit_lens):
#         print(i-1, tokouts)
#     print('\n')

"""# nw- 1536 prompts, among words"""

import pickle

task = "nw"
prompts_list = []

temps = ['done', 'lost', 'names']

for i in temps:
    file_name = f'/content/{task}_prompts_{i}.pkl'
    with open(file_name, 'rb') as file:
        filelist = pickle.load(file)

    print(filelist[0]['text'])
    prompts_list += filelist [:512] #768 512

len(prompts_list)

num_words = ["one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",
                 "eleven", "twelve"]

anomolies = []
num_corr = 0
for pd in prompts_list:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)

    """
    Check if the 8th layer's predicted token is the sequence member just "one before"
    the correct next sequence member output found in the ninth layer

    Use `try` because when indexing, the output may not be a seq member of the right type!
    """
    try:
        a = num_words.index(tok_logit_lens[9][0].replace(' ', ''))
        b= num_words.index(tok_logit_lens[10][0].replace(' ', ''))
        if int(a) < int(b):
            if tok_logit_lens[10][0] == pd['corr']:
                num_corr += 1
            else:
                anomolies.append(pd)
        else:
                anomolies.append(pd)
    except:
        anomolies.append(pd)
    # for i, tokouts in enumerate(tok_logit_lens):
    #     print(i-1, tokouts)
    # print('\n')

num_corr

"""Do a quick scan of what prompts are anomolies in which 8th layer output is not just "the seq member one before" the 9th layer output."""

for pd in anomolies[:2]:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)
    for i, tokouts in enumerate(tok_logit_lens):
        print(i-1, tokouts)
    print('\n')

for pd in anomolies[-2:]:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)
    for i, tokouts in enumerate(tok_logit_lens):
        print(i-1, tokouts)
    print('\n')

"""# months 1536"""

import pickle

task = "months"
prompts_list = []

temps = ['done', 'lost', 'names']

for i in temps:
    file_name = f'/content/{task}_prompts_{i}.pkl'
    with open(file_name, 'rb') as file:
        filelist = pickle.load(file)

    print(filelist[0]['text'])
    prompts_list += filelist [:512] #768 512

len(prompts_list)

num_words = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

anomolies = []
num_corr = 0
for pd in prompts_list:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)

    """
    Check if the 8th layer's predicted token is the sequence member just "one before"
    the correct next sequence member output found in the ninth layer

    Use `try` because when indexing, the output may not be a seq member of the right type!
    """
    try:
        a = num_words.index(tok_logit_lens[9][0].replace(' ', ''))
        b = num_words.index(tok_logit_lens[10][0].replace(' ', ''))
        if int(a) < int(b):
            if tok_logit_lens[10][0] == pd['corr']:
                num_corr += 1
            else:
                anomolies.append(pd)
        else:
                anomolies.append(pd)
    except:
        anomolies.append(pd)
    # for i, tokouts in enumerate(tok_logit_lens):
    #     print(i-1, tokouts)
    # print('\n')

num_corr

"""Do a quick scan of what prompts are anomolies in which 8th layer output is not just "the seq member one before" the 9th layer output."""

for pd in anomolies[:2]:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)
    for i, tokouts in enumerate(tok_logit_lens):
        print(i-1, tokouts)
    print('\n')

for pd in anomolies[-2:]:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)
    for i, tokouts in enumerate(tok_logit_lens):
        print(i-1, tokouts)
    print('\n')

"""# numerals 1536"""

task = "digits"
prompts_list = []

temps = ['done', 'lost', 'names']

for i in temps:
    file_name = f'/content/{task}_prompts_{i}.pkl'
    with open(file_name, 'rb') as file:
        filelist = pickle.load(file)

    print(filelist[0]['text'])
    prompts_list += filelist [:512] #768 512

len(prompts_list)

num_corr = 0
anomolies = []
for pd in prompts_list:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)

    """
    Check if the 8th layer's predicted token is the sequence member just "one before"
    the correct next sequence member output found in the ninth layer

    Use `try` because when indexing, the output may not be a seq member of the right type!
    """
    try:
        a = tok_logit_lens[9][0].replace(' ', '')
        b= tok_logit_lens[10][0].replace(' ', '')
        if int(a) < int(b):
            if tok_logit_lens[10][0] == pd['corr']:
                num_corr += 1
            else:
                anomolies.append(pd)
        else:
                anomolies.append(pd)
    except:
        anomolies.append(pd)
    # for i, tokouts in enumerate(tok_logit_lens):
    #     print(i-1, tokouts)
    # print('\n')

num_corr

"""Do a quick scan of what prompts are anomolies in which 8th layer output is not just "the seq member one before" the 9th layer output."""

for pd in anomolies[:2]:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)
    for i, tokouts in enumerate(tok_logit_lens):
        print(i-1, tokouts)
    print('\n')

for pd in anomolies[-2:]:
    test_text = pd['text']
    layer_logits = get_logits(test_text)
    tok_logit_lens = get_decoded_indiv_toks(layer_logits)
    for i, tokouts in enumerate(tok_logit_lens):
        print(i-1, tokouts)
    print('\n')