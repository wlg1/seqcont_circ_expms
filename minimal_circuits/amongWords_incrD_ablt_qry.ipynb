{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["DcZG9rm2IAiA","cGX9iHAz_UKX","8JDyG6bMk9qM"],"authorship_tag":"ABX9TyNWkIxqIGbnsaM/RN9YEcYL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"582b45096afb436d840a780ae454eb33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83c58e5516da476a90ed6505fcd878fb","IPY_MODEL_9bedb3b67d5745f28814dfdbb9f02938","IPY_MODEL_576f71e1a0944b6e819f5c7c50d9ce1c"],"layout":"IPY_MODEL_4952d74041bd4da091263c1d63586b10"}},"83c58e5516da476a90ed6505fcd878fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8b50bfdee394bcba8de0c742ed0bd74","placeholder":"​","style":"IPY_MODEL_6d3875ad32304bad9247bf812967ce76","value":"Downloading (…)lve/main/config.json: 100%"}},"9bedb3b67d5745f28814dfdbb9f02938":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_793657c0ec954cb59f289dc54c72a04d","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_830633c8103c4a80a36146123e5d95eb","value":665}},"576f71e1a0944b6e819f5c7c50d9ce1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b68c8fa5d04981889d2a4815809032","placeholder":"​","style":"IPY_MODEL_eed8a8a82dab43b7845c368016e76cdd","value":" 665/665 [00:00&lt;00:00, 22.7kB/s]"}},"4952d74041bd4da091263c1d63586b10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8b50bfdee394bcba8de0c742ed0bd74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d3875ad32304bad9247bf812967ce76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"793657c0ec954cb59f289dc54c72a04d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830633c8103c4a80a36146123e5d95eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64b68c8fa5d04981889d2a4815809032":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eed8a8a82dab43b7845c368016e76cdd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d9411c8f9ea470bac9ef278c0af675a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_416c86fce0b844259caebdb25ac05371","IPY_MODEL_a56009588734421abf45d2c09719d4f2","IPY_MODEL_19d513aa6ade4a48b291248add65f570"],"layout":"IPY_MODEL_15d7c44b7dc54f2fa472cf1b7ace0fef"}},"416c86fce0b844259caebdb25ac05371":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0e52509639949c7890c70da5670418a","placeholder":"​","style":"IPY_MODEL_719dc17b3340438a980dc820c8108df6","value":"Downloading model.safetensors: 100%"}},"a56009588734421abf45d2c09719d4f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c4309f2cadb46319394aa2c98e45883","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a79ee1ef4a954d04a09a20782a18e099","value":548105171}},"19d513aa6ade4a48b291248add65f570":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c129cefc81704cc49188d53a71b5f4de","placeholder":"​","style":"IPY_MODEL_d0a8b846be9c41eda2f1621d911f9cf9","value":" 548M/548M [00:06&lt;00:00, 44.0MB/s]"}},"15d7c44b7dc54f2fa472cf1b7ace0fef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0e52509639949c7890c70da5670418a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"719dc17b3340438a980dc820c8108df6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c4309f2cadb46319394aa2c98e45883":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a79ee1ef4a954d04a09a20782a18e099":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c129cefc81704cc49188d53a71b5f4de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0a8b846be9c41eda2f1621d911f9cf9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"196efdd640c04ae18f245cb36419aec1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b1431fc5b0a40a8824dab000c1611e6","IPY_MODEL_405be10add994909bc9c2a676ce7a544","IPY_MODEL_6363ca1b473047f7af7d22580d0e1ae0"],"layout":"IPY_MODEL_2afb8be04ef84a7e8b9f5c4e49119ff2"}},"3b1431fc5b0a40a8824dab000c1611e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43e1f2375e3945a7be2c063dc6499dad","placeholder":"​","style":"IPY_MODEL_013e1451f5db42f1be4f81a80f3f3c3d","value":"Downloading (…)neration_config.json: 100%"}},"405be10add994909bc9c2a676ce7a544":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c97869d966424a43ae522bd3f26937b1","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_496385e7c9ff449c8f691338d8c58946","value":124}},"6363ca1b473047f7af7d22580d0e1ae0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81757a29fa8b471cb578cdde6e543ea1","placeholder":"​","style":"IPY_MODEL_a01b86167a604b83882d5a7322f3ab62","value":" 124/124 [00:00&lt;00:00, 2.21kB/s]"}},"2afb8be04ef84a7e8b9f5c4e49119ff2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43e1f2375e3945a7be2c063dc6499dad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"013e1451f5db42f1be4f81a80f3f3c3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c97869d966424a43ae522bd3f26937b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"496385e7c9ff449c8f691338d8c58946":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81757a29fa8b471cb578cdde6e543ea1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a01b86167a604b83882d5a7322f3ab62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dac08727ec6044dbb9f2d3eb0c541073":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31183f8be9d746818b551ef78e1b43ca","IPY_MODEL_2bc6916ca5ff41a6993f9b051827d241","IPY_MODEL_422ecaf333434ee7b706b9bed342ea89"],"layout":"IPY_MODEL_977cdb0d716d450585e53c97ac30533d"}},"31183f8be9d746818b551ef78e1b43ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f57be2a6126e4703b6acdc26a190dce0","placeholder":"​","style":"IPY_MODEL_eb1edbfef52c41a7a61614ed38f61ab4","value":"Downloading (…)olve/main/vocab.json: 100%"}},"2bc6916ca5ff41a6993f9b051827d241":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cee92cac443e4109839fb338f5cd0183","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2bda1de13d1b4b6fb9f0c40079ed322e","value":1042301}},"422ecaf333434ee7b706b9bed342ea89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad94ea063e804710b98bb27073893038","placeholder":"​","style":"IPY_MODEL_38ec38e12cff459f911009bc1df5f56e","value":" 1.04M/1.04M [00:00&lt;00:00, 5.49MB/s]"}},"977cdb0d716d450585e53c97ac30533d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f57be2a6126e4703b6acdc26a190dce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb1edbfef52c41a7a61614ed38f61ab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cee92cac443e4109839fb338f5cd0183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bda1de13d1b4b6fb9f0c40079ed322e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad94ea063e804710b98bb27073893038":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38ec38e12cff459f911009bc1df5f56e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43e4415145db477183c0abbf7833eea2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1314f1d9859b44b98524046d15a6aac2","IPY_MODEL_3f48eb8eabd54f0a8e974f08c638cdd4","IPY_MODEL_ad09e621d4194fd2800ec441637ce53f"],"layout":"IPY_MODEL_44a81f3450064ab98c0a18a71599bd11"}},"1314f1d9859b44b98524046d15a6aac2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3dd68187a574dccac7005eabaf5a96b","placeholder":"​","style":"IPY_MODEL_b46b2303f1df48ca96b6d13072442bd1","value":"Downloading (…)olve/main/merges.txt: 100%"}},"3f48eb8eabd54f0a8e974f08c638cdd4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_45c2f0dab95d47599074b55c9e6cf759","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db5fcd3f5c434209aa974fc2532a6f7b","value":456318}},"ad09e621d4194fd2800ec441637ce53f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f380985a7ec45469bc9fe33cce6980a","placeholder":"​","style":"IPY_MODEL_79d58d8babbf4166974b8c34b490329f","value":" 456k/456k [00:00&lt;00:00, 17.0MB/s]"}},"44a81f3450064ab98c0a18a71599bd11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3dd68187a574dccac7005eabaf5a96b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b46b2303f1df48ca96b6d13072442bd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45c2f0dab95d47599074b55c9e6cf759":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db5fcd3f5c434209aa974fc2532a6f7b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f380985a7ec45469bc9fe33cce6980a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79d58d8babbf4166974b8c34b490329f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fecd36cec944bbaaa202c5102eb7897":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e15361e7b0a4682a0a58265e814dde1","IPY_MODEL_1e26d2befc474d9c85981824e1ef6773","IPY_MODEL_09baf577213c49b9befe7b0bd852585c"],"layout":"IPY_MODEL_4b3a7e1ae3204e1aa0b3fb21ce0b0ba6"}},"3e15361e7b0a4682a0a58265e814dde1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0705c164ec024c8883c22101bc6ad216","placeholder":"​","style":"IPY_MODEL_e80969e0c5d54d2d834e98cbd90e2242","value":"Downloading (…)/main/tokenizer.json: 100%"}},"1e26d2befc474d9c85981824e1ef6773":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd8be62e05eb43b5be0351a88c380bbe","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_575c4b111c6046cebb18c09b83980d0d","value":1355256}},"09baf577213c49b9befe7b0bd852585c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_767c40a343ca4af18a3cc95a5615df7c","placeholder":"​","style":"IPY_MODEL_d01f17078c154548be03ebcf6ba9a790","value":" 1.36M/1.36M [00:00&lt;00:00, 10.4MB/s]"}},"4b3a7e1ae3204e1aa0b3fb21ce0b0ba6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0705c164ec024c8883c22101bc6ad216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e80969e0c5d54d2d834e98cbd90e2242":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd8be62e05eb43b5be0351a88c380bbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"575c4b111c6046cebb18c09b83980d0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"767c40a343ca4af18a3cc95a5615df7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d01f17078c154548be03ebcf6ba9a790":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Only keep (not ablate) the query activations (query vector, the output of query weights times inputs) of certain positions. Keep all the key activations; the query positions that were kept will automatically attend to relevant key positions by matrix multiplication."],"metadata":{"id":"Gx3CMLFzCvz5"}},{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup\n","(No need to change anything)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"rMcpSDdjIAiA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c17e0f00-8727-4d73-8273-101027847eda","executionInfo":{"status":"ok","timestamp":1698253601073,"user_tz":240,"elapsed":190869,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running as a Colab notebook\n","Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n","  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-uznuk90i\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-uznuk90i\n","  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 174209ea708fe3838ccf08b70f2f4f28e7397cb4\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate>=0.23.0 (from transformer-lens==0.0.0)\n","  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens==0.0.0)\n","  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n","  Downloading jaxtyping-0.2.23-py3-none-any.whl (29 kB)\n","Collecting numpy>=1.24 (from transformer-lens==0.0.0)\n","  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from transformer-lens==0.0.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from transformer-lens==0.0.0)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from transformer-lens==0.0.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.6.0)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.1.0+cu118)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n","Collecting transformers>=4.25.1 (from transformer-lens==0.0.0)\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.5.0)\n","Collecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n","  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->transformer-lens==0.0.0)\n","  Downloading nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.1.0->transformer-lens==0.0.0) (3.12.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n","Collecting huggingface-hub (from accelerate>=0.23.0->transformer-lens==0.0.0)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.4.1)\n","Collecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.6)\n","Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.11->transformer-lens==0.0.0)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.3.post1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2023.6.3)\n","Collecting tokenizers<0.15,>=0.14 (from transformers>=4.25.1->transformer-lens==0.0.0)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.25.1->transformer-lens==0.0.0)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.7.22)\n","Collecting huggingface-hub (from accelerate>=0.23.0->transformer-lens==0.0.0)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens==0.0.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens==0.0.0) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: transformer-lens, pathtools\n","  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=116436 sha256=14d225e7e894b7c6626dc99b760a49395ce929db3af3adfb837b829e7ac916ef\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-f4r1q19_/wheels/8a/1e/37/ffb9c15454a1725b13a9d9f5e74fb91725048884ad734b8c1f\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=42acdd3a97fb62c5f42751d1a44e3f6c54b13e44705b0f53dcc075a7c7a0ad2e\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built transformer-lens pathtools\n","Installing collected packages: pathtools, typeguard, smmap, setproctitle, sentry-sdk, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, huggingface-hub, gitdb, tokenizers, nvidia-cusolver-cu12, GitPython, accelerate, wandb, transformers, datasets, transformer-lens\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.40 accelerate-0.24.0 beartype-0.14.1 datasets-2.14.6 dill-0.3.7 docker-pycreds-0.4.0 einops-0.7.0 fancy-einsum-0.0.3 gitdb-4.0.11 huggingface-hub-0.17.3 jaxtyping-0.2.23 multiprocess-0.70.15 numpy-1.26.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.52 nvidia-nvtx-cu12-12.1.105 pathtools-0.1.2 safetensors-0.4.0 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 tokenizers-0.14.1 transformer-lens-0.0.0 transformers-4.34.1 typeguard-2.13.3 wandb-0.15.12\n","\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\u001b[1m\u001b[31m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[m\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\n","  \u001b[1m\u001b[33m                         \u001b[4mSCRIPT DEPRECATION WARNING\u001b[m                    \u001b[m\n","\n","  \n","  This script, located at \u001b[1mhttps://deb.nodesource.com/setup_X\u001b[m, used to\n","  install Node.js is deprecated now and will eventually be made inactive.\n","\n","  Please visit the NodeSource \u001b[1mdistributions\u001b[m Github and follow the\n","  instructions to migrate your repo.\n","  \u001b[4m\u001b[32m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n","\n","  The \u001b[1mNodeSource\u001b[m Node.js Linux distributions GitHub repository contains\n","  information about which versions of Node.js and which Linux distributions\n","  are supported and how to install it.\n","  \u001b[4m\u001b[32m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n","\n","\n","                          \u001b[4m\u001b[1m\u001b[33mSCRIPT DEPRECATION WARNING\u001b[m\n","\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\u001b[1m\u001b[31m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[m\n","\u001b[1m\u001b[31m================================================================================\u001b[m\n","\n","\u001b[36m\u001b[1mTO AVOID THIS WAIT MIGRATE THE SCRIPT\u001b[m\n","Continuing in 60 seconds (press Ctrl-C to abort) ...\n","\n","\n","## Installing the NodeSource Node.js 16.x repo...\n","\n","\n","## Populating apt-get cache...\n","\n","+ apt-get update\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [556 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,274 kB]\n","Hit:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,398 kB]\n","Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,131 kB]\n","Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,009 kB]\n","Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Fetched 5,711 kB in 3s (2,256 kB/s)\n","Reading package lists... Done\n","\n","## Confirming \"jammy\" is supported...\n","\n","+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_16.x/dists/jammy/Release'\n","\n","## Adding the NodeSource signing key to your keyring...\n","\n","+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | tee /usr/share/keyrings/nodesource.gpg >/dev/null\n","\n","## Creating apt sources list file for the NodeSource Node.js 16.x repo...\n","\n","+ echo 'deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x jammy main' > /etc/apt/sources.list.d/nodesource.list\n","+ echo 'deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x jammy main' >> /etc/apt/sources.list.d/nodesource.list\n","\n","## Running `apt-get update` for you...\n","\n","+ apt-get update\n","Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:3 https://deb.nodesource.com/node_16.x jammy InRelease [4,583 B]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Get:6 https://deb.nodesource.com/node_16.x jammy/main amd64 Packages [776 B]\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Hit:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Fetched 5,359 B in 2s (2,255 B/s)\n","Reading package lists... Done\n","\n","## Run `\u001b[1msudo apt-get install -y nodejs\u001b[m` to install Node.js 16.x and npm\n","## You may also need development tools to build native addons:\n","     sudo apt-get install gcc g++ make\n","## To install the Yarn package manager, run:\n","     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/yarnkey.gpg >/dev/null\n","     echo \"deb [signed-by=/usr/share/keyrings/yarnkey.gpg] https://dl.yarnpkg.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n","     sudo apt-get update && sudo apt-get install yarn\n","\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  nodejs\n","0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\n","Need to get 27.2 MB of archives.\n","After this operation, 128 MB of additional disk space will be used.\n","Get:1 https://deb.nodesource.com/node_16.x jammy/main amd64 nodejs amd64 16.20.2-deb-1nodesource1 [27.2 MB]\n","Fetched 27.2 MB in 0s (72.7 MB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package nodejs.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../nodejs_16.20.2-deb-1nodesource1_amd64.deb ...\n","Unpacking nodejs (16.20.2-deb-1nodesource1) ...\n","Setting up nodejs (16.20.2-deb-1nodesource1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Collecting git+https://github.com/neelnanda-io/PySvelte.git\n","  Cloning https://github.com/neelnanda-io/PySvelte.git to /tmp/pip-req-build-1bep0hgs\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/PySvelte.git /tmp/pip-req-build-1bep0hgs\n","  Resolved https://github.com/neelnanda-io/PySvelte.git to commit 6f5d971a148d40fb7481d400ae74551b37340e83\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}],"source":["# Janky code to do different setup when run in a Colab notebook vs VSCode\n","DEBUG_MODE = False\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n","    # Install another version of node that makes PySvelte work way faster\n","    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")\n","    from IPython import get_ipython\n","\n","    ipython = get_ipython()\n","    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xKoTs7VBIAiD","executionInfo":{"status":"ok","timestamp":1698253601210,"user_tz":240,"elapsed":193,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n","import plotly.io as pio\n","\n","if IN_COLAB or not DEBUG_MODE:\n","    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n","    pio.renderers.default = \"colab\"\n","else:\n","    pio.renderers.default = \"png\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Z6b1n2tvIAiD","executionInfo":{"status":"ok","timestamp":1698253607972,"user_tz":240,"elapsed":6767,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zuhzYxbsIAiE","executionInfo":{"status":"ok","timestamp":1698253609793,"user_tz":240,"elapsed":1830,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["# import pysvelte\n","\n","import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"cFMTUcQiIAiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698253609793,"user_tz":240,"elapsed":5,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8a27a3a9-d3fe-49f0-9895-068b6e6994f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x79a525cffa30>"]},"metadata":{},"execution_count":5}],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","metadata":{"id":"zyKb4C51IAiG"},"source":["Plotting helper functions:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KFs9BrbzIAiH","executionInfo":{"status":"ok","timestamp":1698253609793,"user_tz":240,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["def imshow(tensor, renderer=None, **kwargs):\n","    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n","\n","def line(tensor, renderer=None, **kwargs):\n","    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n","\n","def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n","    x = utils.to_numpy(x)\n","    y = utils.to_numpy(y)\n","    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"OLkInsdjyHMx"}},{"cell_type":"markdown","source":["Decide which model to use (eg. gpt2-small vs -medium)"],"metadata":{"id":"ssJgoKr2yI8O"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"xLwDyosvIAiJ","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["582b45096afb436d840a780ae454eb33","83c58e5516da476a90ed6505fcd878fb","9bedb3b67d5745f28814dfdbb9f02938","576f71e1a0944b6e819f5c7c50d9ce1c","4952d74041bd4da091263c1d63586b10","b8b50bfdee394bcba8de0c742ed0bd74","6d3875ad32304bad9247bf812967ce76","793657c0ec954cb59f289dc54c72a04d","830633c8103c4a80a36146123e5d95eb","64b68c8fa5d04981889d2a4815809032","eed8a8a82dab43b7845c368016e76cdd","1d9411c8f9ea470bac9ef278c0af675a","416c86fce0b844259caebdb25ac05371","a56009588734421abf45d2c09719d4f2","19d513aa6ade4a48b291248add65f570","15d7c44b7dc54f2fa472cf1b7ace0fef","d0e52509639949c7890c70da5670418a","719dc17b3340438a980dc820c8108df6","0c4309f2cadb46319394aa2c98e45883","a79ee1ef4a954d04a09a20782a18e099","c129cefc81704cc49188d53a71b5f4de","d0a8b846be9c41eda2f1621d911f9cf9","196efdd640c04ae18f245cb36419aec1","3b1431fc5b0a40a8824dab000c1611e6","405be10add994909bc9c2a676ce7a544","6363ca1b473047f7af7d22580d0e1ae0","2afb8be04ef84a7e8b9f5c4e49119ff2","43e1f2375e3945a7be2c063dc6499dad","013e1451f5db42f1be4f81a80f3f3c3d","c97869d966424a43ae522bd3f26937b1","496385e7c9ff449c8f691338d8c58946","81757a29fa8b471cb578cdde6e543ea1","a01b86167a604b83882d5a7322f3ab62","dac08727ec6044dbb9f2d3eb0c541073","31183f8be9d746818b551ef78e1b43ca","2bc6916ca5ff41a6993f9b051827d241","422ecaf333434ee7b706b9bed342ea89","977cdb0d716d450585e53c97ac30533d","f57be2a6126e4703b6acdc26a190dce0","eb1edbfef52c41a7a61614ed38f61ab4","cee92cac443e4109839fb338f5cd0183","2bda1de13d1b4b6fb9f0c40079ed322e","ad94ea063e804710b98bb27073893038","38ec38e12cff459f911009bc1df5f56e","43e4415145db477183c0abbf7833eea2","1314f1d9859b44b98524046d15a6aac2","3f48eb8eabd54f0a8e974f08c638cdd4","ad09e621d4194fd2800ec441637ce53f","44a81f3450064ab98c0a18a71599bd11","f3dd68187a574dccac7005eabaf5a96b","b46b2303f1df48ca96b6d13072442bd1","45c2f0dab95d47599074b55c9e6cf759","db5fcd3f5c434209aa974fc2532a6f7b","4f380985a7ec45469bc9fe33cce6980a","79d58d8babbf4166974b8c34b490329f","6fecd36cec944bbaaa202c5102eb7897","3e15361e7b0a4682a0a58265e814dde1","1e26d2befc474d9c85981824e1ef6773","09baf577213c49b9befe7b0bd852585c","4b3a7e1ae3204e1aa0b3fb21ce0b0ba6","0705c164ec024c8883c22101bc6ad216","e80969e0c5d54d2d834e98cbd90e2242","bd8be62e05eb43b5be0351a88c380bbe","575c4b111c6046cebb18c09b83980d0d","767c40a343ca4af18a3cc95a5615df7c","d01f17078c154548be03ebcf6ba9a790"],"height":0},"executionInfo":{"status":"ok","timestamp":1698253637103,"user_tz":240,"elapsed":27314,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"747d3be2-38b3-4616-a6f9-94e78b64c92a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"582b45096afb436d840a780ae454eb33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d9411c8f9ea470bac9ef278c0af675a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196efdd640c04ae18f245cb36419aec1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dac08727ec6044dbb9f2d3eb0c541073"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43e4415145db477183c0abbf7833eea2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fecd36cec944bbaaa202c5102eb7897"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","source":["## Import functions from repo"],"metadata":{"id":"Z4iJEGh6b56v"}},{"cell_type":"code","source":["!git clone https://github.com/callummcdougall/ARENA_2.0.git"],"metadata":{"id":"Fdh5--MfYw7-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698253646361,"user_tz":240,"elapsed":9555,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e1b6255d-7208-4573-fb29-13499e0ed6ef"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ARENA_2.0'...\n","remote: Enumerating objects: 9100, done.\u001b[K\n","remote: Counting objects: 100% (1814/1814), done.\u001b[K\n","remote: Compressing objects: 100% (288/288), done.\u001b[K\n","remote: Total 9100 (delta 1609), reused 1602 (delta 1523), pack-reused 7286\u001b[K\n","Receiving objects: 100% (9100/9100), 155.60 MiB | 32.07 MiB/s, done.\n","Resolving deltas: 100% (5502/5502), done.\n"]}]},{"cell_type":"code","source":["cd ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification"],"metadata":{"id":"iZ4C_bsXZFfj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698253646362,"user_tz":240,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"60af9982-6543-44e0-cda6-81f1d883b536"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification\n"]}]},{"cell_type":"code","source":["import ioi_circuit_extraction as ioi_circuit_extraction"],"metadata":{"id":"OT0Sn571ZnkV","executionInfo":{"status":"ok","timestamp":1698253646362,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Generate dataset with multiple prompts"],"metadata":{"id":"cGX9iHAz_UKX"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.io_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S5\"])[0] for prompt in self.prompts\n","        ]\n","        self.s_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S4\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'S5')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"4wXBNWj5FwVn","executionInfo":{"status":"ok","timestamp":1698253702600,"user_tz":240,"elapsed":1901,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Repalce io_tokens with correct answer (next, which is '5') and s_tokens with incorrect (current, which repeats)"],"metadata":{"id":"exuTCQ_XmmFP"}},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'S5': str(i+4),\n","            'text': f\"Adam is {i}. Bob is {i+1}. Claire is {i+2}. Don is {i+3}. Eve is\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 11)\n","dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"u0NPSKcZ1iDe","executionInfo":{"status":"ok","timestamp":1698253702835,"user_tz":240,"elapsed":5,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'S5': str(i+3),\n","            'text': f\"Adam is {i}. Bob is {i+1}. Claire is {i+2}. Don is {i+2}. Eve is\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","dataset_2 = Dataset(prompts_list_2, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"dzzLlCqZS_wl","executionInfo":{"status":"ok","timestamp":1698253702835,"user_tz":240,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Logit diff is correct - incorr token. Here, correct is S5, and incorr is S4.\n","\n","Because of this, it's possible to have logit diffs HIGHER than the \"full circuit\" because the correct token will still be at first place, but the logit scores assigned will just be bigger (perhaps incorrect is scored even lower in the non-full circuit with a higher logit diff score)?"],"metadata":{"id":"A0W-GaM6Vfm-"}},{"cell_type":"markdown","source":["# Ablation Expm Functions"],"metadata":{"id":"GCCCoO0V7L7J"}},{"cell_type":"code","source":["from torch import Tensor\n","\n","def logits_to_ave_logit_diff_2(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","    '''\n","    Returns logit difference between the correct and incorrect answer.\n","\n","    If per_prompt=True, return the array of differences rather than the average.\n","    '''\n","\n","    # Only the final logits are relevant for the answer\n","    # Get the logits corresponding to the indirect object / subject tokens respectively\n","    io_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.io_tokenIDs]\n","    s_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.s_tokenIDs]\n","    # Find logit difference\n","    answer_logit_diff = io_logits - s_logits\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"],"metadata":{"id":"CgD41x5nbKKP","executionInfo":{"status":"ok","timestamp":1698253704188,"user_tz":240,"elapsed":199,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def mean_ablate_by_lst(CIRCUIT, SEQ_POS_TO_KEEP, model, print_output=True):\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"LqsdFmbVMntG","executionInfo":{"status":"ok","timestamp":1698238810133,"user_tz":240,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["We can also prevent redundant computation of the full circuit score by storing it and just passing it in to the function."],"metadata":{"id":"xUlhxzuGUr1y"}},{"cell_type":"markdown","source":["## test fns on pure digits dataset"],"metadata":{"id":"T_0IKaGW3mlI"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.io_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S5\"])[0] for prompt in self.prompts\n","        ]\n","        self.s_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S4\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'S5')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"NoEdOVf94ZsT","executionInfo":{"status":"ok","timestamp":1698238810133,"user_tz":240,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Repalce io_tokens with correct answer (next, which is '5') and s_tokens with incorrect (current, which repeats)"],"metadata":{"id":"1ScIcKEJ4Zsd"}},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'S5': str(i+4),\n","            'text': f\"{i} {i+1} {i+2} {i+3}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 11)\n","dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"DKoD3C6C4Zsd","executionInfo":{"status":"ok","timestamp":1698238810134,"user_tz":240,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'S5': str(i+3),\n","            'text': f\"{i} {i+1} {i+2} {i+2}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","dataset_2 = Dataset(prompts_list_2, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"GlNCalHl4Zsd","executionInfo":{"status":"ok","timestamp":1698238810134,"user_tz":240,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","logits_to_ave_logit_diff_2(ioi_logits_original, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6zzi4BX3lnq","executionInfo":{"status":"ok","timestamp":1698238813143,"user_tz":240,"elapsed":3019,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a53f6385-13e0-44ec-f8c2-4ba7ec63c93a"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4.6238, device='cuda:0')"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["fullcirc = [(layer, head) for layer in range(12) for head in range(12)]\n","CIRCUIT = {\n","    \"number mover\": fullcirc,\n","    \"number mover 4\": fullcirc,\n","    \"number mover 3\": fullcirc,\n","    \"number mover 2\": fullcirc,\n","    \"number mover 1\": fullcirc,\n","}\n","\n","SEQ_POS_TO_KEEP = {\n","    \"number mover\": \"end\",\n","    \"number mover 4\": \"S4\",\n","    \"number mover 3\": \"S3\",\n","    \"number mover 2\": \"S2\",\n","    \"number mover 1\": \"S1\",\n","}"],"metadata":{"id":"gxM9IQrD324c","executionInfo":{"status":"ok","timestamp":1698238813143,"user_tz":240,"elapsed":4,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["model_abl = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","ioi_logits_minimal = model_abl(dataset.toks)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698238815441,"user_tz":240,"elapsed":2301,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"4sVHT-5x32S-"},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698238815441,"user_tz":240,"elapsed":6,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1b2dd542-7852-40de-8f22-66431eeae785","id":"OrU0k9gf32S_"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4.6238, device='cuda:0')"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["This works, so if any issues with mean ablate 'no heads' is not same score as 'original' for 'among words', the issue is not with the code above but with something in how names pos is \"not kept\""],"metadata":{"id":"weDDT-rb4fFb"}},{"cell_type":"markdown","source":["# Ablate the model and compare with original"],"metadata":{"id":"Lk3bffnCYq-p"}},{"cell_type":"markdown","source":["## keep pos of nums and end only"],"metadata":{"id":"jmosyDRQk3LU"}},{"cell_type":"markdown","source":["https://colab.research.google.com/drive/1CHRn-AMko9RNrl1bqiCwB7DS-rz1CoBP#scrollTo=KZiVdGTC6QlP&line=2&uniqifier=1"],"metadata":{"id":"uvtTBxIXELzy"}},{"cell_type":"code","source":["fullcirc = [(0, 1), (0, 3), (0, 5), (0, 7), (0, 9), (0, 10), (1, 5), (2, 2), (2, 9), (3, 0), (3, 3), (3, 7), (4, 4), (5, 5), (6, 1), (6, 6), (6, 9), (6, 10), (7, 10), (7, 11), (8, 8), (9, 1), (10, 7)]\n","\n","CIRCUIT = {\n","    \"number mover\": fullcirc,\n","    \"number mover 4\": fullcirc,\n","    \"number mover 3\": fullcirc,\n","    \"number mover 2\": fullcirc,\n","    \"number mover 1\": fullcirc,\n","}\n","\n","SEQ_POS_TO_KEEP = {\n","    \"number mover\": \"end\",\n","    \"number mover 4\": \"S4\",\n","    \"number mover 3\": \"S3\",\n","    \"number mover 2\": \"S2\",\n","    \"number mover 1\": \"S1\",\n","}\n","\n","mean_ablate_by_lst(CIRCUIT, SEQ_POS_TO_KEEP, model, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grNpaoRrkate","executionInfo":{"status":"ok","timestamp":1698165852489,"user_tz":240,"elapsed":9704,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"92df7031-de90-43a8-8037-61636ae1cbab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["23.74461555480957"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["circuit = [(0, 1), (0, 3), (0, 5), (0, 7), (0, 9), (0, 10), (1, 5), (2, 2), (2, 9), (3, 0), (3, 3), (3, 7), (4, 4), (5, 5), (6, 1), (6, 6), (6, 9), (6, 10), (7, 10), (7, 11), (8, 8), (9, 1), (10, 7)]"],"metadata":{"id":"irTpp05QELUW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## keep pos of names, nums, end"],"metadata":{"id":"9pCFPSeJk-rc"}},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            # 'Adam': 'Adam',\n","            'Bob': 'Bob',\n","            'Claire': 'Claire',\n","            'Don': 'Don',\n","            'Eve': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'S5': str(i+4),\n","            'text': f\"Adam is {i}. Bob is {i+1}. Claire is {i+2}. Don is {i+3}. Eve is\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 11)\n","dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"hpyu27CQlEd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            # 'Adam': 'Adam',\n","            'Bob': 'Bob',\n","            'Claire': 'Claire',\n","            'Don': 'Don',\n","            'Eve': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'S5': str(i+3),\n","            'text': f\"Adam is {i}. Bob is {i+1}. Claire is {i+2}. Don is {i+2}. Eve is\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","dataset_2 = Dataset(prompts_list_2, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"PkOrX5mdlEeG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fullcirc = [(0, 1), (0, 3), (0, 5), (0, 7), (0, 9), (0, 10), (1, 5), (2, 2), (2, 9), (3, 0), (3, 3), (3, 7), (4, 4), (5, 5), (6, 1), (6, 6), (6, 9), (6, 10), (7, 10), (7, 11), (8, 8), (9, 1), (10, 7)]\n","\n","CIRCUIT = {\n","    \"name mover 4\": fullcirc,\n","    \"name mover 3\": fullcirc,\n","    \"name mover 2\": fullcirc,\n","    \"name mover 1\": fullcirc,\n","    \"number mover\": fullcirc,\n","    \"number mover 4\": fullcirc,\n","    \"number mover 3\": fullcirc,\n","    \"number mover 2\": fullcirc,\n","    \"number mover 1\": fullcirc,\n","}\n","\n","SEQ_POS_TO_KEEP = {\n","    \"name mover 4\": \"Eve\",\n","    \"name mover 3\": \"Don\",\n","    \"name mover 2\": \"Claire\",\n","    \"name mover 1\": \"Bob\",\n","    \"number mover\": \"end\",\n","    \"number mover 4\": \"S4\",\n","    \"number mover 3\": \"S3\",\n","    \"number mover 2\": \"S2\",\n","    \"number mover 1\": \"S1\",\n","}\n","\n","mean_ablate_by_lst(CIRCUIT, SEQ_POS_TO_KEEP, model, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kD-DQJrflgU9","executionInfo":{"status":"ok","timestamp":1698166161918,"user_tz":240,"elapsed":8781,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"54922b84-3ccd-4942-88ac-ebcadc9e15bd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["23.767974853515625"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## among names only"],"metadata":{"id":"HsKjFIkRsm60"}},{"cell_type":"markdown","source":["### test prompts"],"metadata":{"id":"DQF0lzuokQer"}},{"cell_type":"code","source":["modeltest = HookedTransformer.from_pretrained(\"gpt2-small\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FW7ZRS0wtqs4","executionInfo":{"status":"ok","timestamp":1698241198048,"user_tz":240,"elapsed":8059,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e6563815-2bfa-48cf-cd5e-5975431aac8c"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}]},{"cell_type":"code","source":["example_prompt = \"table 1 lamp 2 fridge 3 chair 4 hat\"\n","example_answer = \" 5\"\n","utils.test_prompt(example_prompt, example_answer, modeltest, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"OQpqlEjwED7b","executionInfo":{"status":"ok","timestamp":1698241218199,"user_tz":240,"elapsed":383,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1067abd6-e43c-4d27-f9bb-05107dcce355"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'table', ' 1', ' lamp', ' 2', ' fridge', ' 3', ' chair', ' 4', ' hat']\n","Tokenized answer: [' 5']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.75\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m90.18\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.75</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90.18</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 17.75 Prob: 90.18% Token: | 5|\n","Top 1th token. Logit: 13.12 Prob:  0.88% Token: |ches|\n","Top 2th token. Logit: 12.61 Prob:  0.53% Token: | 1|\n","Top 3th token. Logit: 12.52 Prob:  0.48% Token: |\n","|\n","Top 4th token. Logit: 12.46 Prob:  0.45% Token: |chet|\n","Top 5th token. Logit: 12.41 Prob:  0.43% Token: | 4|\n","Top 6th token. Logit: 12.40 Prob:  0.43% Token: | 50|\n","Top 7th token. Logit: 12.35 Prob:  0.41% Token: |cher|\n","Top 8th token. Logit: 12.26 Prob:  0.37% Token: |chery|\n","Top 9th token. Logit: 11.86 Prob:  0.25% Token: | 6|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 5'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 5'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["example_prompt = \"table 1 lamp table 2 fridge 3 chair 4 hat\"\n","example_answer = \" 5\"\n","utils.test_prompt(example_prompt, example_answer, modeltest, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"M21-BIdzEN2U","executionInfo":{"status":"ok","timestamp":1698241247440,"user_tz":240,"elapsed":510,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b4598fcc-7fbc-470f-d1ed-7c69b17e091d"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'table', ' 1', ' lamp', ' table', ' 2', ' fridge', ' 3', ' chair', ' 4', ' hat']\n","Tokenized answer: [' 5']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m16.60\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m74.12\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.60</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74.12</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 16.60 Prob: 74.12% Token: | 5|\n","Top 1th token. Logit: 13.08 Prob:  2.20% Token: | 4|\n","Top 2th token. Logit: 12.74 Prob:  1.56% Token: | table|\n","Top 3th token. Logit: 12.71 Prob:  1.52% Token: |chery|\n","Top 4th token. Logit: 12.46 Prob:  1.18% Token: |ches|\n","Top 5th token. Logit: 12.33 Prob:  1.04% Token: |\n","|\n","Top 6th token. Logit: 12.26 Prob:  0.97% Token: | 1|\n","Top 7th token. Logit: 12.16 Prob:  0.88% Token: |chet|\n","Top 8th token. Logit: 11.67 Prob:  0.54% Token: | 6|\n","Top 9th token. Logit: 11.58 Prob:  0.49% Token: |cher|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 5'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 5'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["example_prompt = \"Adam 1 Bob 2 Claire 3 Don 4 Eve\"\n","example_answer = \" 5\"\n","utils.test_prompt(example_prompt, example_answer, modeltest, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"3M6ZKSyJtOAE","executionInfo":{"status":"ok","timestamp":1698168243317,"user_tz":240,"elapsed":373,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ffcd6221-365c-4875-9f44-adcc7a51c52f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Adam', ' 1', ' Bob', ' 2', ' Claire', ' 3', ' Don', ' 4', ' Eve']\n","Tokenized answer: [' 5']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.17\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m92.81\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.17</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92.81</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 18.17 Prob: 92.81% Token: | 5|\n","Top 1th token. Logit: 14.02 Prob:  1.46% Token: | 6|\n","Top 2th token. Logit: 12.82 Prob:  0.44% Token: | 10|\n","Top 3th token. Logit: 12.68 Prob:  0.38% Token: |\n","|\n","Top 4th token. Logit: 12.56 Prob:  0.34% Token: |lyn|\n","Top 5th token. Logit: 12.39 Prob:  0.29% Token: | 4|\n","Top 6th token. Logit: 12.09 Prob:  0.21% Token: |5|\n","Top 7th token. Logit: 11.89 Prob:  0.17% Token: | 1|\n","Top 8th token. Logit: 11.84 Prob:  0.16% Token: | 7|\n","Top 9th token. Logit: 11.79 Prob:  0.16% Token: | 50|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 5'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 5'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["example_prompt = \"Adam 1 Bob 2 Claire 3 Don 4 Eve\"\n","example_answer = \" 5\"\n","\n","model_abl = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","utils.test_prompt(example_prompt, example_answer, model_abl, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"id":"Gder9pp52QEG","executionInfo":{"status":"error","timestamp":1698170517407,"user_tz":240,"elapsed":6043,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1001cb38-290c-461f-c016-3b1f4e39fcfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Adam', ' 1', ' Bob', ' 2', ' Claire', ' 3', ' Don', ' 4', ' Eve']\n","Tokenized answer: [' 5']\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-60c2eb86a8f7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_abl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mioi_circuit_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_mean_ablation_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCIRCUIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_pos_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQ_POS_TO_KEEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_abl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/utils.py\u001b[0m in \u001b[0;36mtest_prompt\u001b[0;34m(prompt, answer, model, prepend_space_to_answer, print_details, prepend_bos, top_k)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenized prompt:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_str_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenized answer:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_str_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0manswer_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    549\u001b[0m                     )\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 residual = block(\n\u001b[0m\u001b[1;32m    552\u001b[0m                     \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                     \u001b[0;31m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;31m# queries, keys and values, independently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m             self.attn(\n\u001b[0m\u001b[1;32m   1036\u001b[0m                 \u001b[0mquery_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, head_index, query_pos, key_pos]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         z = self.hook_z(\n\u001b[0m\u001b[1;32m    610\u001b[0m             einsum(\n\u001b[1;32m    611\u001b[0m                 \u001b[0;31m\"\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0mkey_pos\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mfull_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfull_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             full_hook.__name__ = (\n","\u001b[0;32m/content/ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification/ioi_circuit_extraction.py\u001b[0m in \u001b[0;36mhook_fn_mask_z\u001b[0;34m(z, hook, heads_and_posns_to_keep, means)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Set z values to the mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_for_this_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (11) at non-singleton dimension 1"]}]},{"cell_type":"markdown","source":["### test get rep tok index"],"metadata":{"id":"WImgwQ26nb26"}},{"cell_type":"code","source":["tokens = model.tokenizer.tokenize('Adam 1 Bob 2 Claire 3 Don 4 Eve')\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8mMKhFRng3w","executionInfo":{"status":"ok","timestamp":1698232972745,"user_tz":240,"elapsed":145,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"640091e2-d29e-4619-c809-339da9a00681"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Adam', 'Ġ1', 'ĠBob', 'Ġ2', 'ĠClaire', 'Ġ3', 'ĠDon', 'Ġ4', 'ĠEve']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["target_token = \"Ġ\" + \"Claire\"\n","target_index = tokens.index(target_token)\n","target_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5Lr_4KinjpO","executionInfo":{"status":"ok","timestamp":1698232999015,"user_tz":240,"elapsed":152,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2ea926cd-0f34-4c47-8a46-1ca8e87177ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["### ablate"],"metadata":{"id":"AQrkoi9gkTJ7"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.io_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S5\"])[0] for prompt in self.prompts\n","        ]\n","        self.s_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S4\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'S5')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"Adam\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"iNYJUiHAuVfw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://github.com/callummcdougall/ARENA_2.0/blob/main/chapter1_transformers/exercises/part3_indirect_object_identification/ioi_dataset.py\n","\n","ioi_prompt[\"IO\"] = name_1"],"metadata":{"id":"ku7rK5JD1_nK"}},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'Adam': 'Adam',\n","            'Bob': 'Bob',\n","            'Claire': 'Claire',\n","            'Don': 'Don',\n","            'Eve': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'S5': str(i+4),\n","            'text': f\"Adam {i} Bob {i+1} Claire {i+2} Don {i+3} Eve\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 11)\n","dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"vtddE3GEuEkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.word_idx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ra65c6JTlZAJ","executionInfo":{"status":"ok","timestamp":1698233164793,"user_tz":240,"elapsed":11,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"35033f85-4caa-4faf-f38e-1b989b3848e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Adam': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'Bob': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n"," 'Claire': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4]),\n"," 'Don': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]),\n"," 'Eve': tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8]),\n"," 'S1': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n"," 'S2': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3]),\n"," 'S3': tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5]),\n"," 'S4': tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7]),\n"," 'end': tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8])}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'Adam': 'Adam',\n","            'Bob': 'Bob',\n","            'Claire': 'Claire',\n","            'Don': 'Don',\n","            'Eve': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'S5': str(i+3),\n","            'text': f\"Adam {i} Bob {i+1} Claire {i+2} Don {i+2} Eve\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","dataset_2 = Dataset(prompts_list_2, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"M039Uc2guEkY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_2.word_idx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hU5j1QHqm9Pw","executionInfo":{"status":"ok","timestamp":1698233600467,"user_tz":240,"elapsed":157,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"df1f3e91-9e3b-4ed3-a1ff-ce78bc0737c8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Adam': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'Bob': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n"," 'Claire': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4]),\n"," 'Don': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]),\n"," 'Eve': tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8]),\n"," 'S1': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n"," 'S2': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3]),\n"," 'S3': tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5]),\n"," 'S4': tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5]),\n"," 'end': tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8])}"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        \"name mover 4\": lst,\n","        \"name mover 3\": lst,\n","        \"name mover 2\": lst,\n","        \"name mover 1\": lst,\n","        \"name mover 0\": lst,\n","        \"number mover\": lst,\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"eGHACKWoszVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","logits_to_ave_logit_diff_2(ioi_logits_original, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7XQWXF5wC0a","executionInfo":{"status":"ok","timestamp":1698233227027,"user_tz":240,"elapsed":140,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bb22ef0c-c1e7-4ebf-e7c2-4c4a5b456bf5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.4184, device='cuda:0')"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["If don't reset hooks, may accidentally use a hooked model (will STILL happen if renamed the variable that takes the value, so add_mean_ablation hook is by ref, not new copy!)"],"metadata":{"id":"0jtKAq2YzRhr"}},{"cell_type":"code","source":["lst = [(layer, head) for layer in range(12) for head in range(12)]\n","CIRCUIT = {\n","    \"name mover 4\": lst,\n","    \"name mover 3\": lst,\n","    \"name mover 2\": lst,\n","    \"name mover 1\": lst,\n","    \"name mover 0\": lst,\n","    \"number mover\": lst,\n","    \"number mover 4\": lst,\n","    \"number mover 3\": lst,\n","    \"number mover 2\": lst,\n","    \"number mover 1\": lst,\n","}\n","SEQ_POS_TO_KEEP = {\n","    \"name mover 4\": \"Eve\",\n","    \"name mover 3\": \"Don\",\n","    \"name mover 2\": \"Claire\",\n","    \"name mover 1\": \"Bob\",\n","    \"name mover 0\": \"Adam\",\n","    \"number mover\": \"end\",\n","    \"number mover 4\": \"S4\",\n","    \"number mover 3\": \"S3\",\n","    \"number mover 2\": \"S2\",\n","    \"number mover 1\": \"S1\",\n","}"],"metadata":{"id":"rUkPx-CnwrjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_abl = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","ioi_logits_minimal = model_abl(dataset.toks)\n","logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)"],"metadata":{"id":"glzV7TJwwJ5e","executionInfo":{"status":"error","timestamp":1698233240626,"user_tz":240,"elapsed":320,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/","height":530},"outputId":"481ab3c6-b2fa-42af-853a-e2795027a134"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-7967719f2944>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_abl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mioi_circuit_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_mean_ablation_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCIRCUIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_pos_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQ_POS_TO_KEEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mioi_logits_minimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_abl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogits_to_ave_logit_diff_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mioi_logits_minimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification/ioi_circuit_extraction.py\u001b[0m in \u001b[0;36madd_mean_ablation_hook\u001b[0;34m(model, means_dataset, circuit, seq_pos_to_keep, is_permanent)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Convert this into a boolean map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mheads_and_posns_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_heads_and_posns_to_keep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_pos_to_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Get a hook function which will patch in the mean z values for each head, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification/ioi_circuit_extraction.py\u001b[0m in \u001b[0;36mget_heads_and_posns_to_keep\u001b[0;34m(means_dataset, model, circuit, seq_pos_to_keep)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhead_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcircuit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mseq_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_pos_to_keep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeans_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Eve'"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pZD9BGGwUhg","executionInfo":{"status":"ok","timestamp":1698171949006,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"821e9634-8120-47f4-9ea8-34997083db3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.5386, device='cuda:0')"]},"metadata":{},"execution_count":127}]},{"cell_type":"markdown","source":["So this means we're not getting the full circuit. What's missing?"],"metadata":{"id":"dbOZvvc2z6MT"}},{"cell_type":"code","source":["lst = [(layer, head) for layer in range(12) for head in range(12)]\n","CIRCUIT = {\n","    \"number mover\": lst,\n","    \"number mover 4\": lst,\n","    \"number mover 3\": lst,\n","    \"number mover 2\": lst,\n","    \"number mover 1\": lst,\n","}\n","SEQ_POS_TO_KEEP = {\n","    \"number mover\": \"end\",\n","    \"number mover 4\": \"S4\",\n","    \"number mover 3\": \"S3\",\n","    \"number mover 2\": \"S2\",\n","    \"number mover 1\": \"S1\",\n","}\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model_abl = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","ioi_logits_minimal = model_abl(dataset.toks)\n","logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q4q1tF7J6qiC","executionInfo":{"status":"ok","timestamp":1698171903321,"user_tz":240,"elapsed":8435,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6b9fa194-196b-4e04-cc86-ff293f5f1790"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.5517, device='cuda:0')"]},"metadata":{},"execution_count":122}]},{"cell_type":"markdown","source":["This is nearly the same score, so the names are just not being kept but bc the score is diff, something is happening."],"metadata":{"id":"MXXCoik17sLU"}},{"cell_type":"markdown","source":["## try diff ways to keep names"],"metadata":{"id":"YcYIV0mP41N2"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.io_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S5\"])[0] for prompt in self.prompts\n","        ]\n","        self.s_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S4\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'S5')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"Adam1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"ZzN09t2N50g9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'Adam1': 'Adam',\n","            'Bob1': 'Bob',\n","            'Claire1': 'Claire',\n","            'Don1': 'Don',\n","            'Eve1': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'S5': str(i+4),\n","            'text': f\"Adam {i} Bob {i+1} Claire {i+2} Don {i+3} Eve\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 11)\n","dataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"qaTyuwJh46im"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'Adam1': 'Adam',\n","            'Bob1': 'Bob',\n","            'Claire1': 'Claire',\n","            'Don1': 'Don',\n","            'Eve1': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'S5': str(i+3),\n","            'text': f\"Adam {i} Bob {i+1} Claire {i+2} Don {i+2} Eve\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","dataset_2 = Dataset(prompts_list_2, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"JPwOmKA-46ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CIRCUIT = {\n","        \"name mover 4\": lst,\n","        \"name mover 3\": lst,\n","        \"name mover 2\": lst,\n","        \"name mover 1\": lst,\n","        \"name mover 0\": lst,\n","        \"number mover\": lst,\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","SEQ_POS_TO_KEEP = {  # the value is not token, but key to token in prompt_dict dataset\n","    \"name mover 4\": \"Eve1\",\n","    \"name mover 3\": \"Don1\",\n","    \"name mover 2\": \"Claire1\",\n","    \"name mover 1\": \"Bob1\",\n","    \"name mover 0\": \"Adam1\",\n","    \"number mover\": \"end\",\n","    \"number mover 4\": \"S4\",\n","    \"number mover 3\": \"S3\",\n","    \"number mover 2\": \"S2\",\n","    \"number mover 1\": \"S1\",\n","}"],"metadata":{"id":"AKojZYnn8dxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model_abl = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","ioi_logits_minimal = model_abl(dataset.toks)\n","logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698172169423,"user_tz":240,"elapsed":5359,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"T5ij0L8b6C4F","outputId":"9d3d10ce-a8aa-462c-a5ee-def54fb60211"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.5386, device='cuda:0')"]},"metadata":{},"execution_count":133}]},{"cell_type":"markdown","source":["This shows it's not an issue with the keys being the same string value as the tokens."],"metadata":{"id":"WQW1faXm8xHv"}},{"cell_type":"markdown","source":["## fix seq pos issue"],"metadata":{"id":"IBn9iV-CqSkG"}},{"cell_type":"markdown","source":["SOLN: the corrupted had repeated tokens in “Adam 1 Bob 2 Claire 3 Don 3 Eve”, so the repeated query seq pos index (the second 3) was not kept (non-ablated) when running tokens.index(target_token). The previous dataset of “1 2 3 3” did not have this issue as the datasets always kept the query end pos non-ablated, which was coincidentally on the second 3. But in the new case, the last token was “Eve” so this did not occur."],"metadata":{"id":"Pdi8V0q6qUnm"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.io_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.s_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"fzqgroiKqnxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos_dict = {\n","    'Adam': 0,\n","    'Bob': 2,\n","    'Claire': 4,\n","    'Don': 6,\n","    'Eve': 8,\n","    'S1': 1,\n","    'S2': 3,\n","    'S3': 5,\n","    'S4': 7,\n","}"],"metadata":{"id":"WRXClRW-q1CT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'Adam': 'Adam',\n","            'Bob': 'Bob',\n","            'Claire': 'Claire',\n","            'Don': 'Don',\n","            'Eve': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'corr': str(i+4),\n","            'incorr': str(i+3),\n","            'text': f\"Adam {i} Bob {i+1} Claire {i+2} Don {i+3} Eve\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list, pos_dict\n","\n","prompts_list, pos_dict = generate_prompts_list(1, 11)\n","dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"kXrS8eF_qzUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'Adam': 'Adam',\n","            'Bob': 'Bob',\n","            'Claire': 'Claire',\n","            'Don': 'Don',\n","            'Eve': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'corr': str(i+3),\n","            'incorr': str(i+4),\n","            'text': f\"Adam {i} Bob {i+1} Claire {i+2} Don {i+2} Eve\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"vnDlWibUsRDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        \"name mover 4\": lst,\n","        \"name mover 3\": lst,\n","        \"name mover 2\": lst,\n","        \"name mover 1\": lst,\n","        \"name mover 0\": lst,\n","        \"number mover\": lst,\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"NI8WfXIusmw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","logits_to_ave_logit_diff_2(ioi_logits_original, dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698235059897,"user_tz":240,"elapsed":163,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e1e41821-d4f1-4ee8-9241-86d3e5c692bd","id":"wtV2Ltlosmw8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.4184, device='cuda:0')"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["lst = [(layer, head) for layer in range(12) for head in range(12)]\n","CIRCUIT = {\n","    \"name mover 4\": lst,\n","    \"name mover 3\": lst,\n","    \"name mover 2\": lst,\n","    \"name mover 1\": lst,\n","    \"name mover 0\": lst,\n","    \"number mover\": lst,\n","    \"number mover 4\": lst,\n","    \"number mover 3\": lst,\n","    \"number mover 2\": lst,\n","    \"number mover 1\": lst,\n","}\n","SEQ_POS_TO_KEEP = {\n","    \"name mover 4\": \"Eve\",\n","    \"name mover 3\": \"Don\",\n","    \"name mover 2\": \"Claire\",\n","    \"name mover 1\": \"Bob\",\n","    \"name mover 0\": \"Adam\",\n","    \"number mover\": \"end\",\n","    \"number mover 4\": \"S4\",\n","    \"number mover 3\": \"S3\",\n","    \"number mover 2\": \"S2\",\n","    \"number mover 1\": \"S1\",\n","}"],"metadata":{"id":"rOqsXib9smw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_abl = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","ioi_logits_minimal = model_abl(dataset.toks)\n","logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698235072983,"user_tz":240,"elapsed":5165,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1bb9f170-7508-40dc-e353-02acebd9dee3","id":"UvH6S2IUsmw9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.4184, device='cuda:0')"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["## prune backw once"],"metadata":{"id":"ztnoLjm67Wsk"}},{"cell_type":"code","source":["# Start with full circuit\n","curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n","threshold = 3  # This is T, a %. if performance is less than T%, allow its removal\n","\n","for layer in range(11, -1, -1):  # go thru all heads in a layer first\n","    for head in range(12):\n","        # Copying the curr_circuit so we can iterate over one and modify the other\n","        copy_circuit = curr_circuit.copy()\n","\n","        # Temporarily removing the current tuple from the copied circuit\n","        copy_circuit.remove((layer, head))\n","\n","        new_score = mean_ablate_by_lst(copy_circuit, model, dataset, dataset_2, print_output=False).item()\n","\n","        # print((layer,head), new_score)\n","        # If the result is less than the threshold, remove the tuple from the original list\n","        if (100 - new_score) < threshold:\n","            curr_circuit.remove((layer, head))\n","\n","            print(\"Removed:\", (layer, head))\n","            print(new_score)\n","            print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oq0svFCVsp7N","executionInfo":{"status":"ok","timestamp":1698235813012,"user_tz":240,"elapsed":624108,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c6d0a9f4-6fc4-44a2-c942-5f3a1773021b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Removed: (11, 0)\n","99.68688201904297\n","\n","\n","Removed: (11, 1)\n","99.76443481445312\n","\n","\n","Removed: (11, 2)\n","99.79711151123047\n","\n","\n","Removed: (11, 3)\n","99.8869400024414\n","\n","\n","Removed: (11, 4)\n","100.07891845703125\n","\n","\n","Removed: (11, 5)\n","100.17557525634766\n","\n","\n","Removed: (11, 6)\n","100.26312255859375\n","\n","\n","Removed: (11, 7)\n","100.1176986694336\n","\n","\n","Removed: (11, 8)\n","100.04216766357422\n","\n","\n","Removed: (11, 9)\n","99.93612670898438\n","\n","\n","Removed: (11, 10)\n","99.1741943359375\n","\n","\n","Removed: (11, 11)\n","99.58927917480469\n","\n","\n","Removed: (10, 0)\n","99.56360626220703\n","\n","\n","Removed: (10, 1)\n","99.53450775146484\n","\n","\n","Removed: (10, 2)\n","103.23387908935547\n","\n","\n","Removed: (10, 3)\n","103.28461456298828\n","\n","\n","Removed: (10, 4)\n","103.33687591552734\n","\n","\n","Removed: (10, 5)\n","102.95157623291016\n","\n","\n","Removed: (10, 6)\n","102.95909881591797\n","\n","\n","Removed: (10, 8)\n","102.94963836669922\n","\n","\n","Removed: (10, 9)\n","102.81712341308594\n","\n","\n","Removed: (10, 10)\n","103.00299835205078\n","\n","\n","Removed: (10, 11)\n","102.93616485595703\n","\n","\n","Removed: (9, 0)\n","102.90625762939453\n","\n","\n","Removed: (9, 2)\n","102.79837036132812\n","\n","\n","Removed: (9, 3)\n","102.8944091796875\n","\n","\n","Removed: (9, 4)\n","103.00678253173828\n","\n","\n","Removed: (9, 5)\n","102.70673370361328\n","\n","\n","Removed: (9, 6)\n","102.09217071533203\n","\n","\n","Removed: (9, 7)\n","101.77227783203125\n","\n","\n","Removed: (9, 8)\n","101.74076843261719\n","\n","\n","Removed: (9, 9)\n","101.73675537109375\n","\n","\n","Removed: (9, 10)\n","101.78812408447266\n","\n","\n","Removed: (9, 11)\n","101.73698425292969\n","\n","\n","Removed: (8, 0)\n","102.50807189941406\n","\n","\n","Removed: (8, 1)\n","103.10546875\n","\n","\n","Removed: (8, 2)\n","102.89446258544922\n","\n","\n","Removed: (8, 3)\n","103.00113677978516\n","\n","\n","Removed: (8, 4)\n","102.9970474243164\n","\n","\n","Removed: (8, 5)\n","103.07870483398438\n","\n","\n","Removed: (8, 6)\n","101.02293395996094\n","\n","\n","Removed: (8, 7)\n","102.1684799194336\n","\n","\n","Removed: (8, 9)\n","102.51029205322266\n","\n","\n","Removed: (8, 10)\n","102.61512756347656\n","\n","\n","Removed: (7, 0)\n","102.42118072509766\n","\n","\n","Removed: (7, 1)\n","100.74735260009766\n","\n","\n","Removed: (7, 2)\n","100.94426727294922\n","\n","\n","Removed: (7, 3)\n","102.1238784790039\n","\n","\n","Removed: (7, 4)\n","102.18050384521484\n","\n","\n","Removed: (7, 5)\n","102.0845718383789\n","\n","\n","Removed: (7, 6)\n","102.26620483398438\n","\n","\n","Removed: (7, 7)\n","102.07613372802734\n","\n","\n","Removed: (7, 8)\n","102.03611755371094\n","\n","\n","Removed: (7, 9)\n","101.96073150634766\n","\n","\n","Removed: (7, 10)\n","102.16853332519531\n","\n","\n","Removed: (7, 11)\n","97.5019302368164\n","\n","\n","Removed: (6, 3)\n","97.48104858398438\n","\n","\n","Removed: (6, 4)\n","97.20357513427734\n","\n","\n","Removed: (6, 5)\n","98.84095001220703\n","\n","\n","Removed: (6, 7)\n","98.28329467773438\n","\n","\n","Removed: (6, 8)\n","97.00325775146484\n","\n","\n","Removed: (6, 9)\n","98.39256286621094\n","\n","\n","Removed: (6, 11)\n","98.4332275390625\n","\n","\n","Removed: (5, 0)\n","97.20449829101562\n","\n","\n","Removed: (5, 1)\n","97.20577239990234\n","\n","\n","Removed: (5, 2)\n","97.34902954101562\n","\n","\n","Removed: (5, 3)\n","98.20621490478516\n","\n","\n","Removed: (5, 4)\n","98.38618469238281\n","\n","\n","Removed: (5, 6)\n","97.73218536376953\n","\n","\n","Removed: (5, 7)\n","97.73014831542969\n","\n","\n","Removed: (5, 8)\n","97.32868957519531\n","\n","\n","Removed: (5, 9)\n","98.87557983398438\n","\n","\n","Removed: (5, 10)\n","98.77037048339844\n","\n","\n","Removed: (5, 11)\n","98.9723129272461\n","\n","\n","Removed: (4, 0)\n","98.97708892822266\n","\n","\n","Removed: (4, 1)\n","99.05338287353516\n","\n","\n","Removed: (4, 2)\n","99.03290557861328\n","\n","\n","Removed: (4, 3)\n","98.9662094116211\n","\n","\n","Removed: (4, 5)\n","98.99786376953125\n","\n","\n","Removed: (4, 6)\n","98.45587921142578\n","\n","\n","Removed: (4, 7)\n","98.29505920410156\n","\n","\n","Removed: (4, 8)\n","98.4485092163086\n","\n","\n","Removed: (4, 9)\n","98.39722442626953\n","\n","\n","Removed: (4, 10)\n","98.5672607421875\n","\n","\n","Removed: (4, 11)\n","98.57452392578125\n","\n","\n","Removed: (3, 1)\n","98.78215026855469\n","\n","\n","Removed: (3, 2)\n","99.16571044921875\n","\n","\n","Removed: (3, 3)\n","98.58397674560547\n","\n","\n","Removed: (3, 4)\n","99.48835754394531\n","\n","\n","Removed: (3, 5)\n","99.4000473022461\n","\n","\n","Removed: (3, 6)\n","99.32379913330078\n","\n","\n","Removed: (3, 7)\n","99.44088745117188\n","\n","\n","Removed: (3, 8)\n","99.53914642333984\n","\n","\n","Removed: (3, 9)\n","99.36531829833984\n","\n","\n","Removed: (3, 10)\n","99.36311340332031\n","\n","\n","Removed: (3, 11)\n","99.147705078125\n","\n","\n","Removed: (2, 0)\n","99.03527069091797\n","\n","\n","Removed: (2, 1)\n","99.11026763916016\n","\n","\n","Removed: (2, 2)\n","99.25616455078125\n","\n","\n","Removed: (2, 3)\n","99.43875122070312\n","\n","\n","Removed: (2, 4)\n","99.27827453613281\n","\n","\n","Removed: (2, 5)\n","98.96796417236328\n","\n","\n","Removed: (2, 6)\n","99.29242706298828\n","\n","\n","Removed: (2, 7)\n","99.50041198730469\n","\n","\n","Removed: (2, 8)\n","99.61238098144531\n","\n","\n","Removed: (2, 9)\n","99.53150177001953\n","\n","\n","Removed: (2, 10)\n","99.50527954101562\n","\n","\n","Removed: (2, 11)\n","99.4676513671875\n","\n","\n","Removed: (1, 0)\n","99.64298248291016\n","\n","\n","Removed: (1, 1)\n","99.5807113647461\n","\n","\n","Removed: (1, 2)\n","99.60664367675781\n","\n","\n","Removed: (1, 3)\n","99.49852752685547\n","\n","\n","Removed: (1, 4)\n","99.3830795288086\n","\n","\n","Removed: (1, 6)\n","99.80157470703125\n","\n","\n","Removed: (1, 7)\n","100.1041259765625\n","\n","\n","Removed: (1, 8)\n","100.0338363647461\n","\n","\n","Removed: (1, 9)\n","99.99555969238281\n","\n","\n","Removed: (1, 10)\n","100.0009536743164\n","\n","\n","Removed: (1, 11)\n","100.7182388305664\n","\n","\n","Removed: (0, 0)\n","100.65328216552734\n","\n","\n","Removed: (0, 2)\n","99.76892852783203\n","\n","\n","Removed: (0, 3)\n","99.5243148803711\n","\n","\n","Removed: (0, 4)\n","99.43586730957031\n","\n","\n","Removed: (0, 5)\n","98.2079086303711\n","\n","\n","Removed: (0, 6)\n","97.96487426757812\n","\n","\n","Removed: (0, 7)\n","97.48351287841797\n","\n","\n","Removed: (0, 9)\n","97.260009765625\n","\n","\n","Removed: (0, 11)\n","97.355712890625\n","\n","\n"]}]},{"cell_type":"code","source":["curr_circuit"],"metadata":{"id":"XKO1PNHUvejZ","executionInfo":{"status":"ok","timestamp":1698235881983,"user_tz":240,"elapsed":232,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1267a3fd-0ac3-40c7-a1e6-2ad3ed0b22a4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 1),\n"," (0, 8),\n"," (0, 10),\n"," (1, 5),\n"," (3, 0),\n"," (4, 4),\n"," (5, 5),\n"," (6, 0),\n"," (6, 1),\n"," (6, 2),\n"," (6, 6),\n"," (6, 10),\n"," (8, 8),\n"," (8, 11),\n"," (9, 1),\n"," (10, 7)]"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["## try running among words circ on pure digits"],"metadata":{"id":"aWXUmCUSwMmj"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.io_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S5\"])[0] for prompt in self.prompts\n","        ]\n","        self.s_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S4\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'S5')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"GX_55NJxwkMz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Repalce io_tokens with correct answer (next, which is '5') and s_tokens with incorrect (current, which repeats)"],"metadata":{"id":"JGJxhJMswkM9"}},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'S5': str(i+4),\n","            'text': f\"{i} {i+1} {i+2} {i+3}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 11)\n","pureDataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"UcpAt8vEwkM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'S5': str(i+3),\n","            'text': f\"{i} {i+1} {i+2} {i+2}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","pureDataset_2 = Dataset(prompts_list_2, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"N8PtYrS5wkM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": lst,\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"mdUFzKLow4vD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgpGMTWLbibq","executionInfo":{"status":"ok","timestamp":1698236287974,"user_tz":240,"elapsed":3844,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8487d340-0f4c-43fc-c183-f07e06dc1845"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 1.1478\n","Average logit difference (circuit / full) %: 24.8247\n"]},{"output_type":"execute_result","data":{"text/plain":["24.824731826782227"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["## ablate using guessed seq pos from attn pats"],"metadata":{"id":"-kWhwxZqyYuN"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.io_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.s_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698238815876,"user_tz":240,"elapsed":439,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"6mmVIeFeymsD"},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["pos_dict = {\n","    'Adam': 0,\n","    'Bob': 2,\n","    'Claire': 4,\n","    'Don': 6,\n","    'Eve': 8,\n","    'S1': 1,\n","    'S2': 3,\n","    'S3': 5,\n","    'S4': 7,\n","}"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698238815876,"user_tz":240,"elapsed":23,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"fiuLycziymsE"},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'Adam': 'Adam',\n","            'Bob': 'Bob',\n","            'Claire': 'Claire',\n","            'Don': 'Don',\n","            'Eve': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'corr': str(i+4),\n","            'incorr': str(i+3),\n","            'text': f\"Adam {i} Bob {i+1} Claire {i+2} Don {i+3} Eve\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list, pos_dict\n","\n","prompts_list, pos_dict = generate_prompts_list(1, 11)\n","dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698238815876,"user_tz":240,"elapsed":23,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"9G8P-RjMymsF"},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'Adam': 'Adam',\n","            'Bob': 'Bob',\n","            'Claire': 'Claire',\n","            'Don': 'Don',\n","            'Eve': 'Eve',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'corr': str(i+3),\n","            'incorr': str(i+4),\n","            'text': f\"Adam {i} Bob {i+1} Claire {i+2} Don {i+2} Eve\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698238815876,"user_tz":240,"elapsed":23,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"6jM4-mT5ymsF"},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["### all pos"],"metadata":{"id":"4PRZTJBFzvw6"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10), (8, 8), (8, 11), (9, 1), (10, 7)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuHhRWptzVYp","executionInfo":{"status":"ok","timestamp":1698236840278,"user_tz":240,"elapsed":5731,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1d397f4c-860f-4904-ef57-6904ff0531df"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["97.355712890625"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["### late heads only at end"],"metadata":{"id":"0HzzgL-fzx2r"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        \"name mover 4\": lst,\n","        \"name mover 3\": lst,\n","        \"name mover 2\": lst,\n","        \"name mover 1\": lst,\n","        \"name mover 0\": lst,\n","        \"number mover\": [(8, 8), (8, 11), (9, 1), (10, 7)],\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698236981142,"user_tz":240,"elapsed":5097,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"5qpVUZUrymsF","outputId":"dd0a514d-19f1-4690-dec7-0cd8d3aeea7b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["98.0891342163086"]},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["Amazingly, it's even BETTER?"],"metadata":{"id":"4oYP5ppp0CU8"}},{"cell_type":"markdown","source":["### late heads only at end, num detect only at nums"],"metadata":{"id":"I7XSKCg10Ile"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10), (8, 8), (8, 11), (9, 1), (10, 7)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        \"name mover 4\": lst,\n","        \"name mover 3\": lst,\n","        \"name mover 2\": lst,\n","        \"name mover 1\": lst,\n","        \"name mover 0\": lst,\n","        \"number mover\": [(8, 8), (8, 11), (9, 1), (10, 7)],\n","        \"number mover 4\": [(1, 5), (4, 4)],\n","        \"number mover 3\": [(1, 5), (4, 4)],\n","        \"number mover 2\": [(1, 5), (4, 4)],\n","        \"number mover 1\": [(1, 5), (4, 4)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698237152193,"user_tz":240,"elapsed":7955,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"wruOOshUzfVE","outputId":"7c3f2999-7717-4039-f433-5de53aa6dda4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["61.35452651977539"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["Unfortunately, number detectors do much worse here. But this is an obvious bug- you are ablating ALL heads except 1.5 and 4.4 at the numbers. Clearly, you need more heads than those. So just ablate the heads at the word pos."],"metadata":{"id":"olHSw_0Z0nIO"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(8, 8), (8, 11), (9, 1), (10, 7)],\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6iUw9V1u0y7T","executionInfo":{"status":"ok","timestamp":1698237357052,"user_tz":240,"elapsed":5679,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"935ed378-47fe-4f52-a3da-21297b6f2cf4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["91.36726379394531"]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["The score is slightly worse. This means SOME parts of the names are still needed. But which parts?"],"metadata":{"id":"xBcrnxlF1aO9"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        \"name mover 4\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"name mover 3\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"name mover 2\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"name mover 1\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"name mover 0\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"number mover\": [(8, 8), (8, 11), (9, 1), (10, 7)],\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fr6giD0p1l1I","executionInfo":{"status":"ok","timestamp":1698237450282,"user_tz":240,"elapsed":5196,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5651e067-4d75-4782-a2e1-570b36dae40d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["98.05598449707031"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","source":["This shows that 1.5 and 4.4 have no impact on the names at all."],"metadata":{"id":"iC03-WTm1xAM"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)], # these only make 0.02% diff when kept at the end\n","        \"name mover 3\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)],\n","        \"name mover 2\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)],\n","        \"name mover 1\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)],\n","        \"name mover 0\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)],\n","        \"number mover\": [(8, 8), (8, 11), (9, 1), (10, 7)],\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3mKTXBE101_","executionInfo":{"status":"ok","timestamp":1698237543578,"user_tz":240,"elapsed":6358,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"32b10820-c288-4d6d-caa9-fdcde1dc8173"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["91.3373794555664"]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","source":["So L6 still needed for the names. But 10% is not make or break diff. We can get away with removing it for names."],"metadata":{"id":"M0BNmps62NK7"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)], # these only make 0.02% diff when kept at the end\n","        \"name mover 3\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)],\n","        \"name mover 2\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)],\n","        \"name mover 1\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)],\n","        \"name mover 0\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5)],\n","        \"number mover\": [(8, 8), (8, 11), (9, 1), (10, 7)],\n","        \"number mover 4\": [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5)],\n","        \"number mover 3\": [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5)],\n","        \"number mover 2\": [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5)],\n","        \"number mover 1\": [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xbGXM20D3BJv","executionInfo":{"status":"ok","timestamp":1698237813876,"user_tz":240,"elapsed":7137,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"95ce93ce-28e8-461b-e91d-4c438bab60f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["83.72284698486328"]},"metadata":{},"execution_count":76}]},{"cell_type":"markdown","source":["L6 is very impt for numbers; make an 8% diff."],"metadata":{"id":"LiSQjZn13JwJ"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        \"name mover 4\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"name mover 3\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"name mover 2\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"name mover 1\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"name mover 0\": [(0, 1), (0, 8), (0, 10), (3, 0), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"number mover\": [(8, 8), (8, 11), (9, 1), (10, 7)],\n","        \"number mover 4\": [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"number mover 3\": [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"number mover 2\": [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","        \"number mover 1\": [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rrFV2WPB2Uwt","executionInfo":{"status":"ok","timestamp":1698237718047,"user_tz":240,"elapsed":9814,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5b19a0bd-731f-4cb1-80c5-ff04afc7d81b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["95.2208023071289"]},"metadata":{},"execution_count":75}]},{"cell_type":"markdown","source":["5.5 also makes 3% diff for numbers, but not entirely"],"metadata":{"id":"CV5n1Xe_2yZD"}},{"cell_type":"markdown","source":["## work backw once, 10%"],"metadata":{"id":"_sSuNMsf3Nsu"}},{"cell_type":"code","source":["# curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        \"name mover 4\": lst,\n","        \"name mover 3\": lst,\n","        \"name mover 2\": lst,\n","        \"name mover 1\": lst,\n","        \"name mover 0\": lst,\n","        \"number mover\": lst,\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3qTPRmB7R93","executionInfo":{"status":"ok","timestamp":1698240341465,"user_tz":240,"elapsed":6596,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"22ae7f02-0983-4e64-d8e1-773fbf5108da"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["100.0"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# Start with full circuit\n","curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n","# curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10), (8, 8), (8, 11), (9, 1), (10, 7)]\n","threshold = 10  # This is T, a %. if performance is less than T%, allow its removal\n","\n","for layer in range(11, -1, -1):  # go thru all heads in a layer first\n","    for head in range(12):\n","        # Copying the curr_circuit so we can iterate over one and modify the other\n","        copy_circuit = curr_circuit.copy()\n","\n","        # Temporarily removing the current tuple from the copied circuit\n","        copy_circuit.remove((layer, head))\n","\n","        new_score = mean_ablate_by_lst(copy_circuit, model, dataset, dataset_2, print_output=False).item()\n","\n","        # print((layer,head), new_score)\n","        # If the result is less than the threshold, remove the tuple from the original list\n","        if (100 - new_score) < threshold:\n","            curr_circuit.remove((layer, head))\n","\n","            print(\"Removed:\", (layer, head))\n","            print(new_score)\n","            print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698239574081,"user_tz":240,"elapsed":594229,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"51994d1a-582c-4cd8-c73c-ef98a9f43a7e","id":"nJnz8Ep_yYuO"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Removed: (11, 0)\n","99.6868667602539\n","\n","\n","Removed: (11, 1)\n","99.76444244384766\n","\n","\n","Removed: (11, 2)\n","99.7970962524414\n","\n","\n","Removed: (11, 3)\n","99.88690185546875\n","\n","\n","Removed: (11, 4)\n","100.07891845703125\n","\n","\n","Removed: (11, 5)\n","100.17558288574219\n","\n","\n","Removed: (11, 6)\n","100.26310729980469\n","\n","\n","Removed: (11, 7)\n","100.11766815185547\n","\n","\n","Removed: (11, 8)\n","100.04216766357422\n","\n","\n","Removed: (11, 9)\n","99.93612670898438\n","\n","\n","Removed: (11, 10)\n","99.17417907714844\n","\n","\n","Removed: (11, 11)\n","99.58924102783203\n","\n","\n","Removed: (10, 0)\n","99.5635757446289\n","\n","\n","Removed: (10, 1)\n","99.53450775146484\n","\n","\n","Removed: (10, 2)\n","103.23390197753906\n","\n","\n","Removed: (10, 3)\n","103.28462219238281\n","\n","\n","Removed: (10, 4)\n","103.33689880371094\n","\n","\n","Removed: (10, 5)\n","102.95158386230469\n","\n","\n","Removed: (10, 6)\n","102.95907592773438\n","\n","\n","Removed: (10, 7)\n","92.94772338867188\n","\n","\n","Removed: (10, 8)\n","92.93510437011719\n","\n","\n","Removed: (10, 9)\n","92.80360412597656\n","\n","\n","Removed: (10, 10)\n","92.9897232055664\n","\n","\n","Removed: (10, 11)\n","92.92302703857422\n","\n","\n","Removed: (9, 0)\n","92.9098129272461\n","\n","\n","Removed: (9, 2)\n","92.8400650024414\n","\n","\n","Removed: (9, 3)\n","92.843505859375\n","\n","\n","Removed: (9, 4)\n","92.85585021972656\n","\n","\n","Removed: (9, 5)\n","92.36489868164062\n","\n","\n","Removed: (9, 6)\n","92.29570007324219\n","\n","\n","Removed: (9, 7)\n","91.9838638305664\n","\n","\n","Removed: (9, 8)\n","91.93981170654297\n","\n","\n","Removed: (9, 9)\n","91.93893432617188\n","\n","\n","Removed: (9, 10)\n","91.98101043701172\n","\n","\n","Removed: (9, 11)\n","91.93134307861328\n","\n","\n","Removed: (8, 0)\n","92.62820434570312\n","\n","\n","Removed: (8, 1)\n","93.36155700683594\n","\n","\n","Removed: (8, 2)\n","93.1753158569336\n","\n","\n","Removed: (8, 3)\n","93.29148864746094\n","\n","\n","Removed: (8, 4)\n","93.2880630493164\n","\n","\n","Removed: (8, 5)\n","93.37386322021484\n","\n","\n","Removed: (8, 6)\n","91.18830108642578\n","\n","\n","Removed: (8, 7)\n","92.29505920410156\n","\n","\n","Removed: (8, 9)\n","92.66804504394531\n","\n","\n","Removed: (8, 10)\n","92.70318603515625\n","\n","\n","Removed: (7, 0)\n","92.4883041381836\n","\n","\n","Removed: (7, 1)\n","90.51466369628906\n","\n","\n","Removed: (7, 2)\n","90.74198913574219\n","\n","\n","Removed: (7, 3)\n","91.74610137939453\n","\n","\n","Removed: (7, 4)\n","91.8059310913086\n","\n","\n","Removed: (7, 5)\n","91.72198486328125\n","\n","\n","Removed: (7, 6)\n","91.93733215332031\n","\n","\n","Removed: (7, 7)\n","91.6797103881836\n","\n","\n","Removed: (7, 8)\n","91.63168334960938\n","\n","\n","Removed: (7, 9)\n","91.54348754882812\n","\n","\n","Removed: (7, 10)\n","91.88542938232422\n","\n","\n","Removed: (6, 0)\n","91.63063049316406\n","\n","\n","Removed: (6, 1)\n","90.0827407836914\n","\n","\n","Removed: (6, 3)\n","90.10624694824219\n","\n","\n","Removed: (6, 5)\n","91.69551086425781\n","\n","\n","Removed: (6, 7)\n","91.03656768798828\n","\n","\n","Removed: (6, 8)\n","90.80362701416016\n","\n","\n","Removed: (6, 9)\n","92.93582916259766\n","\n","\n","Removed: (6, 11)\n","93.10391998291016\n","\n","\n","Removed: (5, 0)\n","92.20570373535156\n","\n","\n","Removed: (5, 1)\n","92.80052185058594\n","\n","\n","Removed: (5, 2)\n","93.1856460571289\n","\n","\n","Removed: (5, 3)\n","93.8970947265625\n","\n","\n","Removed: (5, 4)\n","94.21048736572266\n","\n","\n","Removed: (5, 6)\n","93.8877944946289\n","\n","\n","Removed: (5, 7)\n","93.88174438476562\n","\n","\n","Removed: (5, 8)\n","93.27685546875\n","\n","\n","Removed: (5, 9)\n","95.64862060546875\n","\n","\n","Removed: (5, 10)\n","95.51376342773438\n","\n","\n","Removed: (5, 11)\n","95.57585906982422\n","\n","\n","Removed: (4, 0)\n","95.61215209960938\n","\n","\n","Removed: (4, 1)\n","95.65701293945312\n","\n","\n","Removed: (4, 2)\n","95.63524627685547\n","\n","\n","Removed: (4, 3)\n","95.45549011230469\n","\n","\n","Removed: (4, 4)\n","92.56520080566406\n","\n","\n","Removed: (4, 5)\n","92.56979370117188\n","\n","\n","Removed: (4, 6)\n","92.03364562988281\n","\n","\n","Removed: (4, 7)\n","91.52715301513672\n","\n","\n","Removed: (4, 8)\n","91.91844940185547\n","\n","\n","Removed: (4, 9)\n","91.8843002319336\n","\n","\n","Removed: (4, 10)\n","92.11022186279297\n","\n","\n","Removed: (4, 11)\n","92.14287567138672\n","\n","\n","Removed: (3, 1)\n","92.71060943603516\n","\n","\n","Removed: (3, 2)\n","93.04208374023438\n","\n","\n","Removed: (3, 3)\n","92.2391357421875\n","\n","\n","Removed: (3, 4)\n","92.3480453491211\n","\n","\n","Removed: (3, 5)\n","92.2662353515625\n","\n","\n","Removed: (3, 6)\n","91.98251342773438\n","\n","\n","Removed: (3, 7)\n","92.32189178466797\n","\n","\n","Removed: (3, 8)\n","92.39496612548828\n","\n","\n","Removed: (3, 9)\n","92.14283752441406\n","\n","\n","Removed: (3, 10)\n","92.10672760009766\n","\n","\n","Removed: (3, 11)\n","91.82115173339844\n","\n","\n","Removed: (2, 0)\n","91.95582580566406\n","\n","\n","Removed: (2, 1)\n","92.12678527832031\n","\n","\n","Removed: (2, 2)\n","92.22807312011719\n","\n","\n","Removed: (2, 3)\n","92.40100860595703\n","\n","\n","Removed: (2, 4)\n","92.36933898925781\n","\n","\n","Removed: (2, 5)\n","91.94721984863281\n","\n","\n","Removed: (2, 6)\n","92.21113586425781\n","\n","\n","Removed: (2, 7)\n","92.22827911376953\n","\n","\n","Removed: (2, 8)\n","92.05858612060547\n","\n","\n","Removed: (2, 9)\n","91.83761596679688\n","\n","\n","Removed: (2, 10)\n","91.8608627319336\n","\n","\n","Removed: (2, 11)\n","91.80194854736328\n","\n","\n","Removed: (1, 0)\n","91.98633575439453\n","\n","\n","Removed: (1, 1)\n","92.09611511230469\n","\n","\n","Removed: (1, 2)\n","92.02864074707031\n","\n","\n","Removed: (1, 3)\n","91.8955307006836\n","\n","\n","Removed: (1, 4)\n","91.79586791992188\n","\n","\n","Removed: (1, 6)\n","92.29031372070312\n","\n","\n","Removed: (1, 7)\n","92.61088562011719\n","\n","\n","Removed: (1, 8)\n","92.66000366210938\n","\n","\n","Removed: (1, 9)\n","92.56143951416016\n","\n","\n","Removed: (1, 10)\n","92.52474975585938\n","\n","\n","Removed: (1, 11)\n","93.34406280517578\n","\n","\n","Removed: (0, 0)\n","93.31422424316406\n","\n","\n","Removed: (0, 2)\n","92.29457092285156\n","\n","\n","Removed: (0, 3)\n","92.38253784179688\n","\n","\n","Removed: (0, 4)\n","92.3596420288086\n","\n","\n","Removed: (0, 5)\n","91.86094665527344\n","\n","\n","Removed: (0, 6)\n","91.60896301269531\n","\n","\n","Removed: (0, 7)\n","91.21076965332031\n","\n","\n","Removed: (0, 8)\n","90.02373504638672\n","\n","\n","Removed: (0, 9)\n","90.38639068603516\n","\n","\n","Removed: (0, 11)\n","90.38836669921875\n","\n","\n"]}]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NiB44XkxAzv5","executionInfo":{"status":"ok","timestamp":1698240373235,"user_tz":240,"elapsed":9034,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"08272508-52a7-4269-fbc3-700f66494840"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4319198727607727"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmmKHvBJBJ-x","executionInfo":{"status":"ok","timestamp":1698240451927,"user_tz":240,"elapsed":9333,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"78d65681-12fa-43e2-a88c-1115ee53494b"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["41.46913146972656"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["curr_circuit = [(7, 11), (8, 8), (8, 11), (9, 1)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYeo01ZxBOma","executionInfo":{"status":"ok","timestamp":1698240477377,"user_tz":240,"elapsed":8403,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9a07e8ce-21a8-4231-ac76-a6fb4f2d0852"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["46.679283142089844"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"350HZN9YBipV","executionInfo":{"status":"ok","timestamp":1698240560075,"user_tz":240,"elapsed":6326,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b057e99f-b648-40dd-90d9-7115f57855ac"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.5266976356506348"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (8, 8), (8, 11)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWVTGD2oBnjF","executionInfo":{"status":"ok","timestamp":1698240583427,"user_tz":240,"elapsed":7955,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"64adb946-8ea4-4bd4-b31a-c280ce47b411"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35.77225112915039"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["curr_circuit = [(9, 1)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gBDc4H8CQZZ","executionInfo":{"status":"ok","timestamp":1698240741071,"user_tz":240,"elapsed":8429,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cff429ff-d1f4-4875-fc29-2f92e8c8e797"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["29.699447631835938"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)]\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5H0PZBzA-yH","executionInfo":{"status":"ok","timestamp":1698240433842,"user_tz":240,"elapsed":5882,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9bb408f8-f11d-4ded-f930-f4124a8dd3de"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["90.38836669921875"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        \"name mover 4\": lst,\n","        \"name mover 3\": lst,\n","        \"name mover 2\": lst,\n","        \"name mover 1\": lst,\n","        \"name mover 0\": lst,\n","        \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698240295584,"user_tz":240,"elapsed":6489,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"87d8cded-61c9-4130-a216-8405646f41c3","id":"LIdwBKj7-rga"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["91.82762908935547"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCQHY4f8DnYS","executionInfo":{"status":"ok","timestamp":1698241110957,"user_tz":240,"elapsed":8575,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7e3083f3-2b85-4154-9725-cb5a974908b8"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["88.11691284179688"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["## ablate this circ on random words"],"metadata":{"id":"3zmrzqDEEb-A"}},{"cell_type":"code","source":["pos_dict = {\n","    'table': 0,\n","    'lamp': 2,\n","    'pencil': 4,\n","    'hat': 6,\n","    'run': 8,\n","    'S1': 1,\n","    'S2': 3,\n","    'S3': 5,\n","    'S4': 7,\n","}"],"metadata":{"id":"Q_jxBcvvFT04","executionInfo":{"status":"ok","timestamp":1698242118345,"user_tz":240,"elapsed":483,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'table': 'table',\n","            'lamp': 'lamp',\n","            'pencil': 'pencil',\n","            'hat': 'hat',\n","            'run': 'run',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'corr': str(i+4),\n","            'incorr': str(i+3),\n","            'text': f\"table {i} Bob {i+1} pencil {i+2} hat {i+3} run\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list, pos_dict\n","\n","prompts_list, pos_dict = generate_prompts_list(1, 11)\n","dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698242122163,"user_tz":240,"elapsed":362,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"jU8SbjTvFJ0D"},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'table': 'table',\n","            'lamp': 'lamp',\n","            'pencil': 'pencil',\n","            'hat': 'hat',\n","            'run': 'run',\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'corr': str(i+3),\n","            'incorr': str(i+4),\n","            'text': f\"table {i} Bob {i+1} pencil {i+2} hat {i+2} run\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698242125916,"user_tz":240,"elapsed":16,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"id":"jGZZMIIgFJ0D"},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","\n","def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"name mover 4\": \"Eve\",\n","        \"name mover 3\": \"Don\",\n","        \"name mover 2\": \"Claire\",\n","        \"name mover 1\": \"Bob\",\n","        \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","mean_ablate_by_lst(curr_circuit, model, dataset, dataset_2, print_output=False).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ew-gfdj8HoHy","executionInfo":{"status":"ok","timestamp":1698242145335,"user_tz":240,"elapsed":5570,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"63a923b5-5a88-45fe-abe4-66dea05eed7b"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["94.33404541015625"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["This works just as well; actually, it works better!"],"metadata":{"id":"mqgQB4xFHrM1"}},{"cell_type":"markdown","source":["## try running new circ on pure digits"],"metadata":{"id":"QiTC3J7QHvos"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.io_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S5\"])[0] for prompt in self.prompts\n","        ]\n","        self.s_tokenIDs = [\n","            self.tokenizer.encode(\" \" + prompt[\"S4\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'S5')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                    target_token = prompt[targ]\n","                else:\n","                    target_token = \"Ġ\" + prompt[targ]\n","                target_index = tokens.index(target_token)\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"ZZyycV3AHvo2","executionInfo":{"status":"ok","timestamp":1698253716661,"user_tz":240,"elapsed":365,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'S5': str(i+4),\n","            'text': f\"{i} {i+1} {i+2} {i+3}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 11)\n","pureDataset = Dataset(prompts_list, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"OutX3js6Hvo2","executionInfo":{"status":"ok","timestamp":1698253720368,"user_tz":240,"elapsed":287,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+2),\n","            'S5': str(i+3),\n","            'text': f\"{i} {i+1} {i+2} {i+2}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list_2 = generate_prompts_list_corr(1, 11)\n","pureDataset_2 = Dataset(prompts_list_2, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"o9BO1IPGHvo2","executionInfo":{"status":"ok","timestamp":1698253722205,"user_tz":240,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)\n","model_abl = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=pureDataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","ioi_logits_minimal = model_abl(pureDataset.toks)\n","logits_to_ave_logit_diff_2(ioi_logits_minimal, pureDataset)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1698242757035,"user_tz":240,"elapsed":7437,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"_Yrc9RbjJ6L6","outputId":"417da179-2d4e-4888-d67f-66936e7be483"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4.6238, device='cuda:0')"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        # \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover\": lst,\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"x0LX8gAUHvo3","executionInfo":{"status":"ok","timestamp":1698242894050,"user_tz":240,"elapsed":415,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698242943468,"user_tz":240,"elapsed":3280,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9a22161e-c77a-41b2-a4ef-209f27a6ecd4","id":"FwLctRgHHvo3"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): -0.0635\n","Average logit difference (circuit / full) %: -1.3728\n"]},{"output_type":"execute_result","data":{"text/plain":["-1.3727796077728271"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","source":["For some reason, this is really bad. It appears the early tokens are needed for positions??"],"metadata":{"id":"Do3Niw0yH9-Q"}},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6, 4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYAnr6pjIIaT","executionInfo":{"status":"ok","timestamp":1698243021027,"user_tz":240,"elapsed":4175,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2948c96b-4ad0-4313-ed1b-18c820aa21c5"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 1.2746\n","Average logit difference (circuit / full) %: 27.5656\n"]},{"output_type":"execute_result","data":{"text/plain":["27.565563201904297"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["curr_circuit = [(0, 1), (0, 8), (0, 10), (1, 5), (3, 0), (4, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 6), (6, 10), (8, 8), (8, 11), (9, 1), (10, 7)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmwicq6rIeHD","executionInfo":{"status":"ok","timestamp":1698242926162,"user_tz":240,"elapsed":3208,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9f64e242-b06f-4aa4-f269-4af3413aab80"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 1.1478\n","Average logit difference (circuit / full) %: 24.8250\n"]},{"output_type":"execute_result","data":{"text/plain":["24.8249568939209"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kmMOPv-I8By","executionInfo":{"status":"ok","timestamp":1698242911929,"user_tz":240,"elapsed":4335,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b4f8acbd-73b6-4e33-8501-52ba136fa780"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 4.6238\n","Average logit difference (circuit / full) %: 100.0000\n"]},{"output_type":"execute_result","data":{"text/plain":["100.0"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        # \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover\": lst,\n","        # \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6, 4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)]\n","# curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhDyDbkHMGry","executionInfo":{"status":"ok","timestamp":1698243701427,"user_tz":240,"elapsed":3631,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e17b6643-9cce-4327-ad1c-ec5ea53a3715"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 1.2746\n","Average logit difference (circuit / full) %: 27.5656\n"]},{"output_type":"execute_result","data":{"text/plain":["27.565563201904297"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        \"number mover 4\": lst,\n","        \"number mover 3\": lst,\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Mua5SAhLdJi","executionInfo":{"status":"ok","timestamp":1698243169274,"user_tz":240,"elapsed":6932,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9569f224-26d8-4d1b-d2cd-6da98d52dd4a"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): -0.2203\n","Average logit difference (circuit / full) %: -4.7644\n"]},{"output_type":"execute_result","data":{"text/plain":["-4.764449119567871"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": lst,\n","        \"number mover 1\": lst,\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23keeC-HLqIk","executionInfo":{"status":"ok","timestamp":1698243263593,"user_tz":240,"elapsed":5419,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"342ac2da-070e-4d9c-f2a0-1f7d7724ae30"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): -0.2194\n","Average logit difference (circuit / full) %: -4.7443\n"]},{"output_type":"execute_result","data":{"text/plain":["-4.744296073913574"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","            # recall num mover 4 has WRONG INDEX due to repeat getting 3rd instead of last pos; this means last pos just keeps num mover\n","        \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVS9xnLlL8fE","executionInfo":{"status":"ok","timestamp":1698243790099,"user_tz":240,"elapsed":3692,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b5dde452-325e-4e04-c76b-ce46650526eb"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): -0.2193\n","Average logit difference (circuit / full) %: -4.7424\n"]},{"output_type":"execute_result","data":{"text/plain":["-4.7424397468566895"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        # \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYqRvu3RN0_I","executionInfo":{"status":"ok","timestamp":1698243933368,"user_tz":240,"elapsed":4789,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2ca63c54-48f6-4614-d95b-dfcad32536de"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 1.2746\n","Average logit difference (circuit / full) %: 27.5656\n"]},{"output_type":"execute_result","data":{"text/plain":["27.565563201904297"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        # \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","        \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2MAY7QxOgV6","executionInfo":{"status":"ok","timestamp":1698243970375,"user_tz":240,"elapsed":6059,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2a48fc0f-ad7e-4f12-db28-6b6c92780b69"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 1.2743\n","Average logit difference (circuit / full) %: 27.5606\n"]},{"output_type":"execute_result","data":{"text/plain":["27.56063461303711"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        # \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","        \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"369VEdDtOsNz","executionInfo":{"status":"ok","timestamp":1698244004939,"user_tz":240,"elapsed":6461,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3715e553-d30d-471a-a89c-6553b22abae1"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): -0.2194\n","Average logit difference (circuit / full) %: -4.7443\n"]},{"output_type":"execute_result","data":{"text/plain":["-4.744296073913574"]},"metadata":{},"execution_count":96}]},{"cell_type":"markdown","source":["So all the early heads are still impt for the end pos, unlike in among words"],"metadata":{"id":"xtqMnMxcOzTC"}},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        # \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","        \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhTV99FOO3gu","executionInfo":{"status":"ok","timestamp":1698244055259,"user_tz":240,"elapsed":3904,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"841e1b97-3a89-4a1e-c854-dc50c2e21679"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): -0.2020\n","Average logit difference (circuit / full) %: -4.3693\n"]},{"output_type":"execute_result","data":{"text/plain":["-4.369297504425049"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        # \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","        \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w8VxI1ViO_iT","executionInfo":{"status":"ok","timestamp":1698244110996,"user_tz":240,"elapsed":4037,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"76cbb189-c581-4047-f7da-c6756f210438"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 0.8827\n","Average logit difference (circuit / full) %: 19.0896\n"]},{"output_type":"execute_result","data":{"text/plain":["19.089550018310547"]},"metadata":{},"execution_count":100}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(0, 1), (0, 10), (1, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        # \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","        \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXCUjE8EPOae","executionInfo":{"status":"ok","timestamp":1698244151857,"user_tz":240,"elapsed":6272,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c1bb1c9a-7e81-4274-9fd1-1345a11f0773"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 0.3068\n","Average logit difference (circuit / full) %: 6.6353\n"]},{"output_type":"execute_result","data":{"text/plain":["6.63534688949585"]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        # \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","        \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MikKfwSPPWvQ","executionInfo":{"status":"ok","timestamp":1698244176874,"user_tz":240,"elapsed":4204,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3e12d58d-df11-47bb-a2b5-5488c6d787c2"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 0.5878\n","Average logit difference (circuit / full) %: 12.7116\n"]},{"output_type":"execute_result","data":{"text/plain":["12.7116117477417"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, dataset, dataset_2, print_output=True):\n","    CIRCUIT = {\n","        # \"name mover 4\": lst,\n","        # \"name mover 3\": lst,\n","        # \"name mover 2\": lst,\n","        # \"name mover 1\": lst,\n","        # \"name mover 0\": lst,\n","        \"number mover\": [(3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover\": lst,\n","        # \"number mover 4\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        \"number mover 3\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10), (7, 11), (8, 8), (8, 11), (9, 1)],\n","        # \"number mover 2\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","        # \"number mover 1\": [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        # \"name mover 4\": \"Eve\",\n","        # \"name mover 3\": \"Don\",\n","        # \"name mover 2\": \"Claire\",\n","        # \"name mover 1\": \"Bob\",\n","        # \"name mover 0\": \"Adam\",\n","        \"number mover\": \"end\",\n","        # \"number mover 4\": \"S4\",\n","        \"number mover 3\": \"S3\",\n","        \"number mover 2\": \"S2\",\n","        \"number mover 1\": \"S1\",\n","    }\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score\n","\n","curr_circuit = [(0, 1), (0, 10), (1, 5), (3, 0), (5, 5), (6, 2), (6,4), (6, 6), (6, 10)]\n","mean_ablate_by_lst(curr_circuit, model, pureDataset, pureDataset_2, print_output=True).item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sExexXkiP4bU","executionInfo":{"status":"ok","timestamp":1698253737495,"user_tz":240,"elapsed":6020,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6190d591-4396-4669-98bc-9866fb3d7e33"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (IOI dataset, using entire model): 4.6238\n","Average logit difference (IOI dataset, only using circuit): 0.5761\n","Average logit difference (circuit / full) %: 12.4591\n"]},{"output_type":"execute_result","data":{"text/plain":["12.459147453308105"]},"metadata":{},"execution_count":19}]}]}