{"cells":[{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSKP_OsTDki6"},"outputs":[],"source":["save_files = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1wsEy0MqHU0"},"outputs":[],"source":["%%capture\n","%pip install git+https://github.com/neelnanda-io/TransformerLens.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6b1n2tvIAiD"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","# import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML\n","\n","import pickle\n","from google.colab import files\n","\n","import matplotlib.pyplot as plt\n","import statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuhzYxbsIAiE"},"outputs":[],"source":["import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer #, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFMTUcQiIAiF"},"outputs":[],"source":["torch.set_grad_enabled(False)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQr6WtEppHgy"},"outputs":[],"source":["import pdb"]},{"cell_type":"markdown","metadata":{"id":"Z4iJEGh6b56v"},"source":["## Import functions from repo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2377,"status":"ok","timestamp":1718309360133,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"F8TXMRL3CoPd","outputId":"15a97265-d566-44b1-b6ae-db87fe0f7d13"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'seqcont_circuits'...\n","remote: Enumerating objects: 1022, done.\u001b[K\n","remote: Counting objects: 100% (488/488), done.\u001b[K\n","remote: Compressing objects: 100% (287/287), done.\u001b[K\n","remote: Total 1022 (delta 296), reused 378 (delta 190), pack-reused 534\u001b[K\n","Receiving objects: 100% (1022/1022), 18.76 MiB | 20.81 MiB/s, done.\n","Resolving deltas: 100% (659/659), done.\n","/content/seqcont_circuits/src/iter_node_pruning\n"]}],"source":["!git clone https://github.com/apartresearch/seqcont_circuits.git\n","%cd /content/seqcont_circuits/src/iter_node_pruning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22TI4zjMDMfQ"},"outputs":[],"source":["## comment this out when debugging functions in colab to use funcs defined in colab\n","\n","# don't improt this\n","# # from dataset import Dataset\n","\n","from metrics import *\n","from head_ablation_fns import *\n","from mlp_ablation_fns import *\n","from node_ablation_fns import *\n","from loop_node_ablation_fns import *"]},{"cell_type":"markdown","metadata":{"id":"9R_g1Ghv7cGE"},"source":["## fns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsJmCq-C2Zu6"},"outputs":[],"source":["import random\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPjHv-Xny4R"},"outputs":[],"source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer):  # , S1_is_first=False\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.corr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.incorr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a dict whose values are tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        # for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","        for targ in [key for key in pos_dict]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = self.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZKVG778QYyn"},"outputs":[],"source":["def generate_prompts_list_longer(text, tokens):\n","    prompts_list = []\n","    prompt_dict = {\n","        'corr': str(1),\n","        'incorr': str(2),\n","        'text': text\n","        # 'text': model.to_string(tokens)[0]\n","        }\n","    tokens_as_strs = model.tokenizer.tokenize(text)\n","    # tokens_as_strs = model.to_string(tokens)[0].split()\n","    # for i in range(tokens.shape[1]):\n","    for i, tok in enumerate(tokens_as_strs):\n","        prompt_dict['S'+str(i)] = tok\n","    # for i, tok in enumerate(tokens):\n","    #     prompt_dict['S'+str(i)] = model.to_string(tok)\n","\n","    # prompt_dict = {\n","    #     'corr': '4',\n","    #     'incorr': '3',\n","    #     'text': model.to_string(tokens)[0]\n","    # }\n","    # # list_tokens = tokenizer.tokenize('1 2 3 ')\n","    # tokens_as_strs = model.to_string(tokens)[0].split()\n","    # for i, tok_as_str in enumerate(tokens_as_strs):\n","    #     if tok_as_str == '▁':\n","    #         prompt_dict['S'+str(i)] = ' '\n","    #     else:\n","    #         prompt_dict['S'+str(i)] = tok_as_str\n","    prompts_list.append(prompt_dict)\n","    return prompts_list"]},{"cell_type":"markdown","metadata":{"id":"PDP2cpaiZpPX"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGCiZUPpJUsD"},"outputs":[],"source":["from transformers import LlamaForCausalLM, LlamaTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5816,"status":"ok","timestamp":1718309409629,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"-CocJpgjsf_M","outputId":"1809cc87-55ad-4746-9b7e-535393656bc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65100,"status":"ok","timestamp":1718309474714,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"xLgpia0tI6O8","outputId":"45c784ac-a70f-49ab-f21c-694a5f734539"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"269ea6b9ba594cfca4546d2e424d2bbc","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9778d8fa241941b4abe82976288d02ef","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b62317bd4cad467691182f0ff629aaa4","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8acdc83177a49c08e1001c3f263f61b","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e1cbfb8c59c4b0eb180067a99be97df","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7232c4ada59847a6a1281dd6063575a7","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"484f3ceac07d4eba9cbee4a3cdb3d816","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7eabbb884ccc44819e1ffb57a350b6ec","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66be565f780b4e00ac8ba45317a1a202","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84e4efe93c5b4cd9ab798c955ffdd9b5","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd54db05c45244ffadb7b784c190e6cc","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["LLAMA_2_7B_CHAT_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","tokenizer = LlamaTokenizer.from_pretrained(LLAMA_2_7B_CHAT_PATH)\n","# tokenizer = LlamaTokenizer.from_pretrained(LLAMA_2_7B_CHAT_PATH, use_fast= False, add_prefix_space= False)\n","hf_model = LlamaForCausalLM.from_pretrained(LLAMA_2_7B_CHAT_PATH, low_cpu_mem_usage=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rtZ2e3sMY5S"},"outputs":[],"source":["import transformer_lens.utils as utils\n","from transformer_lens.hook_points import HookPoint\n","from transformer_lens import HookedTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29377,"status":"ok","timestamp":1718309503166,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"sUnSHvA-Myx8","outputId":"090d970d-fbf9-41dd-b98a-fb8bd66a050b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n","Moving model to device:  cuda\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    LLAMA_2_7B_CHAT_PATH,\n","    hf_model = hf_model,\n","    tokenizer = tokenizer,\n","    device = \"cpu\",\n","    fold_ln = False,\n","    center_writing_weights = False,\n","    center_unembed = False,\n",")\n","\n","del hf_model\n","\n","model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"dsdvChbcvgp5"},"source":["# new ablation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KlWYoEy72Cf"},"outputs":[],"source":["def get_heads_actv_mean(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Output: The mean activations of a head's output\n","    '''\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"z\"),\n","    )\n","    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n","\n","    # for layer in range(model.cfg.n_layers):\n","    #     z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n","    #     for template_group in means_dataset.groups:\n","    #         z_for_this_template = z_for_this_layer[template_group]\n","    #         z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n","    #         if z_means_for_this_template.shape[0] == 5:\n","    #             pdb.set_trace()\n","    #         means[layer, template_group] = z_means_for_this_template\n","\n","    del(means_cache)\n","\n","    return means"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFDQMOt9CyVw"},"outputs":[],"source":["# def mask_circ_heads(\n","#     means_dataset: Dataset,\n","#     model: HookedTransformer,\n","#     circuit: Dict[str, List[Tuple[int, int]]],\n","#     seq_pos_to_keep: Dict[str, str],\n","# ) -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","#     '''\n","#     Output: for each layer, a mask of circuit components that should not be ablated\n","#     '''\n","#     heads_and_posns_to_keep = {}\n","#     batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","\n","#     for layer in range(model.cfg.n_layers):\n","\n","#         mask = t.zeros(size=(batch, seq, n_heads))\n","\n","#         for (head_type, head_list) in circuit.items():\n","#             seq_pos = seq_pos_to_keep[head_type]\n","#             # if seq_pos == 'S7':\n","#             #     pdb.set_trace()\n","#             indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","#             for (layer_idx, head_idx) in head_list:\n","#                 if layer_idx == layer:\n","#                     # if indices.item() == 7:\n","#                     #     pdb.set_trace()\n","#                     mask[:, indices, head_idx] = 1\n","#                     # mask[:, :, head_idx] = 1  # keep L.H at all pos\n","\n","#         heads_and_posns_to_keep[layer] = mask.bool()\n","#     # pdb.set_trace()\n","#     return heads_and_posns_to_keep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1boH1469_HI"},"outputs":[],"source":["def mask_circ_heads(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","    '''\n","    Output: for each layer, a mask of circuit components that should not be ablated\n","    '''\n","    heads_and_posns_to_keep = {}\n","    # batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","    batch, seq, n_heads = len(means_dataset), len(circuit.keys()), model.cfg.n_heads\n","    # print(seq)\n","\n","    for layer in range(model.cfg.n_layers):\n","\n","        mask = t.zeros(size=(batch, seq, n_heads))\n","\n","        for (head_type, head_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[head_type]\n","            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","            for (layer_idx, head_idx) in head_list:\n","                if layer_idx == layer:\n","                    # mask[:, indices, head_idx] = 1\n","                    mask[:, :, head_idx] = 1\n","\n","        heads_and_posns_to_keep[layer] = mask.bool()\n","\n","    return heads_and_posns_to_keep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdxeNJ5C9tHx"},"outputs":[],"source":["def hook_func_mask_head(\n","    z: Float[Tensor, \"batch seq head d_head\"],\n","    hook: HookPoint,\n","    # components_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n","    # means: Float[Tensor, \"layer batch seq head d_head\"],\n","    circuit: Dict[str, List[Tuple[int, int]]],\n",") -> Float[Tensor, \"batch seq head d_head\"]:\n","    '''\n","    Use this to not mask components\n","    '''\n","    # mask_for_this_layer = components_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n","    # z = t.where(mask_for_this_layer, z, means[hook.layer()])\n","\n","    ###\n","    # heads_and_posns_to_keep = {}\n","    # batch, seq, n_heads = z.shape[0], z.shape[1], model.cfg.n_heads  # components_to_keep[0].shape[0] is batch\n","\n","    # for layer in range(model.cfg.n_layers):\n","\n","    #     mask = t.zeros(size=(batch, seq, n_heads))\n","\n","    #     for (head_type, head_list) in circuit.items():\n","    #         # seq_pos = seq_pos_to_keep[head_type]\n","    #         # indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","    #         for (layer_idx, head_idx) in head_list:\n","    #             if layer_idx == layer:\n","    #                 # mask[:, indices, head_idx] = 1\n","    #                 mask[:, :, head_idx] = 1\n","\n","    #     heads_and_posns_to_keep[layer] = mask.bool()\n","    ###\n","    mask_for_this_layer = t.zeros(size=(z.shape[0], z.shape[1], z.shape[2]))\n","    for (head_type, head_list) in circuit.items():\n","        # seq_pos = seq_pos_to_keep[head_type]\n","        # indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","        for (layer_idx, head_idx) in head_list:\n","            if layer_idx == hook.layer():\n","                # mask[:, indices, head_idx] = 1\n","                mask_for_this_layer[:, :, head_idx] = 1\n","\n","    mask_for_this_layer = mask_for_this_layer.bool()\n","    mask_for_this_layer = mask_for_this_layer.unsqueeze(-1).to(z.device)  # d_model is 1; then is broadcast in where\n","\n","    z = t.where(mask_for_this_layer, z, 0)\n","\n","    return z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dg3XuWScAVvG"},"outputs":[],"source":["def add_ablation_hook_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Ablate the model, except as components and positions to keep\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","    means = get_heads_actv_mean(means_dataset, model)\n","    components_to_keep = mask_circ_heads(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    hook_fn = partial(\n","        hook_func_mask_head,\n","        # components_to_keep=components_to_keep,\n","        # means=means,\n","        circuit=circuit,\n","    )\n","\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ILjxwH9YUYP"},"outputs":[],"source":["# from dataset import Dataset\n","from transformer_lens import HookedTransformer, utils\n","from transformer_lens.hook_points import HookPoint\n","import einops\n","from functools import partial\n","import torch as t\n","from torch import Tensor\n","from typing import Dict, Tuple, List\n","from jaxtyping import Float, Bool\n","\n","# from head_ablation_fns import *\n","# from mlp_ablation_fns import *\n","\n","def add_ablation_hook_MLP_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    heads_lst, mlp_lst,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    # for i in range(len(model.tokenizer.tokenize(means_dataset.prompts[0]['text']))):\n","    num_pos = len(model.tokenizer(means_dataset.prompts[0]['text']).input_ids)\n","    for i in range(num_pos ):\n","        CIRCUIT['S'+str(i)] = heads_lst\n","        # if i == len(model.tokenizer.tokenize(means_dataset.prompts[0]['text'])) - 1:\n","        # if i == num_pos - 1:\n","        #     SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        # else:\n","        SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = get_heads_actv_mean(means_dataset, model)\n","    # Convert this into a boolean map\n","    components_to_keep = mask_circ_heads(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_func_mask_head,\n","        # components_to_keep=components_to_keep,\n","        # means=means,\n","        circuit=CIRCUIT,\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    # if all_entries_true(components_to_keep) == False:\n","    #     pdb.set_trace()\n","    ########################\n","    # CIRCUIT = {}\n","    # SEQ_POS_TO_KEEP = {}\n","    # # for i in range(len(model.tokenizer.tokenize(means_dataset.prompts[0]['text']))):\n","    # num_pos = len(model.tokenizer(means_dataset.prompts[0]['text']).input_ids)\n","    # for i in range(num_pos ):\n","    #     CIRCUIT['S'+str(i)] = mlp_lst\n","    #     # if i == len(model.tokenizer.tokenize(means_dataset.prompts[0]['text'])) - 1:\n","    #     # if i == num_pos - 1:\n","    #     #     SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","    #     # else:\n","    #     SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    # # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    # means = get_MLPs_actv_mean(means_dataset, model)\n","\n","    # # Convert this into a boolean map\n","    # components_to_keep = mask_circ_MLPs(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # # Get a hook function which will patch in the mean z values for each head, at\n","    # # all positions which aren't important for the circuit\n","    # hook_fn = partial(\n","    #     hook_func_mask_mlp_out,\n","    #     components_to_keep=components_to_keep,\n","    #     means=means\n","    # )\n","\n","    # model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-YuOEDieLgE"},"outputs":[],"source":["def all_entries_true(tensor_dict):\n","    for key, tensor in tensor_dict.items():\n","        if not torch.all(tensor).item():\n","            return False\n","    return True"]},{"cell_type":"markdown","metadata":{"id":"jtaV1q3SBHow"},"source":["# ablation fns mult tok answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgbtY5fFPb71"},"outputs":[],"source":["def clean_gen(model, clean_text, corr_ans):\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    tokens = model.to_tokens(clean_text).to(device)\n","    # tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","\n","    total_score = 0\n","    corr_ans_tokLen = 0\n","    ans_so_far = ''\n","    # while True:\n","    for i in range(10):\n","        # print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","        logits = model(tokens)\n","        next_token = logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","        next_char = model.to_string(next_token)\n","\n","        corr_logits = logits[:, -1, next_token]\n","        total_score += corr_logits\n","        # print(f\"logit diff of new char: {corr_logits}\")\n","\n","        ans_so_far += next_char\n","        corr_ans_tokLen += 1\n","        # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","        if ans_so_far == corr_ans:\n","            # print('\\nTotal logit diff: ', total_score.item())\n","            break\n","\n","        # Define new input sequence, by appending the previously generated token\n","        tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","        # if next_char == '':\n","        #     next_char = ' '\n","        # clean_text = clean_text + next_char\n","        # tokens = model.to_tokens(clean_text).to(device)\n","    # return corr_ans_tokLen, total_score\n","    return total_score.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lp4MyZ52cUTK"},"outputs":[],"source":["def ablate_then_gen(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, corr_ans_tokLen):\n","    tokens = model.to_tokens(clean_text).to(device)\n","    prompts_list = generate_prompts_list_longer(clean_text, tokens)\n","\n","    corr_tokens = model.to_tokens(corr_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    pos_dict = {}\n","    num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","    for i in range(num_pos ):\n","        pos_dict['S'+str(i)] = i\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    logits = model(tokens)\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    next_char = model.to_string(next_token)\n","\n","    total_score = 0\n","\n","    for i in range(corr_ans_tokLen):\n","        if next_char == '':\n","            next_char = ' '\n","\n","        clean_text = clean_text + next_char\n","        if i == corr_ans_tokLen - 1:\n","            print(model.to_string(tokens))\n","            # print(f\"Sequence so far: {clean_text}\")\n","            # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","        tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","\n","        # get new ablation dataset\n","        # model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","        # corr_text = corr_text + next_char\n","        # corr_tokens = torch.cat([corr_tokens, next_token[None, None]], dim=-1)\n","        # prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","        # pos_dict = {}\n","        # num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","        # for i in range(num_pos ):\n","        #     pos_dict['S'+str(i)] = i\n","\n","        # dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, corr_tokens)\n","\n","        # model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","        logits = model(tokens)\n","        next_token = logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","        next_char = model.to_string(next_token)\n","\n","        # new_score = get_logit_diff(logits, dataset)\n","        # total_score += new_score\n","        # print(f\"corr logit of new char: {new_score}\")\n","    # print('\\n Total corr logit: ', total_score.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8KYb2BBSm-G"},"outputs":[],"source":["# Function to randomly choose 50 pairs ensuring less than 10 overlap with heads_of_circ\n","def choose_heads_to_remove(filtered_pairs, heads_of_circ, num_pairs=50, max_overlap=10):\n","    while True:\n","        head_to_remove = random.sample(filtered_pairs, num_pairs)\n","        overlap_count = len([head for head in head_to_remove if head in heads_of_circ])\n","        if overlap_count < max_overlap:\n","            return head_to_remove"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":400,"status":"ok","timestamp":1718309503308,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"q6YOoSz9Pp7y","outputId":"321ff721-5cf1-4fc1-b64b-50e9d5401033"},"outputs":[{"data":{"text/plain":["['1', '0']"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["answer_str = '10'\n","ans_str_tok = tokenizer.tokenize(answer_str)[1:]\n","ans_str_tok"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":398,"status":"ok","timestamp":1718309503308,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"wjmOYAfpPb6o","outputId":"15be3c61-8cf9-4a35-99ec-0b816649b898"},"outputs":[{"data":{"text/plain":["[29896, 29900]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["corr_tokenIDs = []\n","for ansPos in range(len(ans_str_tok)):\n","    # ansPos_corrTokIDS = [] # this is the inner list. each member is a promptID\n","    # for promptID in range(len(self.prompts)):\n","    #     tokID = self.tokenizer.encode(self.prompts[promptID]['corr'][ansPos])[2:][0] # 2: to skip padding <s> and ''\n","    #     ansPos_corrTokIDS.append(tokID)\n","    # self.corr_tokenIDs.append(ansPos_corrTokIDS)\n","\n","    tokID = model.tokenizer.encode(ans_str_tok[ansPos])[2:][0] # 2: to skip padding <s> and ''\n","    corr_tokenIDs.append(tokID)\n","corr_tokenIDs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWyYbvi3Mout"},"outputs":[],"source":["def ablate_auto_score(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans):\n","    tokens = model.to_tokens(clean_text).to(device)\n","    prompts_list = generate_prompts_list_longer(clean_text, tokens)\n","\n","    corr_tokens = model.to_tokens(corr_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    pos_dict = {}\n","    num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","    for i in range(num_pos ):\n","        pos_dict['S'+str(i)] = i\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    # logits = model(tokens)\n","    # next_token = logits[0, -1].argmax(dim=-1)\n","    # next_char = model.to_string(next_token)\n","\n","    total_score = 0\n","    ans_so_far = ''\n","    ans_str_tok = tokenizer.tokenize(correct_ans)[1:] # correct_ans is str\n","    corr_tokenIDs = []\n","    for correct_ansPos in range(len(ans_str_tok)):\n","        tokID = model.tokenizer.encode(ans_str_tok[correct_ansPos])[2:][0] # 2: to skip padding <s> and ''\n","        corr_tokenIDs.append(tokID)\n","    correct_ans_tokLen = len(corr_tokenIDs)\n","    for ansPos in range(correct_ans_tokLen):\n","        # if next_char == '':\n","        #     next_char = ' '\n","\n","        # clean_text = clean_text + next_char\n","        # if i == correct_ans_tokLen - 1:\n","        #     print(model.to_string(tokens))\n","        #     # print(f\"Sequence so far: {clean_text}\")\n","        #     # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","        # tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","\n","        # get new ablation dataset\n","        # model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","        # corr_text = corr_text + next_char\n","        # corr_tokens = torch.cat([corr_tokens, next_token[None, None]], dim=-1)\n","        # prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","        # pos_dict = {}\n","        # num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","        # for i in range(num_pos ):\n","        #     pos_dict['S'+str(i)] = i\n","\n","        # dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, corr_tokens)\n","\n","        # model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","        logits = model(tokens)\n","        next_token = logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","        next_char = model.to_string(next_token)\n","\n","        if next_char == '':\n","            next_char = ' '\n","\n","        clean_text = clean_text + next_char\n","        # if i == correct_ans_tokLen - 1:\n","            # print(model.to_string(tokens))\n","            # print(f\"Sequence so far: {clean_text}\")\n","            # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","        tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","\n","        ans_so_far += next_char\n","        correct_ans_tokLen += 1\n","        # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","        ansTok_IDs = torch.tensor(corr_tokenIDs[ansPos])\n","\n","        # new_score = get_logit_diff(logits, dataset)\n","        # total_score += new_score\n","        # corrTok_logits = logits[:, -1, next_token]\n","        corrTok_logits = logits[range(logits.size(0)), -1, ansTok_IDs]  # not next_token, as that's what's pred, not the token to measure\n","        # pdb.set_trace()\n","        total_score += corrTok_logits\n","        # print(f\"corr logit of new char: {new_score}\")\n","    # print('\\n Total corr logit: ', total_score.item())\n","    # return ans_so_far, total_score.item()\n","    return ans_so_far"]},{"cell_type":"markdown","metadata":{"id":"JPKiYdKTAMni"},"source":["# Define circs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1718309503309,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"YId1M9rIroEe","outputId":"1dcd0c5c-d16d-427f-cf59-14296f642644"},"outputs":[{"data":{"text/plain":["84"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# from Llama2_numerals_1to10.ipynb\n","nums_1to9 = [(0, 2), (0, 5), (0, 6), (0, 15), (1, 15), (1, 28), (2, 13), (2, 24), (3, 24), (4, 3), (4, 16), (5, 11), (5, 13), (5, 15), (5, 16), (5, 23), (5, 25), (5, 27), (6, 11), (6, 14), (6, 20), (6, 23), (6, 24), (6, 26), (6, 28), (6, 30), (6, 31), (7, 0), (7, 13), (7, 21), (7, 30), (8, 0), (8, 2), (8, 12), (8, 15), (8, 26), (8, 27), (8, 30), (8, 31), (9, 15), (9, 16), (9, 23), (9, 26), (9, 27), (9, 29), (9, 31), (10, 1), (10, 13), (10, 18), (10, 23), (10, 29), (11, 7), (11, 8), (11, 9), (11, 17), (11, 18), (11, 25), (11, 28), (12, 18), (12, 19), (12, 23), (12, 27), (13, 6), (13, 11), (13, 20), (14, 18), (14, 19), (14, 20), (14, 21), (16, 0), (18, 19), (18, 21), (18, 25), (18, 26), (18, 31), (19, 28), (20, 17), (21, 0), (21, 2), (22, 18), (22, 20), (22, 25), (23, 27), (26, 2)]\n","len(nums_1to9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1718309503309,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"wzTeD-OvAOwC","outputId":"89d7b04a-18a9-4570-e3d7-e54db59ac514"},"outputs":[{"data":{"text/plain":["82"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# nw_circ = [(0, 1), (0, 4), (0, 6), (0, 7), (0, 8), (0, 10), (0, 11), (0, 12), (1, 16), (1, 24), (1, 27), (1, 28), (2, 2), (2, 5), (2, 8), (2, 24), (2, 30), (3, 7), (3, 14), (3, 19), (3, 23), (4, 3), (5, 16), (5, 25), (6, 11), (6, 14), (7, 0), (7, 30), (8, 0), (8, 2), (8, 3), (8, 4), (8, 6), (8, 21), (8, 31), (9, 1), (9, 3), (9, 7), (9, 11), (9, 29), (9, 31), (10, 13), (10, 18), (10, 23), (10, 24), (10, 25), (10, 27), (11, 18), (11, 28), (12, 18), (12, 26), (13, 11), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 23), (14, 7), (14, 14), (15, 25), (15, 28), (16, 0), (16, 12), (16, 14), (16, 15), (16, 16), (16, 19), (16, 24), (16, 29), (17, 17), (17, 23), (17, 31), (18, 31), (19, 12), (20, 17), (27, 20), (27, 25), (27, 27), (27, 31), (28, 5), (29, 5)]\n","# in order from most impt to least based on how much changes perf when ablated\n","nw_circ = [(20, 17), (5, 25), (16, 0), (29, 5), (3, 19), (6, 11), (15, 25), (8, 0), (16, 24), (8, 4), (7, 0), (6, 14), (16, 29), (5, 16), (12, 26), (4, 3), (3, 7), (7, 30), (11, 28), (28, 5), (17, 31), (13, 11), (13, 20), (12, 18), (1, 27), (10, 13), (18, 31), (8, 6), (9, 1), (0, 4), (2, 2), (9, 11), (19, 12), (1, 16), (13, 17), (9, 7), (11, 18), (2, 24), (10, 18), (9, 31), (9, 29), (2, 30), (2, 5), (1, 24), (2, 8), (15, 28), (27, 31), (16, 14), (3, 23), (3, 14), (10, 23), (27, 20), (8, 3), (14, 7), (14, 14), (16, 15), (8, 2), (17, 17), (0, 1), (10, 27), (16, 19), (0, 8), (0, 12), (1, 28), (0, 11), (17, 23), (0, 10), (0, 6), (13, 19), (8, 31), (10, 24), (16, 12), (13, 23), (13, 21), (27, 27), (9, 3), (27, 25), (16, 16), (8, 21), (0, 7), (13, 18), (10, 25)]\n","len(nw_circ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":396,"status":"ok","timestamp":1718309503309,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"a8zrblGeHiND","outputId":"f4d513ad-fe0a-4c92-b030-3eefa4863070"},"outputs":[{"data":{"text/plain":["64"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# impt_months_heads = ([(23, 17), (17, 11), (16, 0), (26, 14), (18, 9), (5, 25), (22, 20), (6, 24), (26, 9), (12, 18), (13, 20), (19, 12), (27, 29), (13, 14), (16, 14), (12, 26), (19, 30), (16, 18), (31, 27), (26, 28), (16, 1), (18, 1), (19, 28), (18, 31), (29, 4), (17, 0), (14, 1), (17, 12), (12, 15), (28, 16), (10, 1), (16, 19), (9, 27), (30, 1), (19, 27), (0, 3), (15, 11), (21, 3), (11, 19), (12, 0), (23, 11), (8, 14), (16, 8), (22, 13), (13, 3), (4, 19), (14, 15), (12, 20), (19, 16), (18, 5)])\n","months_circ = [(20, 17), (6, 11), (16, 0), (5, 15), (17, 11), (23, 16), (5, 25), (7, 0), (26, 14), (6, 14), (12, 22), (8, 4), (12, 15), (16, 29), (15, 25), (5, 16), (18, 31), (14, 7), (11, 18), (4, 12), (3, 19), (12, 2), (11, 28), (4, 3), (18, 9), (8, 14), (12, 3), (11, 2), (10, 13), (4, 16), (1, 22), (11, 16), (3, 15), (13, 31), (2, 4), (2, 16), (8, 13), (0, 13), (8, 15), (12, 28), (1, 5), (0, 4), (0, 25), (3, 24), (13, 11), (1, 24), (8, 16), (13, 8), (3, 26), (0, 6), (3, 23), (1, 3), (14, 3), (8, 19), (8, 12), (14, 2), (8, 5), (1, 28), (8, 20), (2, 30), (8, 6), (10, 1), (13, 20), (19, 27)]\n","len(months_circ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1718309503309,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"e2XFugMsd3BY","outputId":"09346522-928a-451b-94f7-2678ce3af41b"},"outputs":[{"data":{"text/plain":["16"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["intersect_all = list(set(nums_1to9) & set(nw_circ) & set(months_circ))\n","len(intersect_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1718309503309,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"i2CL3XqQ4E7R","outputId":"b695f7af-796e-4663-9fe3-70a0f1a9b6b9"},"outputs":[{"data":{"text/plain":["172"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["union_all = list(set(nums_1to9) | set(nw_circ) | set(months_circ))\n","len(union_all)"]},{"cell_type":"markdown","metadata":{"id":"qCVySrfhk-YH"},"source":["# turn into fn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Njf4fDdk_k9"},"outputs":[],"source":["def ablate_circ_autoScore(model, circuit, sequences_as_str, next_members):\n","    corr_text = \"5 3 9\"\n","    list_outputs = []\n","    score = 0\n","    total_orig_logits = 0\n","    total_abl_logits = 0\n","    for clean_text, correct_ans in zip(sequences_as_str, next_members):\n","        # correct_ans_tokLen, orig_score = clean_gen(model, clean_text, correct_ans)\n","        # orig_score = clean_gen(model, clean_text, correct_ans)\n","\n","        heads_not_ablate = [(layer, head) for layer in range(32) for head in range(32)]  # unablated\n","        head_to_remove = circuit\n","        heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","\n","        mlps_not_ablate = [layer for layer in range(32)]\n","\n","        output_after_ablate, ablated_score = ablate_auto_score(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans)\n","        list_outputs.append(output_after_ablate)\n","        # total_orig_logits += orig_score\n","        # total_abl_logits += ablated_score\n","        print(correct_ans, output_after_ablate)\n","        # print(orig_score, ablated_score)\n","        # print('logit ratio: ', ablated_score / orig_score)\n","        if correct_ans == output_after_ablate:\n","            score += 1\n","    perc_score = score / len(next_members)\n","    # return perc_score, list_outputs, total_abl_logits / total_orig_logits\n","    return perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CswxAMn4oRfW"},"outputs":[],"source":["def ablate_randcirc_autoScore(model, sequences_as_str, next_members, num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap):\n","    corr_text = \"5 3 9\"\n","    # list_outputs = []\n","    all_scores = []\n","    for clean_text, correct_ans in zip(sequences_as_str, next_members):\n","        prompt_score = 0\n","        # correct_ans_tokLen = clean_gen(model, clean_text, correct_ans)\n","        for j in range(num_rand_runs):\n","            all_possible_pairs =  [(layer, head) for layer in range(32) for head in range(32)]\n","            filtered_pairs = [pair for pair in all_possible_pairs if pair not in heads_not_overlap] # Filter out heads_not_overlap from all_possible_pairs\n","\n","            # Randomly choose num_heads_rand pairs ensuring less than num_not_overlap overlaps with heads_not_overlap\n","            head_to_remove = choose_heads_to_remove(filtered_pairs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","\n","            heads_not_ablate = [x for x in all_possible_pairs if x not in head_to_remove]\n","\n","            mlps_not_ablate = [layer for layer in range(32)]\n","\n","            output_after_ablate = ablate_auto_score(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans)\n","            # list_outputs.append(output_after_ablate)\n","            # print(correct_ans, output_after_ablate)\n","            if correct_ans == output_after_ablate:\n","                prompt_score += 1\n","        # print(prompt_score / num_rand_runs)\n","        print(clean_text)\n","        all_scores.append(prompt_score / num_rand_runs)\n","\n","    perc_score = sum(all_scores) / len(next_members)\n","    return perc_score #, list_outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hemN_c9EsT3q"},"outputs":[],"source":["def gen_intervaled_seqs(interval, start, num_prompts):\n","    sequences = []\n","    next_members = []\n","\n","    # Generate overlapping intervals\n","    for _ in range(num_prompts):\n","        sequence = [start, start + interval, start + interval*2]\n","        next_member = str(start + interval*3)\n","        sequences.append(sequence)\n","        next_members.append(next_member)\n","        start += interval  # Move to the next starting point\n","\n","    sequences_as_str = [\" \".join(map(str, seq)) for seq in sequences]\n","    sequences_as_str = [member + \" \" for member in sequences_as_str]\n","    print(\"Sequences:\")\n","    print(sequences_as_str)\n","    print(\"\\nNext Members:\")\n","    print(next_members)\n","    return sequences_as_str, next_members"]},{"cell_type":"markdown","metadata":{"id":"Z6Jlr70EoNdy"},"source":["# test fns on (+1) seq"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1718309503310,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"gxzgw-2noNdz","outputId":"eb4de744-d91e-4b14-e981-2a178e26482a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequences:\n","['1 2 3 ', '2 3 4 ', '3 4 5 ', '4 5 6 ', '5 6 7 ', '6 7 8 ', '7 8 9 ', '8 9 10 ', '9 10 11 ', '10 11 12 ', '11 12 13 ', '12 13 14 ', '13 14 15 ', '14 15 16 ', '15 16 17 ', '16 17 18 ', '17 18 19 ', '18 19 20 ', '19 20 21 ', '20 21 22 ', '21 22 23 ', '22 23 24 ', '23 24 25 ', '24 25 26 ', '25 26 27 ', '26 27 28 ', '27 28 29 ', '28 29 30 ', '29 30 31 ', '30 31 32 ', '31 32 33 ', '32 33 34 ', '33 34 35 ', '34 35 36 ', '35 36 37 ', '36 37 38 ', '37 38 39 ', '38 39 40 ', '39 40 41 ', '40 41 42 ', '41 42 43 ', '42 43 44 ', '43 44 45 ', '44 45 46 ', '45 46 47 ', '46 47 48 ', '47 48 49 ', '48 49 50 ', '49 50 51 ', '50 51 52 ']\n","\n","Next Members:\n","['4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53']\n"]}],"source":["interval = 1\n","start = 1\n","num_prompts = 50\n","sequences_as_str, next_members = gen_intervaled_seqs(interval, start, num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48580,"status":"ok","timestamp":1718299749123,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"HUSp5ap_oNdz","outputId":"9fc3e943-7e4e-4ca1-ec9c-dd4cc20839e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["4 4\n","5 5\n","6 2\n","7 8\n","8 1\n","9 9\n","10 10\n","11 10\n","12 10\n","13 13\n","14 14\n","15 15\n","16 16\n","17 17\n","18 18\n","19 19\n","20 20\n","21 20\n","22 10\n","23 23\n","24 24\n","25 25\n","26 26\n","27 27\n","28 28\n","29 29\n","30 30\n","31 31\n","32 32\n","33 33\n","34 34\n","35 35\n","36 36\n","37 37\n","38 38\n","39 39\n","40 37\n","41 41\n","42 42\n","43 43\n","44 44\n","45 45\n","46 46\n","47 47\n","48 48\n","49 49\n","50 50\n","51 51\n","52 52\n","53 53\n","0.84\n"]}],"source":["perc_score = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","print(perc_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46131,"status":"ok","timestamp":1718299795170,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"riPzANT4oNdz","outputId":"b92dc84a-0285-416f-c6da-3ba9ea8c37c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["4 1\n","5 4\n","6 5\n","7 7\n","8 1\n","9 1\n","10 10\n","11 10\n","12 10\n","13 11\n","14 11\n","15 14\n","16 14\n","17 15\n","18 16\n","19 17\n","20 18\n","21 19\n","22 10\n","23 10\n","24 11\n","25 10\n","26 10\n","27 20\n","28 11\n","29 10\n","30 20\n","31 20\n","32 29\n","33 31\n","34 13\n","35 34\n","36 33\n","37 35\n","38 37\n","39 33\n","40 39\n","41 39\n","42 41\n","43 40\n","44 43\n","45 44\n","46 45\n","47 46\n","48 47\n","49 48\n","50 48\n","51 50\n","52 50\n","53 51\n","0.04\n"]}],"source":["perc_score = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","print(perc_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46422,"status":"ok","timestamp":1718299841524,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"Jd1V3CcnoNdz","outputId":"95a93301-04b8-4c5e-c6b5-ff9634d16905"},"outputs":[{"name":"stdout","output_type":"stream","text":["4 1\n","5 3\n","6 3\n","7 3\n","8 1\n","9 1\n","10 1 \n","11 0 \n","12 1 \n","13 1 \n","14 1 \n","15 1 \n","16 1 \n","17 10\n","18 10\n","19 10\n","20 10\n","21 10\n","22 1 \n","23 0 \n","24 1 \n","25 3 \n","26 3 \n","27 3 \n","28 3 \n","29 3 \n","30 10\n","31 10\n","32 1 \n","33 3 \n","34 3 \n","35 3 \n","36 3 \n","37 3 \n","38 4 \n","39 38\n","40 38\n","41 30\n","42 3 \n","43 0 \n","44 4 \n","45 3 \n","46 3 \n","47 4 \n","48 5 \n","49 3 \n","50 3 \n","51 10\n","52 1 \n","53 5 \n"]},{"data":{"text/plain":["0.0"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["perc_score = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47101,"status":"ok","timestamp":1718299888565,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"PuMI3NTDoNdz","outputId":"1f2e683e-c530-4082-9231-230d45f8adac"},"outputs":[{"name":"stdout","output_type":"stream","text":["4 2\n","5 4\n","6 5\n","7 6\n","8 7\n","9 7\n","10 9 \n","11 10\n","12 10\n","13 12\n","14 13\n","15 14\n","16 15\n","17 16\n","18 16\n","19 18\n","20 19\n","21 19\n","22 20\n","23 22\n","24 23\n","25 24\n","26 25\n","27 26\n","28 27\n","29 28\n","30 29\n","31 30\n","32 31\n","33 32\n","34 32\n","35 34\n","36 35\n","37 36\n","38 36\n","39 38\n","40 39\n","41 39\n","42 42\n","43 42\n","44 4 \n","45 44\n","46 45\n","47 46\n","48 47\n","49 48\n","50 49\n","51 50\n","52 51\n","53 52\n"]},{"data":{"text/plain":["0.02"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["perc_score = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":463257,"status":"ok","timestamp":1718309966175,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"G7-zgUxmoNd0","outputId":"feebe55b-2592-4d5f-afd4-6e0e0ed79cb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 2 3 \n","2 3 4 \n","3 4 5 \n","4 5 6 \n","5 6 7 \n","6 7 8 \n","7 8 9 \n","8 9 10 \n","9 10 11 \n","10 11 12 \n","11 12 13 \n","12 13 14 \n","13 14 15 \n","14 15 16 \n","15 16 17 \n","16 17 18 \n","17 18 19 \n","18 19 20 \n","19 20 21 \n","20 21 22 \n","21 22 23 \n","22 23 24 \n","23 24 25 \n","24 25 26 \n","25 26 27 \n","26 27 28 \n","27 28 29 \n","28 29 30 \n","29 30 31 \n","30 31 32 \n","31 32 33 \n","32 33 34 \n","33 34 35 \n","34 35 36 \n","35 36 37 \n","36 37 38 \n","37 38 39 \n","38 39 40 \n","39 40 41 \n","40 41 42 \n","41 42 43 \n","42 43 44 \n","43 44 45 \n","44 45 46 \n","45 46 47 \n","46 47 48 \n","47 48 49 \n","48 49 50 \n","49 50 51 \n","50 51 52 \n"]},{"data":{"text/plain":["0.9699999999999998"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]},{"cell_type":"markdown","metadata":{"id":"G1XF6akcoF0O"},"source":["# test fns on (+2) seq"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":391,"status":"ok","timestamp":1718310004171,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"zPyf-rqBuqiR","outputId":"ffd1afbe-a3dc-4b0f-d2f5-d744542db2be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequences:\n","['2 4 6 ', '4 6 8 ', '6 8 10 ', '8 10 12 ', '10 12 14 ', '12 14 16 ', '14 16 18 ', '16 18 20 ', '18 20 22 ', '20 22 24 ', '22 24 26 ', '24 26 28 ', '26 28 30 ', '28 30 32 ', '30 32 34 ', '32 34 36 ', '34 36 38 ', '36 38 40 ', '38 40 42 ', '40 42 44 ', '42 44 46 ', '44 46 48 ', '46 48 50 ', '48 50 52 ', '50 52 54 ', '52 54 56 ', '54 56 58 ', '56 58 60 ', '58 60 62 ', '60 62 64 ', '62 64 66 ', '64 66 68 ', '66 68 70 ', '68 70 72 ', '70 72 74 ', '72 74 76 ', '74 76 78 ', '76 78 80 ', '78 80 82 ', '80 82 84 ', '82 84 86 ', '84 86 88 ', '86 88 90 ', '88 90 92 ', '90 92 94 ', '92 94 96 ', '94 96 98 ', '96 98 100 ', '98 100 102 ', '100 102 104 ']\n","\n","Next Members:\n","['8', '10', '12', '14', '16', '18', '20', '22', '24', '26', '28', '30', '32', '34', '36', '38', '40', '42', '44', '46', '48', '50', '52', '54', '56', '58', '60', '62', '64', '66', '68', '70', '72', '74', '76', '78', '80', '82', '84', '86', '88', '90', '92', '94', '96', '98', '100', '102', '104', '106']\n"]}],"source":["interval = 2\n","start = 2\n","num_prompts = 50\n","sequences_as_str, next_members = gen_intervaled_seqs(interval, start, num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51287,"status":"ok","timestamp":1718290412376,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"c2UHBRC1qryL","outputId":"b53c6fc0-483b-4499-be37-b80a99aeae49"},"outputs":[{"name":"stdout","output_type":"stream","text":["8 8\n","10 10\n","12 10\n","14 10\n","16 10\n","18 10\n","20 10\n","22 10\n","24 10\n","26 25\n","28 28\n","30 29\n","32 32\n","34 33\n","36 35\n","38 38\n","40 39\n","42 42\n","44 45\n","46 45\n","48 48\n","50 47\n","52 50\n","54 54\n","56 55\n","58 58\n","60 57\n","62 62\n","64 65\n","66 65\n","68 67\n","70 69\n","72 74\n","74 74\n","76 75\n","78 77\n","80 79\n","82 82\n","84 84\n","86 85\n","88 88\n","90 89\n","92 90\n","94 94\n","96 95\n","98 98\n","100 97 \n","102 100\n","104 104\n","106 105\n","0.34\n"]}],"source":["perc_score = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","print(perc_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49387,"status":"ok","timestamp":1718290468507,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"bi7kgQ_4m_Pf","outputId":"5f404dcf-91c5-49c1-f4d1-bb072c368c02"},"outputs":[{"name":"stdout","output_type":"stream","text":["8 6\n","10 10\n","12 10\n","14 10\n","16 10\n","18 14\n","20 16\n","22 18\n","24 10\n","26 10\n","28 10\n","30 20\n","32 10\n","34 20\n","36 33\n","38 34\n","40 36\n","42 40\n","44 40\n","46 44\n","48 46\n","50 48\n","52 50\n","54 50\n","56 50\n","58 56\n","60 56\n","62 58\n","64 60\n","66 64\n","68 64\n","70 70\n","72 70\n","74 70\n","76 70\n","78 74\n","80 76\n","82 78\n","84 82\n","86 82\n","88 86\n","90 88\n","92 88\n","94 88\n","96 92\n","98 94\n","100 97 \n","102 100\n","104 100\n","106 100\n","0.04\n"]}],"source":["perc_score = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","print(perc_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49568,"status":"ok","timestamp":1718290518050,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"QpIK7ie_nTAP","outputId":"0e4a51d7-b138-4bf8-9bee-8298dcea60e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["8 2\n","10 1 \n","12 0 \n","14 1 \n","16 1 \n","18 10\n","20 10\n","22 10\n","24 1 \n","26 0 \n","28 2 \n","30 10\n","32 10\n","34 30\n","36 3 \n","38 32\n","40 36\n","42 3 \n","44 3 \n","46 0 \n","48 4 \n","50 4 \n","52 10\n","54 10\n","56 0 \n","58 5 \n","60 4 \n","62 10\n","64 10\n","66 60\n","68 6 \n","70 6 \n","72 10\n","74 10\n","76 70\n","78 7 \n","80 7 \n","82 10\n","84 80\n","86 80\n","88 84\n","90 86\n","92 88\n","94 88\n","96 90\n","98 94\n","100 94 \n","102 000\n","104 000\n","106 000\n"]},{"data":{"text/plain":["0.0"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["perc_score = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49727,"status":"ok","timestamp":1718290567608,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"b3sXeG-yn-x_","outputId":"d0fff513-f8fc-49e7-c3a7-1664e22a1599"},"outputs":[{"name":"stdout","output_type":"stream","text":["8 6\n","10 6 \n","12 10\n","14 10\n","16 14\n","18 16\n","20 16\n","22 18\n","24 20\n","26 24\n","28 26\n","30 28\n","32 30\n","34 32\n","36 34\n","38 36\n","40 36\n","42 38\n","44 43\n","46 44\n","48 46\n","50 48\n","52 50\n","54 52\n","56 54\n","58 56\n","60 56\n","62 60\n","64 60\n","66 64\n","68 66\n","70 68\n","72 68\n","74 72\n","76 75\n","78 76\n","80 76\n","82 78\n","84 80\n","86 84\n","88 86\n","90 86\n","92 90\n","94 92\n","96 94\n","98 96\n","100 96 \n","102 10 \n","104 102\n","106 104\n"]},{"data":{"text/plain":["0.0"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["perc_score = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"tpljlRXupLxH","outputId":"5b48cf83-a882-488a-a0c2-9b8322827946"},"outputs":[{"name":"stdout","output_type":"stream","text":["2 4 6 \n","4 6 8 \n","6 8 10 \n","8 10 12 \n","10 12 14 \n","12 14 16 \n","14 16 18 \n","16 18 20 \n","18 20 22 \n","20 22 24 \n","22 24 26 \n","24 26 28 \n","26 28 30 \n","28 30 32 \n","30 32 34 \n","32 34 36 \n","34 36 38 \n","36 38 40 \n","38 40 42 \n","40 42 44 \n","42 44 46 \n","44 46 48 \n","46 48 50 \n","48 50 52 \n","50 52 54 \n","52 54 56 \n","54 56 58 \n","56 58 60 \n","58 60 62 \n","60 62 64 \n","62 64 66 \n","64 66 68 \n","66 68 70 \n","68 70 72 \n","70 72 74 \n","72 74 76 \n","74 76 78 \n","76 78 80 \n","78 80 82 \n","80 82 84 \n","82 84 86 \n","84 86 88 \n","86 88 90 \n","88 90 92 \n","90 92 94 \n","92 94 96 \n","94 96 98 \n","96 98 100 \n","98 100 102 \n","100 102 104 \n"]},{"data":{"text/plain":["0.9239999999999998"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]},{"cell_type":"markdown","metadata":{"id":"brPaYxQkGujJ"},"source":["# (+3) seq"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZPKFJCqVGujU","outputId":"fdc39555-7ba3-4a8d-d8f2-2d65627d21ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequences:\n","['3 6 9 ', '6 9 12 ', '9 12 15 ', '12 15 18 ', '15 18 21 ', '18 21 24 ', '21 24 27 ', '24 27 30 ', '27 30 33 ', '30 33 36 ', '33 36 39 ', '36 39 42 ', '39 42 45 ', '42 45 48 ', '45 48 51 ', '48 51 54 ', '51 54 57 ', '54 57 60 ', '57 60 63 ', '60 63 66 ', '63 66 69 ', '66 69 72 ', '69 72 75 ', '72 75 78 ', '75 78 81 ', '78 81 84 ', '81 84 87 ', '84 87 90 ', '87 90 93 ', '90 93 96 ', '93 96 99 ', '96 99 102 ', '99 102 105 ', '102 105 108 ', '105 108 111 ', '108 111 114 ', '111 114 117 ', '114 117 120 ', '117 120 123 ', '120 123 126 ', '123 126 129 ', '126 129 132 ', '129 132 135 ', '132 135 138 ', '135 138 141 ', '138 141 144 ', '141 144 147 ', '144 147 150 ', '147 150 153 ', '150 153 156 ']\n","\n","Next Members:\n","['12', '15', '18', '21', '24', '27', '30', '33', '36', '39', '42', '45', '48', '51', '54', '57', '60', '63', '66', '69', '72', '75', '78', '81', '84', '87', '90', '93', '96', '99', '102', '105', '108', '111', '114', '117', '120', '123', '126', '129', '132', '135', '138', '141', '144', '147', '150', '153', '156', '159']\n"]}],"source":["interval = 3\n","start = 3\n","num_prompts = 50\n","sequences_as_str, next_members = gen_intervaled_seqs(interval, start, num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57573,"status":"ok","timestamp":1718290746864,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"AnPCx13oGujV","outputId":"5bed1c36-ac30-4509-9bca-1264b35b0362"},"outputs":[{"name":"stdout","output_type":"stream","text":["12 10\n","15 15\n","18 18\n","21 12\n","24 10\n","27 25\n","30 30\n","33 30\n","36 30\n","39 30\n","42 36\n","45 45\n","48 48\n","51 47\n","54 55\n","57 55\n","60 58\n","63 63\n","66 65\n","69 67\n","72 70\n","75 75\n","78 78\n","81 79\n","84 85\n","87 85\n","90 88\n","93 93\n","96 94\n","99 97\n","102 97 \n","105 105\n","108 108\n","111 109\n","114 109\n","117 115\n","120 119\n","123 119\n","126 125\n","129 127\n","132 130\n","135 135\n","138 138\n","141 141\n","144 144\n","147 147\n","150 150\n","153 153\n","156 155\n","159 159\n","0.38\n"]}],"source":["perc_score = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","print(perc_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55612,"status":"ok","timestamp":1718290802347,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"uBkGmpPIGujV","outputId":"6feda9c0-5ef6-4f9e-c0fa-1860862d1155"},"outputs":[{"name":"stdout","output_type":"stream","text":["12 10\n","15 12\n","18 12\n","21 15\n","24 18\n","27 11\n","30 10\n","33 10\n","36 27\n","39 33\n","42 33\n","45 39\n","48 45\n","51 45\n","54 48\n","57 48\n","60 54\n","63 60\n","66 53\n","69 63\n","72 66\n","75 72\n","78 75\n","81 78\n","84 10\n","87 77\n","90 87\n","93 90\n","96 87\n","99 93\n","102 96 \n","105 101\n","108 105\n","111 100\n","114 111\n","117 111\n","120 111\n","123 111\n","126 10 \n","129 123\n","132 126\n","135 129\n","138 135\n","141 133\n","144 13 \n","147 141\n","150 147\n","153 144\n","156 153\n","159 153\n","0.0\n"]}],"source":["perc_score = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","print(perc_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54337,"status":"ok","timestamp":1718290856531,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"hpuQhIYtGujV","outputId":"73047c80-e053-4bc8-b582-3658cff35b14"},"outputs":[{"name":"stdout","output_type":"stream","text":["12 1 \n","15 1 \n","18 1 \n","21 10\n","24 1 \n","27 1 \n","30 1 \n","33 10\n","36 10\n","39 30\n","42 3 \n","45 3 \n","48 3 \n","51 4 \n","54 1 \n","57 3 \n","60 1 \n","63 10\n","66 10\n","69 63\n","72 10\n","75 10\n","78 3 \n","81 7 \n","84 1 \n","87 8 \n","90 81\n","93 84\n","96 10\n","99 90\n","102 96 \n","105 000\n","108 000\n","111 000\n","114 005\n","117 08 \n","120 111\n","123 100\n","126 17 \n","129 10 \n","132 10 \n","135 10 \n","138 132\n","141 132\n","144 10 \n","147 141\n","150 141\n","153 144\n","156 10 \n","159 150\n"]},{"data":{"text/plain":["0.0"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["perc_score = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55721,"status":"ok","timestamp":1718290912092,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"JqpQr2kAGujV","outputId":"4335ea03-65fe-4f65-dd2b-63b6821ffce6"},"outputs":[{"name":"stdout","output_type":"stream","text":["12 3 \n","15 12\n","18 15\n","21 18\n","24 18\n","27 24\n","30 27\n","33 20\n","36 30\n","39 36\n","42 36\n","45 39\n","48 45\n","51 48\n","54 51\n","57 54\n","60 57\n","63 57\n","66 60\n","69 63\n","72 69\n","75 6 \n","78 75\n","81 75\n","84 78\n","87 84\n","90 88\n","93 90\n","96 93\n","99 93\n","102 96 \n","105 9 9\n","108 105\n","111 108\n","114 10 \n","117 14 \n","120 117\n","123 17 \n","126 123\n","129 126\n","132 129\n","135 132\n","138 135\n","141 138\n","144 141\n","147 144\n","150 147\n","153 150\n","156 15 \n","159 156\n"]},{"data":{"text/plain":["0.0"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["perc_score = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KUM5KDssGujW","outputId":"50dc644f-7c16-4f5c-cf5c-0c5809376ac8"},"outputs":[{"name":"stdout","output_type":"stream","text":["3 6 9 \n","6 9 12 \n","9 12 15 \n","12 15 18 \n","15 18 21 \n","18 21 24 \n","21 24 27 \n","24 27 30 \n","27 30 33 \n","30 33 36 \n","33 36 39 \n","36 39 42 \n","39 42 45 \n","42 45 48 \n","45 48 51 \n","48 51 54 \n","51 54 57 \n","54 57 60 \n","57 60 63 \n","60 63 66 \n","63 66 69 \n","66 69 72 \n","69 72 75 \n","72 75 78 \n","75 78 81 \n","78 81 84 \n","81 84 87 \n","84 87 90 \n","87 90 93 \n","90 93 96 \n","93 96 99 \n","96 99 102 \n","99 102 105 \n","102 105 108 \n","105 108 111 \n","108 111 114 \n","111 114 117 \n","114 117 120 \n","117 120 123 \n","120 123 126 \n","123 126 129 \n","126 129 132 \n","129 132 135 \n","132 135 138 \n","135 138 141 \n","138 141 144 \n","141 144 147 \n","144 147 150 \n","147 150 153 \n","150 153 156 \n"]},{"data":{"text/plain":["0.7240000000000002"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]},{"cell_type":"markdown","metadata":{"id":"9fgfGogxHu5g"},"source":["# (+10) seq"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K3MksZbFHu5h","outputId":"754efdfc-0731-428a-8401-2f8b7a130fd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequences:\n","['0 10 20 ', '10 20 30 ', '20 30 40 ', '30 40 50 ', '40 50 60 ', '50 60 70 ', '60 70 80 ', '70 80 90 ', '80 90 100 ', '90 100 110 ', '100 110 120 ', '110 120 130 ', '120 130 140 ', '130 140 150 ', '140 150 160 ', '150 160 170 ', '160 170 180 ', '170 180 190 ', '180 190 200 ', '190 200 210 ', '200 210 220 ', '210 220 230 ', '220 230 240 ', '230 240 250 ', '240 250 260 ', '250 260 270 ', '260 270 280 ', '270 280 290 ', '280 290 300 ', '290 300 310 ', '300 310 320 ', '310 320 330 ', '320 330 340 ', '330 340 350 ', '340 350 360 ', '350 360 370 ', '360 370 380 ', '370 380 390 ', '380 390 400 ', '390 400 410 ', '400 410 420 ', '410 420 430 ', '420 430 440 ', '430 440 450 ', '440 450 460 ', '450 460 470 ', '460 470 480 ', '470 480 490 ', '480 490 500 ', '490 500 510 ']\n","\n","Next Members:\n","['30', '40', '50', '60', '70', '80', '90', '100', '110', '120', '130', '140', '150', '160', '170', '180', '190', '200', '210', '220', '230', '240', '250', '260', '270', '280', '290', '300', '310', '320', '330', '340', '350', '360', '370', '380', '390', '400', '410', '420', '430', '440', '450', '460', '470', '480', '490', '500', '510', '520']\n"]}],"source":["interval = 10\n","start = 0\n","num_prompts = 50\n","sequences_as_str, next_members = gen_intervaled_seqs(interval, start, num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65425,"status":"ok","timestamp":1718291016026,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"KftJumgvHu5h","outputId":"9600d427-730b-4e04-fd29-cb22cf6d8476"},"outputs":[{"name":"stdout","output_type":"stream","text":["30 10\n","40 10\n","50 50\n","60 50\n","70 70\n","80 80\n","90 90\n","100 70 \n","110 100\n","120 100\n","130 130\n","140 140\n","150 150\n","160 160\n","170 170\n","180 180\n","190 190\n","200 195\n","210 100\n","220 100\n","230 230\n","240 240\n","250 250\n","260 260\n","270 270\n","280 280\n","290 290\n","300 300\n","310 310\n","320 320\n","330 330\n","340 340\n","350 350\n","360 360\n","370 370\n","380 380\n","390 390\n","400 390\n","410 410\n","420 420\n","430 430\n","440 440\n","450 440\n","460 460\n","470 470\n","480 480\n","490 490\n","500 500\n","510 500\n","520 520\n","0.76\n"]}],"source":["perc_score = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","print(perc_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62221,"status":"ok","timestamp":1718291078222,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"fX8O3iR7Hu5i","outputId":"ab7eeb0d-4046-4775-ec2a-712bfe0129da"},"outputs":[{"name":"stdout","output_type":"stream","text":["30 10\n","40 10\n","50 10\n","60 00\n","70 70\n","80 10\n","90 10\n","100 100\n","110 100\n","120 100\n","130 100\n","140 100\n","150 100\n","160 100\n","170 150\n","180 100\n","190 100\n","200 100\n","210 100\n","220 100\n","230 100\n","240 100\n","250 100\n","260 230\n","270 260\n","280 260\n","290 260\n","300 270\n","310 280\n","320 290\n","330 310\n","340 30 \n","350 330\n","360 300\n","370 350\n","380 333\n","390 333\n","400 333\n","410 000\n","420 100\n","430 400\n","440 430\n","450 440\n","460 450\n","470 460\n","480 470\n","490 480\n","500 480\n","510 480\n","520 500\n","0.04\n"]}],"source":["perc_score = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","print(perc_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62901,"status":"ok","timestamp":1718291141075,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"xl-WgL40Hu5i","outputId":"acb210b7-5fd8-412e-a305-e9aab1371766"},"outputs":[{"name":"stdout","output_type":"stream","text":["30 0 \n","40 0 \n","50 30\n","60 40\n","70 50\n","80 60\n","90 70\n","100 80 \n","110 000\n","120 000\n","130 000\n","140 10 \n","150 140\n","160 10 \n","170 160\n","180 10 \n","190 180\n","200 10 \n","210 100\n","220 000\n","230 00 \n","240 20 \n","250 20 \n","260 30 \n","270 30 \n","280 30 \n","290 280\n","300 20 \n","310 200\n","320 300\n","330 30 \n","340 320\n","350 340\n","360 350\n","370 360\n","380 370\n","390 380\n","400 390\n","410 300\n","420 400\n","430 40 \n","440 430\n","450 430\n","460 450\n","470 460\n","480 470\n","490 480\n","500 490\n","510 40 \n","520 500\n"]},{"data":{"text/plain":["0.0"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["perc_score = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63909,"status":"ok","timestamp":1718291204614,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"Sh2D_A61Hu5i","outputId":"af1e2ac4-a1ab-4a51-e1ae-d3c4a25fc2e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["30 10\n","40 30\n","50 40\n","60 50\n","70 60\n","80 70\n","90 70\n","100 90 \n","110 10 \n","120 10 \n","130 10 \n","140 10 \n","150 140\n","160 10 \n","170 160\n","180 160\n","190 180\n","200 180\n","210 190\n","220 20 \n","230 202\n","240 20 \n","250 24 \n","260 25 \n","270 260\n","280 270\n","290 280\n","300 290\n","310 30 \n","320 30 \n","330 30 \n","340 30 \n","350 30 \n","360 35 \n","370 30 \n","380 36 \n","390 38 \n","400 39 \n","410 30 \n","420 41 \n","430 43 \n","440 43 \n","450 440\n","460 45 \n","470 46 \n","480 48 \n","490 48 \n","500 490\n","510 50 \n","520 50 \n"]},{"data":{"text/plain":["0.0"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["perc_score = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"-whrLEfHHu5i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718311677506,"user_tz":-60,"elapsed":463658,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d3add25b-9aac-4c08-e0c3-8e529780ac30"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 10 20 \n","10 20 30 \n","20 30 40 \n","30 40 50 \n","40 50 60 \n","50 60 70 \n","60 70 80 \n","70 80 90 \n","80 90 100 \n","90 100 110 \n","100 110 120 \n","110 120 130 \n","120 130 140 \n","130 140 150 \n","140 150 160 \n","150 160 170 \n","160 170 180 \n","170 180 190 \n","180 190 200 \n","190 200 210 \n","200 210 220 \n","210 220 230 \n","220 230 240 \n","230 240 250 \n","240 250 260 \n","250 260 270 \n","260 270 280 \n","270 280 290 \n","280 290 300 \n","290 300 310 \n","300 310 320 \n","310 320 330 \n","320 330 340 \n","330 340 350 \n","340 350 360 \n","350 360 370 \n","360 370 380 \n","370 380 390 \n","380 390 400 \n","390 400 410 \n","400 410 420 \n","410 420 430 \n","420 430 440 \n","430 440 450 \n","440 450 460 \n","450 460 470 \n","460 470 480 \n","470 480 490 \n","480 490 500 \n","490 500 510 \n"]},{"output_type":"execute_result","data":{"text/plain":["0.9559999999999998"]},"metadata":{},"execution_count":45}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]},{"cell_type":"markdown","metadata":{"id":"sav9M8Rzw25Y"},"source":["# (+100) seq"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"djv1stYSw25Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718311823598,"user_tz":-60,"elapsed":253,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3da2c39b-dafc-4dff-aa51-7c3f1a26b5a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequences:\n","['0 100 200 ', '100 200 300 ', '200 300 400 ', '300 400 500 ', '400 500 600 ', '500 600 700 ', '600 700 800 ', '700 800 900 ', '800 900 1000 ', '900 1000 1100 ', '1000 1100 1200 ', '1100 1200 1300 ', '1200 1300 1400 ', '1300 1400 1500 ', '1400 1500 1600 ', '1500 1600 1700 ', '1600 1700 1800 ', '1700 1800 1900 ', '1800 1900 2000 ', '1900 2000 2100 ', '2000 2100 2200 ', '2100 2200 2300 ', '2200 2300 2400 ', '2300 2400 2500 ', '2400 2500 2600 ', '2500 2600 2700 ', '2600 2700 2800 ', '2700 2800 2900 ', '2800 2900 3000 ', '2900 3000 3100 ', '3000 3100 3200 ', '3100 3200 3300 ', '3200 3300 3400 ', '3300 3400 3500 ', '3400 3500 3600 ', '3500 3600 3700 ', '3600 3700 3800 ', '3700 3800 3900 ', '3800 3900 4000 ', '3900 4000 4100 ', '4000 4100 4200 ', '4100 4200 4300 ', '4200 4300 4400 ', '4300 4400 4500 ', '4400 4500 4600 ', '4500 4600 4700 ', '4600 4700 4800 ', '4700 4800 4900 ', '4800 4900 5000 ', '4900 5000 5100 ']\n","\n","Next Members:\n","['300', '400', '500', '600', '700', '800', '900', '1000', '1100', '1200', '1300', '1400', '1500', '1600', '1700', '1800', '1900', '2000', '2100', '2200', '2300', '2400', '2500', '2600', '2700', '2800', '2900', '3000', '3100', '3200', '3300', '3400', '3500', '3600', '3700', '3800', '3900', '4000', '4100', '4200', '4300', '4400', '4500', '4600', '4700', '4800', '4900', '5000', '5100', '5200']\n"]}],"source":["interval = 100\n","start = 0\n","num_prompts = 50\n","sequences_as_str, next_members = gen_intervaled_seqs(interval, start, num_prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13107,"status":"ok","timestamp":1717999835462,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"V7Dg9Pc9w25Z","outputId":"4d794003-140f-4589-aea0-81321e01fd5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["300 300\n","400 400\n","500 500\n","600 600\n","700 700\n"]},{"data":{"text/plain":["1.0"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, intersect_all, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12732,"status":"ok","timestamp":1717999848181,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"ys884NHJw25Z","outputId":"a62cb8aa-760b-48d6-c257-e465484d01f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["300 000\n","400 100\n","500 100\n","600 000\n","700 100\n"]},{"data":{"text/plain":["0.0"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nums_1to9, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12548,"status":"ok","timestamp":1717999860574,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"BV5nlp2Bw25Z","outputId":"75d79f58-4a19-4160-ca0d-467c4d91788d"},"outputs":[{"name":"stdout","output_type":"stream","text":["300 000\n","400 000\n","500 300\n","600 300\n","700 500\n"]},{"data":{"text/plain":["0.0"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, nw_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12643,"status":"ok","timestamp":1717999873189,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"8ZEdPNHVw25Z","outputId":"7e6ca24f-e9b1-4add-c188-0734ed26f37c"},"outputs":[{"name":"stdout","output_type":"stream","text":["300 101\n","400 303\n","500 404\n","600 505\n","700 606\n"]},{"data":{"text/plain":["0.0"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["perc_score, list_outputs = ablate_circ_autoScore(model, months_circ, sequences_as_str, next_members)\n","perc_score"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"6NUtXGLgw25a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718313800745,"user_tz":-60,"elapsed":267948,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"54398474-94a1-4fea-9821-def7197b3bf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 100 200 \n","100 200 300 \n","200 300 400 \n","300 400 500 \n","400 500 600 \n","500 600 700 \n","600 700 800 \n","700 800 900 \n","800 900 1000 \n","900 1000 1100 \n","1000 1100 1200 \n","1100 1200 1300 \n","1200 1300 1400 \n","1300 1400 1500 \n","1400 1500 1600 \n","1500 1600 1700 \n","1600 1700 1800 \n","1700 1800 1900 \n","1800 1900 2000 \n","1900 2000 2100 \n","2000 2100 2200 \n","2100 2200 2300 \n","2200 2300 2400 \n","2300 2400 2500 \n","2400 2500 2600 \n","2500 2600 2700 \n","2600 2700 2800 \n","2700 2800 2900 \n","2800 2900 3000 \n","2900 3000 3100 \n","3000 3100 3200 \n","3100 3200 3300 \n","3200 3300 3400 \n","3300 3400 3500 \n","3400 3500 3600 \n","3500 3600 3700 \n","3600 3700 3800 \n","3700 3800 3900 \n","3800 3900 4000 \n","3900 4000 4100 \n","4000 4100 4200 \n","4100 4200 4300 \n","4200 4300 4400 \n","4300 4400 4500 \n","4400 4500 4600 \n","4500 4600 4700 \n","4600 4700 4800 \n","4700 4800 4900 \n","4800 4900 5000 \n","4900 5000 5100 \n"]},{"output_type":"execute_result","data":{"text/plain":["0.9119999999999996"]},"metadata":{},"execution_count":50}],"source":["num_rand_runs = 10\n","heads_not_overlap = intersect_all\n","num_heads_rand = 100\n","num_not_overlap = len(intersect_all)\n","perc_score = ablate_randcirc_autoScore(model, sequences_as_str, next_members,\n","                                                    num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","perc_score"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["DcZG9rm2IAiA","PDP2cpaiZpPX","dsdvChbcvgp5"],"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNQNAUfur6+LCngrRb4+kVI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}