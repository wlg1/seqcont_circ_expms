{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"KSKP_OsTDki6","executionInfo":{"status":"ok","timestamp":1718357912302,"user_tz":-60,"elapsed":11,"user":{"displayName":"mike lan","userId":"00221534718597437140"}}},"outputs":[],"source":["save_files = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1wsEy0MqHU0"},"outputs":[],"source":["%%capture\n","%pip install git+https://github.com/neelnanda-io/TransformerLens.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6b1n2tvIAiD"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","# import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML\n","\n","import pickle\n","from google.colab import files\n","\n","import matplotlib.pyplot as plt\n","import statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuhzYxbsIAiE"},"outputs":[],"source":["import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer #, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFMTUcQiIAiF"},"outputs":[],"source":["torch.set_grad_enabled(False)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQr6WtEppHgy"},"outputs":[],"source":["import pdb"]},{"cell_type":"markdown","metadata":{"id":"Z4iJEGh6b56v"},"source":["## Import functions from repo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8TXMRL3CoPd"},"outputs":[],"source":["!git clone https://github.com/apartresearch/seqcont_circuits.git\n","%cd /content/seqcont_circuits/src/iter_node_pruning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22TI4zjMDMfQ"},"outputs":[],"source":["## comment this out when debugging functions in colab to use funcs defined in colab\n","\n","# don't improt this\n","# # from dataset import Dataset\n","\n","from metrics import *\n","from head_ablation_fns import *\n","from mlp_ablation_fns import *\n","from node_ablation_fns import *\n","from loop_node_ablation_fns import *"]},{"cell_type":"markdown","metadata":{"id":"9R_g1Ghv7cGE"},"source":["## fns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsJmCq-C2Zu6"},"outputs":[],"source":["import random\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NPjHv-Xny4R"},"outputs":[],"source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer):  # , S1_is_first=False\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.corr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.incorr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a dict whose values are tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        # for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","        for targ in [key for key in pos_dict]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = self.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZKVG778QYyn"},"outputs":[],"source":["def generate_prompts_list_longer(text, tokens):\n","    prompts_list = []\n","    prompt_dict = {\n","        'corr': str(1),\n","        'incorr': str(2),\n","        'text': text\n","        # 'text': model.to_string(tokens)[0]\n","        }\n","    tokens_as_strs = model.tokenizer.tokenize(text)\n","    # tokens_as_strs = model.to_string(tokens)[0].split()\n","    # for i in range(tokens.shape[1]):\n","    for i, tok in enumerate(tokens_as_strs):\n","        prompt_dict['S'+str(i)] = tok\n","    # for i, tok in enumerate(tokens):\n","    #     prompt_dict['S'+str(i)] = model.to_string(tok)\n","\n","    # prompt_dict = {\n","    #     'corr': '4',\n","    #     'incorr': '3',\n","    #     'text': model.to_string(tokens)[0]\n","    # }\n","    # # list_tokens = tokenizer.tokenize('1 2 3 ')\n","    # tokens_as_strs = model.to_string(tokens)[0].split()\n","    # for i, tok_as_str in enumerate(tokens_as_strs):\n","    #     if tok_as_str == '▁':\n","    #         prompt_dict['S'+str(i)] = ' '\n","    #     else:\n","    #         prompt_dict['S'+str(i)] = tok_as_str\n","    prompts_list.append(prompt_dict)\n","    return prompts_list"]},{"cell_type":"markdown","metadata":{"id":"PDP2cpaiZpPX"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGCiZUPpJUsD"},"outputs":[],"source":["from transformers import LlamaForCausalLM, LlamaTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15677,"status":"ok","timestamp":1718291415617,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"-CocJpgjsf_M","outputId":"e89975eb-ed44-4f06-c7c9-08d3025b4c48"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["c77db85c9ef64ff1b55a54b604de4822","8301f6476fe548469248aff71d360ece","87e9d04d06ec4e0ebd60a2c4c509e554","40ebb98d8e344ea78dcf084bf2d31dfc","de435dc74c8545b7a2a57bff55b0c1ef","e77870b7cf1945dc9d545b116d6d949c","9a507fdf33bf4df0a82f15a5695a47d1","687993a1eef9460ba806fe46d35febc1","8454ec884d8f4d34b9917e4049681dfd","2d9ccae72bfe4c8cb48816068bf1d60b","6c6f8435e3be4a2b859618c1a2cf0dff"]},"executionInfo":{"elapsed":46380,"status":"ok","timestamp":1718291461888,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"xLgpia0tI6O8","outputId":"3c896b79-f5ba-40bc-f181-ea6b6829272c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c77db85c9ef64ff1b55a54b604de4822","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8301f6476fe548469248aff71d360ece","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87e9d04d06ec4e0ebd60a2c4c509e554","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40ebb98d8e344ea78dcf084bf2d31dfc","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de435dc74c8545b7a2a57bff55b0c1ef","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e77870b7cf1945dc9d545b116d6d949c","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a507fdf33bf4df0a82f15a5695a47d1","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"687993a1eef9460ba806fe46d35febc1","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8454ec884d8f4d34b9917e4049681dfd","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d9ccae72bfe4c8cb48816068bf1d60b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c6f8435e3be4a2b859618c1a2cf0dff","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["LLAMA_2_7B_CHAT_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","tokenizer = LlamaTokenizer.from_pretrained(LLAMA_2_7B_CHAT_PATH)\n","# tokenizer = LlamaTokenizer.from_pretrained(LLAMA_2_7B_CHAT_PATH, use_fast= False, add_prefix_space= False)\n","hf_model = LlamaForCausalLM.from_pretrained(LLAMA_2_7B_CHAT_PATH, low_cpu_mem_usage=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rtZ2e3sMY5S"},"outputs":[],"source":["import transformer_lens.utils as utils\n","from transformer_lens.hook_points import HookPoint\n","from transformer_lens import HookedTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31692,"status":"ok","timestamp":1718291493565,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"sUnSHvA-Myx8","outputId":"c9bdaffb-0763-4028-bc79-a4fb83a9298f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n","Moving model to device:  cuda\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    LLAMA_2_7B_CHAT_PATH,\n","    hf_model = hf_model,\n","    tokenizer = tokenizer,\n","    device = \"cpu\",\n","    fold_ln = False,\n","    center_writing_weights = False,\n","    center_unembed = False,\n",")\n","\n","del hf_model\n","\n","model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"dsdvChbcvgp5"},"source":["# new ablation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KlWYoEy72Cf"},"outputs":[],"source":["def get_heads_actv_mean(\n","    # means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Output: The mean activations of a head's output\n","    '''\n","    # _, means_cache = model.run_with_cache(\n","    #     means_dataset.toks.long(),\n","    #     return_type=None,\n","    #     names_filter=lambda name: name.endswith(\"z\"),\n","    # )\n","    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n","    # batch, seq_len = len(means_dataset), means_dataset.max_len\n","\n","    seq_len = max(\n","            [\n","                len(model.tokenizer(prompt[\"text\"]).input_ids[1:])\n","                for prompt in self.prompts\n","            ]\n","        )\n","\n","    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n","\n","    # for layer in range(model.cfg.n_layers):\n","    #     z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n","    #     for template_group in means_dataset.groups:\n","    #         z_for_this_template = z_for_this_layer[template_group]\n","    #         z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n","    #         if z_means_for_this_template.shape[0] == 5:\n","    #             pdb.set_trace()\n","    #         means[layer, template_group] = z_means_for_this_template\n","\n","    # del(means_cache)\n","\n","    return means"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFDQMOt9CyVw"},"outputs":[],"source":["# def mask_circ_heads(\n","#     means_dataset: Dataset,\n","#     model: HookedTransformer,\n","#     circuit: Dict[str, List[Tuple[int, int]]],\n","#     seq_pos_to_keep: Dict[str, str],\n","# ) -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","#     '''\n","#     Output: for each layer, a mask of circuit components that should not be ablated\n","#     '''\n","#     heads_and_posns_to_keep = {}\n","#     batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","\n","#     for layer in range(model.cfg.n_layers):\n","\n","#         mask = t.zeros(size=(batch, seq, n_heads))\n","\n","#         for (head_type, head_list) in circuit.items():\n","#             seq_pos = seq_pos_to_keep[head_type]\n","#             # if seq_pos == 'S7':\n","#             #     pdb.set_trace()\n","#             indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","#             for (layer_idx, head_idx) in head_list:\n","#                 if layer_idx == layer:\n","#                     # if indices.item() == 7:\n","#                     #     pdb.set_trace()\n","#                     mask[:, indices, head_idx] = 1\n","#                     # mask[:, :, head_idx] = 1  # keep L.H at all pos\n","\n","#         heads_and_posns_to_keep[layer] = mask.bool()\n","#     # pdb.set_trace()\n","#     return heads_and_posns_to_keep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1boH1469_HI"},"outputs":[],"source":["def mask_circ_heads(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","    '''\n","    Output: for each layer, a mask of circuit components that should not be ablated\n","    '''\n","    heads_and_posns_to_keep = {}\n","    # batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","    batch, seq, n_heads = len(means_dataset), len(circuit.keys()), model.cfg.n_heads\n","    # print(seq)\n","\n","    for layer in range(model.cfg.n_layers):\n","\n","        mask = t.zeros(size=(batch, seq, n_heads))\n","\n","        for (head_type, head_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[head_type]\n","            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","            for (layer_idx, head_idx) in head_list:\n","                if layer_idx == layer:\n","                    # mask[:, indices, head_idx] = 1\n","                    mask[:, :, head_idx] = 1\n","\n","        heads_and_posns_to_keep[layer] = mask.bool()\n","\n","    return heads_and_posns_to_keep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdxeNJ5C9tHx"},"outputs":[],"source":["def hook_func_mask_head(\n","    z: Float[Tensor, \"batch seq head d_head\"],\n","    hook: HookPoint,\n","    # components_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n","    # means: Float[Tensor, \"layer batch seq head d_head\"],\n","    circuit: Dict[str, List[Tuple[int, int]]],\n",") -> Float[Tensor, \"batch seq head d_head\"]:\n","    '''\n","    Use this to not mask components\n","    '''\n","    # mask_for_this_layer = components_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n","    # z = t.where(mask_for_this_layer, z, means[hook.layer()])\n","\n","    ###\n","    # heads_and_posns_to_keep = {}\n","    # batch, seq, n_heads = z.shape[0], z.shape[1], model.cfg.n_heads  # components_to_keep[0].shape[0] is batch\n","\n","    # for layer in range(model.cfg.n_layers):\n","\n","    #     mask = t.zeros(size=(batch, seq, n_heads))\n","\n","    #     for (head_type, head_list) in circuit.items():\n","    #         # seq_pos = seq_pos_to_keep[head_type]\n","    #         # indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","    #         for (layer_idx, head_idx) in head_list:\n","    #             if layer_idx == layer:\n","    #                 # mask[:, indices, head_idx] = 1\n","    #                 mask[:, :, head_idx] = 1\n","\n","    #     heads_and_posns_to_keep[layer] = mask.bool()\n","    ###\n","    mask_for_this_layer = t.zeros(size=(z.shape[0], z.shape[1], z.shape[2]))\n","    for (head_type, head_list) in circuit.items():\n","        # seq_pos = seq_pos_to_keep[head_type]\n","        # indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","        for (layer_idx, head_idx) in head_list:\n","            if layer_idx == hook.layer():\n","                # mask[:, indices, head_idx] = 1\n","                mask_for_this_layer[:, :, head_idx] = 1\n","\n","    mask_for_this_layer = mask_for_this_layer.bool()\n","    mask_for_this_layer = mask_for_this_layer.unsqueeze(-1).to(z.device)  # d_model is 1; then is broadcast in where\n","\n","    z = t.where(mask_for_this_layer, z, 0)\n","\n","    return z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dg3XuWScAVvG"},"outputs":[],"source":["def add_ablation_hook_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Ablate the model, except as components and positions to keep\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","    means = get_heads_actv_mean(means_dataset, model)\n","    components_to_keep = mask_circ_heads(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    hook_fn = partial(\n","        hook_func_mask_head,\n","        # components_to_keep=components_to_keep,\n","        # means=means,\n","        circuit=circuit,\n","    )\n","\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ILjxwH9YUYP"},"outputs":[],"source":["# from dataset import Dataset\n","from transformer_lens import HookedTransformer, utils\n","from transformer_lens.hook_points import HookPoint\n","import einops\n","from functools import partial\n","import torch as t\n","from torch import Tensor\n","from typing import Dict, Tuple, List\n","from jaxtyping import Float, Bool\n","\n","# from head_ablation_fns import *\n","# from mlp_ablation_fns import *\n","\n","def add_ablation_hook_MLP_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    heads_lst, mlp_lst,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    # for i in range(len(model.tokenizer.tokenize(means_dataset.prompts[0]['text']))):\n","    num_pos = len(model.tokenizer(means_dataset.prompts[0]['text']).input_ids)\n","    for i in range(num_pos ):\n","        CIRCUIT['S'+str(i)] = heads_lst\n","        # if i == len(model.tokenizer.tokenize(means_dataset.prompts[0]['text'])) - 1:\n","        # if i == num_pos - 1:\n","        #     SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        # else:\n","        SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = get_heads_actv_mean(means_dataset, model)\n","    # Convert this into a boolean map\n","    components_to_keep = mask_circ_heads(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_func_mask_head,\n","        # components_to_keep=components_to_keep,\n","        # means=means,\n","        circuit=CIRCUIT,\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    # if all_entries_true(components_to_keep) == False:\n","    #     pdb.set_trace()\n","    ########################\n","    # CIRCUIT = {}\n","    # SEQ_POS_TO_KEEP = {}\n","    # # for i in range(len(model.tokenizer.tokenize(means_dataset.prompts[0]['text']))):\n","    # num_pos = len(model.tokenizer(means_dataset.prompts[0]['text']).input_ids)\n","    # for i in range(num_pos ):\n","    #     CIRCUIT['S'+str(i)] = mlp_lst\n","    #     # if i == len(model.tokenizer.tokenize(means_dataset.prompts[0]['text'])) - 1:\n","    #     # if i == num_pos - 1:\n","    #     #     SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","    #     # else:\n","    #     SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    # # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    # means = get_MLPs_actv_mean(means_dataset, model)\n","\n","    # # Convert this into a boolean map\n","    # components_to_keep = mask_circ_MLPs(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # # Get a hook function which will patch in the mean z values for each head, at\n","    # # all positions which aren't important for the circuit\n","    # hook_fn = partial(\n","    #     hook_func_mask_mlp_out,\n","    #     components_to_keep=components_to_keep,\n","    #     means=means\n","    # )\n","\n","    # model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-YuOEDieLgE"},"outputs":[],"source":["def all_entries_true(tensor_dict):\n","    for key, tensor in tensor_dict.items():\n","        if not torch.all(tensor).item():\n","            return False\n","    return True"]},{"cell_type":"markdown","metadata":{"id":"qCVySrfhk-YH"},"source":["# auto measure fns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Njf4fDdk_k9"},"outputs":[],"source":["def ablate_circ_autoScore(model, circuit, sequences_as_str, next_members):\n","    corr_text = \"5 3 9\"\n","    list_outputs = []\n","    score = 0\n","    for clean_text, correct_ans in zip(sequences_as_str, next_members):\n","        correct_ans_tokLen = clean_gen(model, clean_text, correct_ans)\n","\n","        heads_not_ablate = [(layer, head) for layer in range(32) for head in range(32)]  # unablated\n","        head_to_remove = circuit\n","        heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","\n","        mlps_not_ablate = [layer for layer in range(32)]\n","\n","        output_after_ablate = ablate_auto_score(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans_tokLen)\n","        list_outputs.append(output_after_ablate)\n","        print(correct_ans, output_after_ablate)\n","        if correct_ans == output_after_ablate:\n","            score += 1\n","    perc_score = score / len(next_members)\n","    return perc_score, list_outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CswxAMn4oRfW"},"outputs":[],"source":["def ablate_randcirc_autoScore(model, sequences_as_str, next_members, num_rand_runs, heads_not_overlap, num_heads_rand, num_not_overlap):\n","    corr_text = \"5 3 9\"\n","    list_outputs = []\n","    all_scores = []\n","    for clean_text, correct_ans in zip(sequences_as_str, next_members):\n","        prompt_score = 0\n","        correct_ans_tokLen = clean_gen(model, clean_text, correct_ans)\n","        for j in range(num_rand_runs):\n","            all_possible_pairs =  [(layer, head) for layer in range(32) for head in range(32)]\n","            filtered_pairs = [pair for pair in all_possible_pairs if pair not in heads_not_overlap] # Filter out heads_not_overlap from all_possible_pairs\n","\n","            # Randomly choose num_heads_rand pairs ensuring less than num_not_overlap overlaps with heads_not_overlap\n","            head_to_remove = choose_heads_to_remove(filtered_pairs, heads_not_overlap, num_heads_rand, num_not_overlap)\n","\n","            heads_not_ablate = [x for x in all_possible_pairs if x not in head_to_remove]\n","\n","            mlps_not_ablate = [layer for layer in range(32)]\n","\n","            output_after_ablate = ablate_auto_score(model, clean_text, corr_text, heads_not_ablate, mlps_not_ablate, correct_ans_tokLen)\n","            # list_outputs.append(output_after_ablate)\n","            # print(correct_ans, output_after_ablate)\n","            if correct_ans == output_after_ablate:\n","                prompt_score += 1\n","        print(prompt_score / num_rand_runs)\n","        all_scores.append(prompt_score / num_rand_runs)\n","\n","    perc_score = sum(all_scores) / len(next_members)\n","    return perc_score, list_outputs"]},{"cell_type":"markdown","source":["# dataset as token matrix"],"metadata":{"id":"jfQHPhWuHvuv"}},{"cell_type":"code","source":["max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids[1:])\n","                for prompt in self.prompts\n","            ]\n","        )"],"metadata":{"id":"7-8hpzAqK5xZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dbfQG9TjK7Ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1718309503310,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":-60},"id":"gxzgw-2noNdz","outputId":"eb4de744-d91e-4b14-e981-2a178e26482a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequences:\n","['1 2 3 ', '2 3 4 ', '3 4 5 ', '4 5 6 ', '5 6 7 ', '6 7 8 ', '7 8 9 ', '8 9 10 ', '9 10 11 ', '10 11 12 ', '11 12 13 ', '12 13 14 ', '13 14 15 ', '14 15 16 ', '15 16 17 ', '16 17 18 ', '17 18 19 ', '18 19 20 ', '19 20 21 ', '20 21 22 ', '21 22 23 ', '22 23 24 ', '23 24 25 ', '24 25 26 ', '25 26 27 ', '26 27 28 ', '27 28 29 ', '28 29 30 ', '29 30 31 ', '30 31 32 ', '31 32 33 ', '32 33 34 ', '33 34 35 ', '34 35 36 ', '35 36 37 ', '36 37 38 ', '37 38 39 ', '38 39 40 ', '39 40 41 ', '40 41 42 ', '41 42 43 ', '42 43 44 ', '43 44 45 ', '44 45 46 ', '45 46 47 ', '46 47 48 ', '47 48 49 ', '48 49 50 ', '49 50 51 ', '50 51 52 ']\n","\n","Next Members:\n","['4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53']\n"]}],"source":["interval = 1\n","start = 1\n","num_prompts = 50\n","sequences_as_str, next_members = gen_intervaled_seqs(interval, start, num_prompts)"]},{"cell_type":"code","source":["# toks = torch.Tensor(model.tokenizer(texts, padding=True).input_ids).type(torch.int)[:, 1:]\n","\n","tokens = model.to_tokens(sequences_as_str).to(device)\n","print(tokens.shape)"],"metadata":{"id":"LBvJ6_1THzPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens"],"metadata":{"id":"1li90YUoJejB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next_members"],"metadata":{"id":"fNEZnehMIMIo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corr_tokenIDs = []\n","\n","for correct_ansPos in range(len(ans_str_tok)):\n","    tokID = model.tokenizer.encode(ans_str_tok[correct_ansPos])[2:][0] # 2: to skip padding <s> and ''\n","    corr_tokenIDs.append(tokID)\n","correct_ans_tokLen = len(corr_tokenIDs)\n","\n","for ansPos in range(len(self.prompts[0]['corr'])):\n","    ansPos_corrTokIDS = [] # this is the inner list. each member is a promptID\n","    for promptID in range(len(self.prompts)):\n","        tokID = self.tokenizer.encode(self.prompts[promptID]['corr'][ansPos])[2:][0] # 2: to skip padding <s> and ''\n","        ansPos_corrTokIDS.append(tokID)\n","    self.corr_tokenIDs.append(ansPos_corrTokIDS)"],"metadata":{"id":"p4XOgIZXHyP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MM auto ablate"],"metadata":{"id":"1VJ6zJyAFCoC"}},{"cell_type":"code","source":["def ablate_auto_score(model, input_tokens, correct_ans_tokens, heads_not_ablate, mlps_not_ablate):\n","    # tokens = model.to_tokens(clean_text).to(device)\n","    # prompts_list = generate_prompts_list_longer(clean_text, tokens)\n","\n","    corr_tokens = model.to_tokens(corr_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    pos_dict = {}\n","    num_pos = len(model.tokenizer(prompts_list_2[0]['text']).input_ids)\n","    for i in range(num_pos ):\n","        pos_dict['S'+str(i)] = i\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    total_score = 0\n","    ans_so_far = ''\n","    ans_str_tok = tokenizer.tokenize(correct_ans)[1:] # correct_ans is str\n","    corr_tokenIDs = []\n","    for correct_ansPos in range(len(ans_str_tok)):\n","        tokID = model.tokenizer.encode(ans_str_tok[correct_ansPos])[2:][0] # 2: to skip padding <s> and ''\n","        corr_tokenIDs.append(tokID)\n","    correct_ans_tokLen = len(corr_tokenIDs)\n","    for ansPos in range(correct_ans_tokLen):\n","\n","        logits = model(tokens)\n","        next_token = logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","        next_char = model.to_string(next_token)\n","\n","        if next_char == '':\n","            next_char = ' '\n","\n","        clean_text = clean_text + next_char\n","\n","        tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","\n","        ans_so_far += next_char\n","        correct_ans_tokLen += 1\n","        # print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","        ansTok_IDs = torch.tensor(corr_tokenIDs[ansPos])\n","\n","        # new_score = get_logit_diff(logits, dataset)\n","        # total_score += new_score\n","        # corrTok_logits = logits[:, -1, next_token]\n","        corrTok_logits = logits[range(logits.size(0)), -1, ansTok_IDs]  # not next_token, as that's what's pred, not the token to measure\n","        total_score += corrTok_logits\n","        # print(f\"corr logit of new char: {new_score}\")\n","    # print('\\n Total corr logit: ', total_score.item())\n","    # return ans_so_far, total_score.item()\n","    return ans_so_far"],"metadata":{"id":"Q6_1lXxPFEXi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ablate_circ_autoScore(model, circuit, sequences_as_str, next_members):\n","    list_outputs = []\n","    score = 0\n","    # for clean_text, correct_ans in zip(sequences_as_str, next_members):\n","    correct_ans_tokLen = clean_gen(model, clean_text, correct_ans)\n","\n","    heads_not_ablate = [(layer, head) for layer in range(32) for head in range(32)]  # unablated\n","    head_to_remove = circuit\n","    heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","\n","    mlps_not_ablate = [layer for layer in range(32)]\n","\n","    output_after_ablate = ablate_auto_score(model, input_tokens, correct_ans_tokens, heads_not_ablate, mlps_not_ablate)\n","\n","    # list_outputs.append(output_after_ablate)\n","    # print(correct_ans, output_after_ablate)\n","    # if correct_ans == output_after_ablate:\n","    #     score += 1\n","\n","    perc_score = score / len(next_members)\n","    return perc_score, list_outputs"],"metadata":{"id":"dDX9g407GHAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = dataset.toks\n","tokens = tokens.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"cJiDu8IXFwqB"},"execution_count":null,"outputs":[]}]}