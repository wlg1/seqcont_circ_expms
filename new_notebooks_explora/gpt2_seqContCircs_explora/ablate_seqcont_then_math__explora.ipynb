{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["vKYgaZ9JjihZ","KenbMVMP7ZcY","6DDK7QaqELh1"],"machine_shape":"hm","toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyN0fZ1Kgb182fTaf7OmVCrB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e558d6eec4e549d1bd7f0493a13b8571":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f2e0c1260944f75af847f946e6f7032","IPY_MODEL_fdf89ee081c64b47a7bc18893fb6e18a","IPY_MODEL_8f999303098f40acb47faf7f7e678747"],"layout":"IPY_MODEL_81ee7a7fa5c0459388f257c88c36078b"}},"2f2e0c1260944f75af847f946e6f7032":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_085ac047d9444706993ab9def71a9ef1","placeholder":"​","style":"IPY_MODEL_1ce43d87e6414b849ffbccdc8d3760a1","value":"config.json: 100%"}},"fdf89ee081c64b47a7bc18893fb6e18a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_271c241d661142a6ac9c373f6fb619ce","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_089cae00fd1649f2bbec15388079ff13","value":665}},"8f999303098f40acb47faf7f7e678747":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f34045fb1f84144be32c1fc99abb116","placeholder":"​","style":"IPY_MODEL_c38b5766e6684c0d9da3d4791192c4c9","value":" 665/665 [00:00&lt;00:00, 51.3kB/s]"}},"81ee7a7fa5c0459388f257c88c36078b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"085ac047d9444706993ab9def71a9ef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ce43d87e6414b849ffbccdc8d3760a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"271c241d661142a6ac9c373f6fb619ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"089cae00fd1649f2bbec15388079ff13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f34045fb1f84144be32c1fc99abb116":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c38b5766e6684c0d9da3d4791192c4c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b34c063a20d84df1965315c74e9a0536":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_baf1b2b0a0654fe1955e98a2b2d3c402","IPY_MODEL_a3d39b0133034dbbbd52ca96f71308a8","IPY_MODEL_9651b45bb93a4d6984bed78eb731b003"],"layout":"IPY_MODEL_dffe2dcdaea54173907e199f8e36e552"}},"baf1b2b0a0654fe1955e98a2b2d3c402":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5b004a969f440d0b20856db7cd90916","placeholder":"​","style":"IPY_MODEL_1b7f5ddbb177440fb0226662201c1eef","value":"model.safetensors: 100%"}},"a3d39b0133034dbbbd52ca96f71308a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6627f6b96a06419092eb5c7a24a19cca","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9e1202c24c14ba7bec090c8275ab262","value":548105171}},"9651b45bb93a4d6984bed78eb731b003":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77378465a9844698a4ca6939a538cb41","placeholder":"​","style":"IPY_MODEL_5cc731edf16241a9a47a0ea028d80d9e","value":" 548M/548M [00:01&lt;00:00, 441MB/s]"}},"dffe2dcdaea54173907e199f8e36e552":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5b004a969f440d0b20856db7cd90916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b7f5ddbb177440fb0226662201c1eef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6627f6b96a06419092eb5c7a24a19cca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9e1202c24c14ba7bec090c8275ab262":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77378465a9844698a4ca6939a538cb41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cc731edf16241a9a47a0ea028d80d9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45ff176bdd544c07858101b8adaa3bf4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a0d5973902f44a985cd298290ebc74a","IPY_MODEL_0fe4a0378df54b93b29ae3d98acf62e2","IPY_MODEL_57f05f3959b5436c8d056353015a5140"],"layout":"IPY_MODEL_a2fd489461fe4e74be99951d42a9b9ce"}},"6a0d5973902f44a985cd298290ebc74a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8efb482367dd4901ba29d6f680f14456","placeholder":"​","style":"IPY_MODEL_8d6f92dbfe944e07bc637aa2dcf9e9ee","value":"generation_config.json: 100%"}},"0fe4a0378df54b93b29ae3d98acf62e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc7a11953fd94ea78c20e994f7e4e382","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3faa4988039e42caa44e0759942de431","value":124}},"57f05f3959b5436c8d056353015a5140":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_beb914427f7546fc8f562a668cf58ba6","placeholder":"​","style":"IPY_MODEL_9955906d108b4ea2b2e55f20957bf432","value":" 124/124 [00:00&lt;00:00, 11.9kB/s]"}},"a2fd489461fe4e74be99951d42a9b9ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8efb482367dd4901ba29d6f680f14456":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d6f92dbfe944e07bc637aa2dcf9e9ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc7a11953fd94ea78c20e994f7e4e382":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3faa4988039e42caa44e0759942de431":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"beb914427f7546fc8f562a668cf58ba6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9955906d108b4ea2b2e55f20957bf432":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c242b746b6844e0bdb9dc31e8bdc71c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa43e5c5b3ea4db0ad1c2b8f791ad394","IPY_MODEL_d2920eeb20ab4c66944b851c7d768ae1","IPY_MODEL_81783daae8604a6a8eeeabfcd035b7d4"],"layout":"IPY_MODEL_d1be01ac8c5a4666be19c13587022c87"}},"fa43e5c5b3ea4db0ad1c2b8f791ad394":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98207c120dab4193b8eabbbe8613edcc","placeholder":"​","style":"IPY_MODEL_2eb18ccf4ed14218bbf549315a6ff5c8","value":"tokenizer_config.json: 100%"}},"d2920eeb20ab4c66944b851c7d768ae1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfa4ffa45bc94173ae0b61e914ea1e6e","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7370024bb5d54683af60db2c4828baa0","value":26}},"81783daae8604a6a8eeeabfcd035b7d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_493f35a3d53b45798a83443a9b8253f0","placeholder":"​","style":"IPY_MODEL_521359e0368440d0b048dca70022b576","value":" 26.0/26.0 [00:00&lt;00:00, 2.19kB/s]"}},"d1be01ac8c5a4666be19c13587022c87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98207c120dab4193b8eabbbe8613edcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eb18ccf4ed14218bbf549315a6ff5c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfa4ffa45bc94173ae0b61e914ea1e6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7370024bb5d54683af60db2c4828baa0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"493f35a3d53b45798a83443a9b8253f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"521359e0368440d0b048dca70022b576":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a49868fe31cc49e191da03a22d26746a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_264eb578c1b64ca3bb89277cfbdcfe05","IPY_MODEL_c3531457f6e14320ba8881f7922c9122","IPY_MODEL_b57708afb55a4ce89bc86b978e268443"],"layout":"IPY_MODEL_e2e7401c89a745eaa82ddae3f96b5bd9"}},"264eb578c1b64ca3bb89277cfbdcfe05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b86ee689c39435992fcaf35ae36be88","placeholder":"​","style":"IPY_MODEL_26ee1ca5ee2b46e283394ee8496f92ea","value":"vocab.json: 100%"}},"c3531457f6e14320ba8881f7922c9122":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f50b8e0971614a16b21f204f16787abb","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f533f57b67544a99722ac125e7c4c87","value":1042301}},"b57708afb55a4ce89bc86b978e268443":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b95be96ec8a14021ba1f69bcc8732cb1","placeholder":"​","style":"IPY_MODEL_2163a40c520f4189a77c83931761d495","value":" 1.04M/1.04M [00:00&lt;00:00, 1.45MB/s]"}},"e2e7401c89a745eaa82ddae3f96b5bd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b86ee689c39435992fcaf35ae36be88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26ee1ca5ee2b46e283394ee8496f92ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f50b8e0971614a16b21f204f16787abb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f533f57b67544a99722ac125e7c4c87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b95be96ec8a14021ba1f69bcc8732cb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2163a40c520f4189a77c83931761d495":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c2694e455bb468c88dbe0f0aa188578":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b76a05f882a4ade9d5384e90ed71362","IPY_MODEL_ffb5f6aab1334862a0cb51bb8451d576","IPY_MODEL_27bc3ab5f66b4ed5adfaeb750ebf8282"],"layout":"IPY_MODEL_6de4559fa3434a5f8be8a8c3de0dc94f"}},"5b76a05f882a4ade9d5384e90ed71362":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e400cdcdd6245218e56b5874227569f","placeholder":"​","style":"IPY_MODEL_20816f12defd419cb9383a6a6a9fdf9d","value":"merges.txt: 100%"}},"ffb5f6aab1334862a0cb51bb8451d576":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2d2f82ebfc54ce9b3a27f16bd77dc31","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc3e5f63589e4a418c2a112f4fa9c04a","value":456318}},"27bc3ab5f66b4ed5adfaeb750ebf8282":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c8674c1c49647398521e4a24121c0fe","placeholder":"​","style":"IPY_MODEL_6c0a41534cbb4fbbbdc1ac106748a2e3","value":" 456k/456k [00:00&lt;00:00, 1.93MB/s]"}},"6de4559fa3434a5f8be8a8c3de0dc94f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e400cdcdd6245218e56b5874227569f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20816f12defd419cb9383a6a6a9fdf9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2d2f82ebfc54ce9b3a27f16bd77dc31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3e5f63589e4a418c2a112f4fa9c04a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c8674c1c49647398521e4a24121c0fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c0a41534cbb4fbbbdc1ac106748a2e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03febec4a990427384963b26f8343354":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c0a819193abc492c99d0008abd5d3783","IPY_MODEL_cd7dc258678c4936951192e8c6c0510e","IPY_MODEL_b9fec22c33234673bebf5f31c2ad47d7"],"layout":"IPY_MODEL_8f00ea03956c4b169a803acaa0459fb9"}},"c0a819193abc492c99d0008abd5d3783":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b43530edd5604c408785870fc815d673","placeholder":"​","style":"IPY_MODEL_aea29beb65cc4198a85d2dc664983dbe","value":"tokenizer.json: 100%"}},"cd7dc258678c4936951192e8c6c0510e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c146092ef44483fb4f30638bee159ec","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dcc5011273904b58b044e589d661de6d","value":1355256}},"b9fec22c33234673bebf5f31c2ad47d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_500b10bc88ff486a9e233e58ef70a9f8","placeholder":"​","style":"IPY_MODEL_75f50efc50144dc2b16699e36715f2c8","value":" 1.36M/1.36M [00:00&lt;00:00, 5.65MB/s]"}},"8f00ea03956c4b169a803acaa0459fb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b43530edd5604c408785870fc815d673":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aea29beb65cc4198a85d2dc664983dbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c146092ef44483fb4f30638bee159ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcc5011273904b58b044e589d661de6d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"500b10bc88ff486a9e233e58ef70a9f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75f50efc50144dc2b16699e36715f2c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Change Inputs Here"],"metadata":{"id":"vKYgaZ9JjihZ"}},{"cell_type":"code","source":["task = \"numerals\"  # choose: numerals, numwords, months\n","prompt_types = ['done', 'lost', 'names']\n","num_samps_per_ptype = 512 #768 512\n","\n","model_name = \"gpt2-small\"\n","\n","save_files = True\n","run_on_other_tasks = True"],"metadata":{"id":"KSKP_OsTDki6","executionInfo":{"status":"ok","timestamp":1716760653168,"user_tz":240,"elapsed":20,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup"]},{"cell_type":"code","source":["%%capture\n","%pip install git+https://github.com/neelnanda-io/TransformerLens.git"],"metadata":{"id":"F1wsEy0MqHU0","executionInfo":{"status":"ok","timestamp":1716760731078,"user_tz":240,"elapsed":77929,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Z6b1n2tvIAiD","executionInfo":{"status":"ok","timestamp":1716760734602,"user_tz":240,"elapsed":3543,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","# import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML\n","\n","import pickle\n","from google.colab import files\n","\n","import matplotlib.pyplot as plt\n","import statistics"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zuhzYxbsIAiE","executionInfo":{"status":"ok","timestamp":1716760737709,"user_tz":240,"elapsed":3118,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"cFMTUcQiIAiF","executionInfo":{"status":"ok","timestamp":1716760737709,"user_tz":240,"elapsed":124,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["torch.set_grad_enabled(False)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"OLkInsdjyHMx"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"xLwDyosvIAiJ","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["e558d6eec4e549d1bd7f0493a13b8571","2f2e0c1260944f75af847f946e6f7032","fdf89ee081c64b47a7bc18893fb6e18a","8f999303098f40acb47faf7f7e678747","81ee7a7fa5c0459388f257c88c36078b","085ac047d9444706993ab9def71a9ef1","1ce43d87e6414b849ffbccdc8d3760a1","271c241d661142a6ac9c373f6fb619ce","089cae00fd1649f2bbec15388079ff13","3f34045fb1f84144be32c1fc99abb116","c38b5766e6684c0d9da3d4791192c4c9","b34c063a20d84df1965315c74e9a0536","baf1b2b0a0654fe1955e98a2b2d3c402","a3d39b0133034dbbbd52ca96f71308a8","9651b45bb93a4d6984bed78eb731b003","dffe2dcdaea54173907e199f8e36e552","c5b004a969f440d0b20856db7cd90916","1b7f5ddbb177440fb0226662201c1eef","6627f6b96a06419092eb5c7a24a19cca","d9e1202c24c14ba7bec090c8275ab262","77378465a9844698a4ca6939a538cb41","5cc731edf16241a9a47a0ea028d80d9e","45ff176bdd544c07858101b8adaa3bf4","6a0d5973902f44a985cd298290ebc74a","0fe4a0378df54b93b29ae3d98acf62e2","57f05f3959b5436c8d056353015a5140","a2fd489461fe4e74be99951d42a9b9ce","8efb482367dd4901ba29d6f680f14456","8d6f92dbfe944e07bc637aa2dcf9e9ee","dc7a11953fd94ea78c20e994f7e4e382","3faa4988039e42caa44e0759942de431","beb914427f7546fc8f562a668cf58ba6","9955906d108b4ea2b2e55f20957bf432","2c242b746b6844e0bdb9dc31e8bdc71c","fa43e5c5b3ea4db0ad1c2b8f791ad394","d2920eeb20ab4c66944b851c7d768ae1","81783daae8604a6a8eeeabfcd035b7d4","d1be01ac8c5a4666be19c13587022c87","98207c120dab4193b8eabbbe8613edcc","2eb18ccf4ed14218bbf549315a6ff5c8","bfa4ffa45bc94173ae0b61e914ea1e6e","7370024bb5d54683af60db2c4828baa0","493f35a3d53b45798a83443a9b8253f0","521359e0368440d0b048dca70022b576","a49868fe31cc49e191da03a22d26746a","264eb578c1b64ca3bb89277cfbdcfe05","c3531457f6e14320ba8881f7922c9122","b57708afb55a4ce89bc86b978e268443","e2e7401c89a745eaa82ddae3f96b5bd9","7b86ee689c39435992fcaf35ae36be88","26ee1ca5ee2b46e283394ee8496f92ea","f50b8e0971614a16b21f204f16787abb","7f533f57b67544a99722ac125e7c4c87","b95be96ec8a14021ba1f69bcc8732cb1","2163a40c520f4189a77c83931761d495","3c2694e455bb468c88dbe0f0aa188578","5b76a05f882a4ade9d5384e90ed71362","ffb5f6aab1334862a0cb51bb8451d576","27bc3ab5f66b4ed5adfaeb750ebf8282","6de4559fa3434a5f8be8a8c3de0dc94f","2e400cdcdd6245218e56b5874227569f","20816f12defd419cb9383a6a6a9fdf9d","c2d2f82ebfc54ce9b3a27f16bd77dc31","cc3e5f63589e4a418c2a112f4fa9c04a","3c8674c1c49647398521e4a24121c0fe","6c0a41534cbb4fbbbdc1ac106748a2e3","03febec4a990427384963b26f8343354","c0a819193abc492c99d0008abd5d3783","cd7dc258678c4936951192e8c6c0510e","b9fec22c33234673bebf5f31c2ad47d7","8f00ea03956c4b169a803acaa0459fb9","b43530edd5604c408785870fc815d673","aea29beb65cc4198a85d2dc664983dbe","8c146092ef44483fb4f30638bee159ec","dcc5011273904b58b044e589d661de6d","500b10bc88ff486a9e233e58ef70a9f8","75f50efc50144dc2b16699e36715f2c8"],"height":422},"executionInfo":{"status":"ok","timestamp":1716760750339,"user_tz":240,"elapsed":12753,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ea69fd78-8e02-4f4e-9dd8-4bd5ba5c894d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e558d6eec4e549d1bd7f0493a13b8571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34c063a20d84df1965315c74e9a0536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45ff176bdd544c07858101b8adaa3bf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c242b746b6844e0bdb9dc31e8bdc71c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a49868fe31cc49e191da03a22d26746a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2694e455bb468c88dbe0f0aa188578"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03febec4a990427384963b26f8343354"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    model_name,\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","source":["## Import functions from repo"],"metadata":{"id":"Z4iJEGh6b56v"}},{"cell_type":"code","source":["!git clone https://github.com/apartresearch/seqcont_circuits.git\n","%cd /content/seqcont_circuits/src/iter_node_pruning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716760753238,"user_tz":240,"elapsed":2923,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ada20fea-6956-41e0-c290-59d2fe66f16a","id":"F8TXMRL3CoPd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'seqcont_circuits'...\n","remote: Enumerating objects: 831, done.\u001b[K\n","remote: Counting objects: 100% (297/297), done.\u001b[K\n","remote: Compressing objects: 100% (197/197), done.\u001b[K\n","remote: Total 831 (delta 161), reused 221 (delta 89), pack-reused 534\u001b[K\n","Receiving objects: 100% (831/831), 16.50 MiB | 14.81 MiB/s, done.\n","Resolving deltas: 100% (524/524), done.\n","/content/seqcont_circuits/src/iter_node_pruning\n"]}]},{"cell_type":"code","source":["## comment this out when debugging functions in colab to use funcs defined in colab\n","\n","# don't improt this\n","# # from dataset import Dataset\n","\n","\n","from metrics import *\n","from head_ablation_fns import *\n","from mlp_ablation_fns import *\n","from node_ablation_fns import *\n","from loop_node_ablation_fns import *"],"metadata":{"id":"22TI4zjMDMfQ","executionInfo":{"status":"ok","timestamp":1716761090818,"user_tz":240,"elapsed":106,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["## fns"],"metadata":{"id":"9R_g1Ghv7cGE"}},{"cell_type":"code","source":["# import pdb"],"metadata":{"id":"bQr6WtEppHgy","executionInfo":{"status":"ok","timestamp":1716760893518,"user_tz":240,"elapsed":156,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# from transformer_lens import HookedTransformer, utils\n","# from transformer_lens.hook_points import HookPoint\n","# import einops\n","# from functools import partial\n","# import torch as t\n","# from torch import Tensor\n","# from typing import Dict, Tuple, List\n","# from jaxtyping import Float, Bool"],"metadata":{"id":"9zH9yt-Z395D","executionInfo":{"status":"ok","timestamp":1716760893519,"user_tz":240,"elapsed":156,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import numpy as np\n","\n","# class Dataset:\n","#     def __init__(self, prompts, pos_dict, tokenizer):  # , S1_is_first=False\n","#         self.prompts = prompts\n","#         self.tokenizer = tokenizer\n","#         self.N = len(prompts)\n","#         self.max_len = max(\n","#             [\n","#                 len(self.tokenizer(prompt[\"text\"]).input_ids)\n","#                 for prompt in self.prompts\n","#             ]\n","#         )\n","#         all_ids = [0 for prompt in self.prompts] # only 1 template\n","#         all_ids_ar = np.array(all_ids)\n","#         self.groups = []\n","#         for id in list(set(all_ids)):\n","#             self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","#         texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","#         self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","#             torch.int\n","#         )\n","#         self.corr_tokenIDs = [\n","#             # self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","#             self.tokenizer.encode(prompt[\"corr\"])[0] for prompt in self.prompts\n","#         ]\n","#         self.incorr_tokenIDs = [\n","#             # self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","#             self.tokenizer.encode(prompt[\"incorr\"])[0] for prompt in self.prompts\n","#         ]\n","\n","#         # word_idx: for every prompt, find the token index of each target token and \"end\"\n","#         # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","#         self.word_idx = {}\n","#         # for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","#         for targ in [key for key in pos_dict]:\n","#             targ_lst = []\n","#             for prompt in self.prompts:\n","#                 input_text = prompt[\"text\"]\n","#                 tokens = self.tokenizer.tokenize(input_text)\n","#                 # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","#                 #     target_token = prompt[targ]\n","#                 # else:\n","#                 #     target_token = \"Ġ\" + prompt[targ]\n","#                 # target_index = tokens.index(target_token)\n","#                 target_index = pos_dict[targ]\n","#                 targ_lst.append(target_index)\n","#             self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","#         targ_lst = []\n","#         for prompt in self.prompts:\n","#             input_text = prompt[\"text\"]\n","#             tokens = self.tokenizer.tokenize(input_text)\n","#             end_token_index = len(tokens) - 1\n","#             targ_lst.append(end_token_index)\n","#         self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","#     def __len__(self):\n","#         return self.N"],"metadata":{"id":"nza97Rs635O0","executionInfo":{"status":"ok","timestamp":1716760893519,"user_tz":240,"elapsed":156,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def get_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","    '''\n","    '''\n","    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n","    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n","    answer_logit_diff = corr_logits - incorr_logits\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"],"metadata":{"id":"sh01hn4v4A18","executionInfo":{"status":"ok","timestamp":1716760893519,"user_tz":240,"elapsed":156,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["def get_heads_actv_mean(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Output: The mean activations of a head's output\n","    '''\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"z\"),\n","    )\n","    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n","\n","    for layer in range(model.cfg.n_layers):\n","        z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n","        for template_group in means_dataset.groups:\n","            z_for_this_template = z_for_this_layer[template_group]\n","            z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n","            means[layer, template_group] = z_means_for_this_template\n","\n","    del(means_cache)\n","\n","    return means\n","\n","def mask_circ_heads(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","    '''\n","    Output: for each layer, a mask of circuit components that should not be ablated\n","    '''\n","    heads_and_posns_to_keep = {}\n","    batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","\n","    for layer in range(model.cfg.n_layers):\n","\n","        mask = t.zeros(size=(batch, seq, n_heads))\n","\n","        for (head_type, head_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[head_type]\n","            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","            for (layer_idx, head_idx) in head_list:\n","                if layer_idx == layer:\n","                    mask[:, indices, head_idx] = 1\n","\n","        heads_and_posns_to_keep[layer] = mask.bool()\n","\n","    return heads_and_posns_to_keep\n","\n","def hook_func_mask_head(\n","    z: Float[Tensor, \"batch seq head d_head\"],\n","    hook: HookPoint,\n","    components_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n","    means: Float[Tensor, \"layer batch seq head d_head\"],\n",") -> Float[Tensor, \"batch seq head d_head\"]:\n","    '''\n","    Use this to not mask components\n","    '''\n","    # print(hook.layer())\n","    # print(z.shape)\n","    # print(means[hook.layer()].shape)\n","\n","    mask_for_this_layer = components_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n","    z = t.where(mask_for_this_layer, z, means[hook.layer()])\n","\n","    return z\n","\n","def add_ablation_hook_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Ablate the model, except as components and positions to keep\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","    means = get_heads_actv_mean(means_dataset, model)\n","    components_to_keep = mask_circ_heads(means_dataset, model, circuit, seq_pos_to_keep)\n","    # pdb.set_trace()\n","\n","    hook_fn = partial(\n","        hook_func_mask_head,\n","        components_to_keep=components_to_keep,\n","        means=means\n","    )\n","\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","    return model\n","\n","def ablate_head_from_full(\n","        lst: List[Tuple[int, int]],\n","        model: HookedTransformer,\n","        dataset: Dataset,\n","        dataset_2: Dataset,\n","        orig_score: float,\n","        print_output: bool = True,\n",") -> float:\n","    # CIRCUIT contains the components to not ablate\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(dataset_2.prompts[0]['text']))):\n","        CIRCUIT['S'+str(i)] = lst\n","        if i == len(model.tokenizer.tokenize(dataset_2.prompts[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    model = add_ablation_hook_head(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    logits_minimal = model(dataset.toks)\n","\n","    new_score = get_logit_diff(logits_minimal, dataset)\n","    if print_output:\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    return 100 * new_score / orig_score\n"],"metadata":{"id":"vxS06MvQJWlu","executionInfo":{"status":"ok","timestamp":1716760893519,"user_tz":240,"elapsed":156,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["# Load datasets"],"metadata":{"id":"6Fuq8XW770vX"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer):  # , S1_is_first=False\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.corr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.incorr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        # for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","        for targ in [key for key in pos_dict]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = self.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"6NPjHv-Xny4R","executionInfo":{"status":"ok","timestamp":1716763392820,"user_tz":240,"elapsed":1102,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["# prompts_list = []\n","\n","# for i in prompt_types:\n","#     file_name = f'/content/seqcont_circuits/data/{task}/{task}_prompts_{i}.pkl'\n","#     with open(file_name, 'rb') as file:\n","#         filelist = pickle.load(file)\n","\n","#     print(filelist[0]['text'])\n","#     prompts_list += filelist [:num_samps_per_ptype]\n","\n","# len(prompts_list)"],"metadata":{"id":"CIe5yXuDhgEK","executionInfo":{"status":"ok","timestamp":1716763393307,"user_tz":240,"elapsed":62,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x ,y):\n","    prompts_list = []\n","    for i in range(x, y):\n","        prompt_dict = {\n","            'S1': str(i),\n","            'S2': str(i+1),\n","            'S3': str(i+2),\n","            'S4': str(i+3),\n","            'corr': str(i+4),\n","            'incorr': str(i+3),\n","            'text': f\"{i} {i+1} {i+2} {i+3}\"\n","        }\n","        prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","prompts_list = generate_prompts_list(1, 2)\n","prompts_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvAkKJI06Vt5","executionInfo":{"status":"ok","timestamp":1716763393307,"user_tz":240,"elapsed":62,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cee63504-4fce-4002-d944-8b6c6064d097"},"execution_count":119,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'S1': '1',\n","  'S2': '2',\n","  'S3': '3',\n","  'S4': '4',\n","  'corr': '5',\n","  'incorr': '4',\n","  'text': '1 2 3 4'}]"]},"metadata":{},"execution_count":119}]},{"cell_type":"code","source":["pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list[0]['text']))):\n","    pos_dict['S'+str(i)] = i"],"metadata":{"id":"kS_Tlrb_70vg","executionInfo":{"status":"ok","timestamp":1716763393307,"user_tz":240,"elapsed":60,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":120,"outputs":[]},{"cell_type":"code","source":["dataset = Dataset(prompts_list, pos_dict, model.tokenizer)"],"metadata":{"id":"u0NPSKcZ1iDe","executionInfo":{"status":"ok","timestamp":1716763393307,"user_tz":240,"elapsed":60,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["# file_name = f'/content/seqcont_circuits/data/{task}/randDS_{task}.pkl'\n","# with open(file_name, 'rb') as file:\n","#     prompts_list_2 = pickle.load(file)"],"metadata":{"id":"-GJ_ZC48FB1i","executionInfo":{"status":"ok","timestamp":1716763393308,"user_tz":240,"elapsed":61,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","def generate_prompts_list_corr(prompt_list):\n","    outlist = []\n","    # for i in range(100):\n","    for prompt_dict in prompts_list:\n","        r1 = random.randint(1, 12)\n","        r2 = random.randint(1, 12)\n","        while True:\n","            r3 = random.randint(1, 12)\n","            r4 = random.randint(1, 12)\n","            if r4 - 1 != r3:\n","                break\n","        new_text = prompt_dict['text'].replace(prompt_dict['S1'], str(r1)).replace(prompt_dict['S2'], str(r2)).replace(prompt_dict['S3'], str(r3)).replace(prompt_dict['S4'], str(r4))\n","        new_prompt_dict = {\n","            'S1': str(r1),\n","            'S2': str(r2),\n","            'S3': str(r3),\n","            'S4': str(r4),\n","            'corr': prompt_dict['corr'],\n","            'incorr': prompt_dict['incorr'],\n","            'text': new_text\n","        }\n","        outlist.append(new_prompt_dict)\n","    return outlist\n","prompts_list_2 = generate_prompts_list_corr(prompts_list)\n","len(prompts_list_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Jtstw1o7Gkj","executionInfo":{"status":"ok","timestamp":1716763393308,"user_tz":240,"elapsed":61,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d5916993-b18f-47f5-80e9-b96d85f6ddf7"},"execution_count":123,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":123}]},{"cell_type":"code","source":["dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)"],"metadata":{"id":"msu6D4p_feW5","executionInfo":{"status":"ok","timestamp":1716763393308,"user_tz":240,"elapsed":59,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":["## Get orig score"],"metadata":{"id":"BHHvz84w70vh"}},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)\n","logits_original = model(dataset.toks)\n","orig_score = get_logit_diff(logits_original, dataset)\n","orig_score"],"metadata":{"id":"OI3FcmpMaNxB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716763393308,"user_tz":240,"elapsed":59,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cfa60bb7-c5a4-4f2b-9a72-022528ef0a89"},"execution_count":125,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(6.0631, device='cuda:0')"]},"metadata":{},"execution_count":125}]},{"cell_type":"code","source":["import gc\n","\n","del(logits_original)\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"executionInfo":{"status":"ok","timestamp":1716763393308,"user_tz":240,"elapsed":58,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"A-TjmW5PUwGC","outputId":"b4a479c8-9cdc-4d97-9fbc-f00951bed10d"},"execution_count":126,"outputs":[{"output_type":"execute_result","data":{"text/plain":["474"]},"metadata":{},"execution_count":126}]},{"cell_type":"markdown","source":["# Generate- Unablated"],"metadata":{"id":"UbR3j-75oLqU"}},{"cell_type":"markdown","source":["Generate output in GPT-2 ()"],"metadata":{"id":"lp8XaAfuntDE"}},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1716760907985,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"mommexyJm4R3"},"outputs":[],"source":["reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","\n","logits, cache = model.run_with_cache(tokens)\n","# probs = logits.softmax(dim=-1)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1716760908351,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"Pp9PB99Mm4R4","outputId":"a6edfa1a-e2ad-4f74-9292-3faf4be9c975"},"outputs":[{"output_type":"stream","name":"stdout","text":["' Wednesday'\n"]}],"source":["next_token = logits[0, -1].argmax(dim=-1)  # logits have shape [1, sequence_length, vocab_size]\n","next_char = model.to_string(next_token)\n","print(repr(next_char))"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1716760908701,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"dpgN-auXm4R6","outputId":"30c2ff55-892e-42ff-e789-d2fbe05bb629"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '<|endoftext|>What comes after Monday is Tuesday, and two days after is'\n","14th char = ' Wednesday'\n","15th char = '.'\n","16th char = '\\n'\n","17th char = '\\n'\n","18th char = 'The'\n","19th char = ' first'\n","20th char = ' is'\n","21th char = ' the'\n","22th char = ' day'\n","23th char = ' after'\n"]}],"source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","for i in range(10):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","    # Define new input sequence, by appending the previously generated token\n","    tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)"]},{"cell_type":"code","source":["circ = [(layer, head) for layer in range(12) for head in range(12)]\n","to_loop = [(9, 1)]\n","\n","lh_scores = {}\n","for lh in to_loop:\n","    copy_circuit = circ.copy()\n","    copy_circuit.remove(lh)\n","    print(\"removed: \" + str(lh))\n","    new_score = ablate_head_from_full(copy_circuit, model, dataset, dataset_2, orig_score, print_output=True).item()\n","    lh_scores[lh] = new_score"],"metadata":{"id":"4WB1EYT33YMT","executionInfo":{"status":"ok","timestamp":1716760909911,"user_tz":240,"elapsed":1228,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3fb1db51-bba2-4ad2-fb41-17fab7c8138d"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["removed: (9, 1)\n","Average logit difference (circuit / full) %: 88.5788\n"]}]},{"cell_type":"markdown","source":["# Generate- Ablated"],"metadata":{"id":"MLIHcn-gJUc-"}},{"cell_type":"markdown","source":["## new"],"metadata":{"id":"KenbMVMP7ZcY"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","# heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","heads_not_ablate = []  # ablate all heads but not MLPs\n","mlps_not_ablate = []  # ablate all MLPs\n","\n","# CIRCUIT = {}\n","# SEQ_POS_TO_KEEP = {}\n","# for i in range(len(model.tokenizer.tokenize(dataset_2.prompts[0]['text']))):\n","#     CIRCUIT['S'+str(i)] = lst\n","#     if i == len(model.tokenizer.tokenize(dataset_2.prompts[0]['text'])) - 1:\n","#         SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","#     else:\n","#         SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","# model = add_ablation_hook_head(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","logits_minimal = model(dataset.toks)\n","\n","new_score = get_logit_diff(logits_minimal, dataset)\n","new_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wJGVCAt7XX9","executionInfo":{"status":"ok","timestamp":1716761144723,"user_tz":240,"elapsed":572,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"79868575-5313-41f6-9e52-a6997f3c16b5"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.3340, device='cuda:0')"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["# reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","reference_text = '1 2 3 4'\n","tokens = model.to_tokens(reference_text).to(device)\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLUKkQ2zT7e_","executionInfo":{"status":"ok","timestamp":1716761144724,"user_tz":240,"elapsed":165,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f8a5b899-3fd8-4338-e9f2-03d9f6d1f35d"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[50256,    16,   362,   513,   604]], device='cuda:0')"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["tokens = tokens[:, 1:]\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"roR0FcvMT9fq","executionInfo":{"status":"ok","timestamp":1716761144724,"user_tz":240,"elapsed":163,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cbfa7dcd-a31d-4a13-8581-014bd1d10b02"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 16, 362, 513, 604]], device='cuda:0')"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","for i in range(1):\n","    # Define new input sequence, by appending the previously generated token\n","    # tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vM0CTQy9cJ1","executionInfo":{"status":"ok","timestamp":1716761144724,"user_tz":240,"elapsed":163,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1a3da434-b00a-4fd3-ac6a-7614252f4e35"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3 4'\n","5th char = ' 3'\n"]}]},{"cell_type":"code","source":["# example_prompt = \"1 2 3\"\n","# example_answer = \" 4\"\n","# # need prepend_bos=False to prev adding EOS token in front\n","# utils.test_prompt(example_prompt, example_answer, model, prepend_bos=False)"],"metadata":{"id":"JQok2e8YN4ZK","executionInfo":{"status":"ok","timestamp":1716761144724,"user_tz":240,"elapsed":162,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  # reset to unablated\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","for i in range(1):\n","    # Define new input sequence, by appending the previously generated token\n","    # tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n","    # Pass our new sequence through the model, to get new output\n","    logits_unabl = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits_unabl[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_KJPSUAUaR5","executionInfo":{"status":"ok","timestamp":1716761144724,"user_tz":240,"elapsed":161,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3a24c087-c950-433f-e8da-cd0808fbda7e"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3 4'\n","5th char = ' 5'\n"]}]},{"cell_type":"code","source":["logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOIL2K_qYr9x","executionInfo":{"status":"ok","timestamp":1716761144724,"user_tz":240,"elapsed":161,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2be32df9-619b-4fa6-d1e9-2be003fb9905"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 4, 50257])"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["logits_unabl.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wwx2sp-hYqMO","executionInfo":{"status":"ok","timestamp":1716761144724,"user_tz":240,"elapsed":160,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"167ad7d8-50ee-4cac-fe8f-eef00dffa14d"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 4, 50257])"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["logits == logits_unabl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6pD-EiG9UgRo","executionInfo":{"status":"ok","timestamp":1716761144724,"user_tz":240,"elapsed":159,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"976210a6-7394-492a-de06-fbd60c08cb6c"},"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[False, False, False,  ..., False, False, False],\n","         [False, False, False,  ..., False, False, False],\n","         [False, False, False,  ..., False, False, False],\n","         [False, False, False,  ..., False, False, False]]], device='cuda:0')"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["# model.reset_hooks(including_permanent=True)\n","\n","# example_prompt = \"1 2 3\"\n","# example_answer = \" 4\"\n","# # need prepend_bos=False to prev adding EOS token in front\n","# utils.test_prompt(example_prompt, example_answer, model, prepend_bos=False)"],"metadata":{"id":"KljXuqsRUkGW","executionInfo":{"status":"ok","timestamp":1716761144725,"user_tz":240,"elapsed":160,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":["## ablate head 9.1 and mlp 9 and see if corr"],"metadata":{"id":"nz_PIb7-a33I"}},{"cell_type":"markdown","source":["This is necessary (AND) beacuse is seeing if components are essential (no backups)"],"metadata":{"id":"vuE6mbM3bFPn"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","heads_not_ablate.remove((9, 1))\n","mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","logits_minimal = model(dataset.toks)\n","\n","new_score = get_logit_diff(logits_minimal, dataset)\n","new_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AvVrQx8Za6Ai","executionInfo":{"status":"ok","timestamp":1716761144725,"user_tz":240,"elapsed":160,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0bd7abcc-8eeb-4cbd-be7a-8a1805d4b2da"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.3742, device='cuda:0')"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","for i in range(1):\n","    # Define new input sequence, by appending the previously generated token\n","    # tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVM5ZLmqcJTk","executionInfo":{"status":"ok","timestamp":1716761144725,"user_tz":240,"elapsed":159,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f97e0e86-63c4-45a3-b2a4-145b47613da4"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3 4'\n","5th char = ' 4'\n"]}]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  # reset to unablated\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","for i in range(1):\n","    # Define new input sequence, by appending the previously generated token\n","    # tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n","    # Pass our new sequence through the model, to get new output\n","    logits_unabl = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits_unabl[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nzgc_EbHcPhf","executionInfo":{"status":"ok","timestamp":1716761144725,"user_tz":240,"elapsed":159,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cf344424-cbee-4856-b868-5bcc226bea9c"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3 4'\n","5th char = ' 5'\n"]}]},{"cell_type":"markdown","source":["# Explora tests- debug means code for diff seq lens"],"metadata":{"id":"ecUNF1uRd7hk"}},{"cell_type":"markdown","source":["Explore internals of `get_MLPs_actv_mean()`"],"metadata":{"id":"zBIqSW7FeGeP"}},{"cell_type":"code","source":["means_dataset = dataset_2"],"metadata":{"id":"XGliyUq-eQK0","executionInfo":{"status":"ok","timestamp":1716761189449,"user_tz":240,"elapsed":620,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["_, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"mlp_out\"),\n","    )\n","n_layers, d_model = model.cfg.n_layers, model.cfg.d_model\n","batch, seq_len = len(means_dataset), means_dataset.max_len\n","means = t.zeros(size=(n_layers, batch, seq_len, d_model), device=model.cfg.device)\n","\n","for layer in range(n_layers):\n","    mlp_output_for_this_layer: Float[Tensor, \"batch seq d_model\"] = means_cache[utils.get_act_name(\"mlp_out\", layer)]\n","    for template_group in means_dataset.groups:  # here, we only have one group\n","        mlp_output_for_this_template = mlp_output_for_this_layer[template_group]\n","        # aggregate all batches\n","        mlp_output_means_for_this_template = einops.reduce(mlp_output_for_this_template, \"batch seq d_model -> seq d_model\", \"mean\")\n","        means[layer, template_group] = mlp_output_means_for_this_template\n","        # at layer, each batch ind is tempalte group (a tensor of size seq d_model)\n","        # is assigned the SAME mean, \"mlp_output_means_for_this_template\""],"metadata":{"id":"ybYpTNFHd-37","executionInfo":{"status":"ok","timestamp":1716761190513,"user_tz":240,"elapsed":23,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["means.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdJsvklUeX4v","executionInfo":{"status":"ok","timestamp":1716761190514,"user_tz":240,"elapsed":23,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e8513e3e-aee1-4ced-e69b-54c81ea98dd8"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([12, 1, 4, 768])"]},"metadata":{},"execution_count":82}]},{"cell_type":"markdown","source":["Instead of making means shape be `n_layers, batch, seq_len, d_model`, using seq_len from means dataset, we should create a means for the specific current input. That means using a new means dataset based on the current input len. (Eg. if \"1 2 3 4 5 6\", make new means dataset that's len 6). This is needed since we need to get a means value for each pos of the input.\n","\n","Thus, define and pass in new dataset, and change this: `batch, seq_len = len(means_dataset), means_dataset.max_len`\n","\n","Then add a NEW HOOK using new dataset. So if generating, need to do this every loop\n","\n","Alt, use zero ablation"],"metadata":{"id":"p3L0E8cPec7s"}},{"cell_type":"markdown","source":["# means dataset for longer prompts"],"metadata":{"id":"6DDK7QaqELh1"}},{"cell_type":"markdown","source":["What does pos dict have to do with mean ablation? Nothing. But SEQ_POS_TO_KEEP is the token pos to NOT ablate."],"metadata":{"id":"zgJJ9q92UNH2"}},{"cell_type":"code","source":["def generate_prompts_list_longer(text, tokens):\n","    prompts_list = []\n","    prompt_dict = {\n","        'corr': str(1),\n","        'incorr': str(2),\n","        'text': text}\n","    tokens_as_strs = model.tokenizer.tokenize(text)\n","    # for i in range(tokens.shape[1]):\n","    for i, tok in enumerate(tokens_as_strs):\n","        prompt_dict['S'+str(i)] = tok\n","    prompts_list.append(prompt_dict)\n","    return prompts_list\n","\n","reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","prompts_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZKVG778QYyn","executionInfo":{"status":"ok","timestamp":1716763581971,"user_tz":240,"elapsed":408,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"972f7d67-650f-4605-a557-dbf665392338"},"execution_count":130,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'corr': '1',\n","  'incorr': '2',\n","  'text': 'What comes after Monday is Tuesday, and two days after is',\n","  'S0': 'What',\n","  'S1': 'Ġcomes',\n","  'S2': 'Ġafter',\n","  'S3': 'ĠMonday',\n","  'S4': 'Ġis',\n","  'S5': 'ĠTuesday',\n","  'S6': ',',\n","  'S7': 'Ġand',\n","  'S8': 'Ġtwo',\n","  'S9': 'Ġdays',\n","  'S10': 'Ġafter',\n","  'S11': 'Ġis'}]"]},"metadata":{},"execution_count":130}]},{"cell_type":"code","source":["model.tokenizer.tokenize(reference_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFjX0Tjx4x89","executionInfo":{"status":"ok","timestamp":1716763583145,"user_tz":240,"elapsed":61,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6e267fba-6491-46aa-b792-7421652e5f3a"},"execution_count":131,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['What',\n"," 'Ġcomes',\n"," 'Ġafter',\n"," 'ĠMonday',\n"," 'Ġis',\n"," 'ĠTuesday',\n"," ',',\n"," 'Ġand',\n"," 'Ġtwo',\n"," 'Ġdays',\n"," 'Ġafter',\n"," 'Ġis']"]},"metadata":{},"execution_count":131}]},{"cell_type":"code","source":["model.to_tokens(reference_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1y5AC3ql409H","executionInfo":{"status":"ok","timestamp":1716763583145,"user_tz":240,"elapsed":60,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d084634b-c729-407d-804f-2fe1e5ca4222"},"execution_count":132,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[50256,  2061,  2058,   706,  3321,   318,  3431,    11,   290,   734,\n","          1528,   706,   318]], device='cuda:0')"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","source":["# pos_dict = {}\n","# for i in range(tokens.shape[1]):\n","#     pos_dict['S'+str(i)] = i\n","# prompts_list_2 = generate_prompts_list_corr(prompts_list)\n","\n","corr_text = \"What comes after X is Y, and two days after is\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","prompts_list_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNMju-7TTavw","executionInfo":{"status":"ok","timestamp":1716763583145,"user_tz":240,"elapsed":60,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6ff837bd-288e-4db1-a744-b27ab82d0b85"},"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'corr': '1',\n","  'incorr': '2',\n","  'text': 'What comes after X is Y, and two days after is',\n","  'S0': 'What',\n","  'S1': 'Ġcomes',\n","  'S2': 'Ġafter',\n","  'S3': 'ĠX',\n","  'S4': 'Ġis',\n","  'S5': 'ĠY',\n","  'S6': ',',\n","  'S7': 'Ġand',\n","  'S8': 'Ġtwo',\n","  'S9': 'Ġdays',\n","  'S10': 'Ġafter',\n","  'S11': 'Ġis'}]"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":["len(model.tokenizer.tokenize(dataset_2.prompts[0]['text']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0QsCG16VXBR","executionInfo":{"status":"ok","timestamp":1716763583145,"user_tz":240,"elapsed":59,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"889bac79-1188-4ef9-8e37-622e5266d98c"},"execution_count":134,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":134}]},{"cell_type":"code","source":["model.tokenizer.tokenize(dataset_2.prompts[0]['text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnTmrQWgVlwZ","executionInfo":{"status":"ok","timestamp":1716763583145,"user_tz":240,"elapsed":59,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"00dd92d7-08fe-4abc-bfd5-1a6bf7562668"},"execution_count":135,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['4', 'Ġ4', 'Ġ11', 'Ġ4']"]},"metadata":{},"execution_count":135}]},{"cell_type":"code","source":["dataset_2.max_len"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDqvR7l0Xja1","executionInfo":{"status":"ok","timestamp":1716763583145,"user_tz":240,"elapsed":58,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bed7e21a-fe8e-4543-8a8b-64bbc9183ba9"},"execution_count":136,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":136}]},{"cell_type":"code","source":["tokens.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJN3Sa-wr8g3","executionInfo":{"status":"ok","timestamp":1716763583145,"user_tz":240,"elapsed":58,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9e4c8a77-d2cf-4e74-aec7-e8e7e61a472c"},"execution_count":137,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 13])"]},"metadata":{},"execution_count":137}]},{"cell_type":"code","source":["corr_tokens.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JuDTjBO83Lqp","executionInfo":{"status":"ok","timestamp":1716763583145,"user_tz":240,"elapsed":57,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"941603e4-bb40-4cd5-d6fe-6890dc072a6f"},"execution_count":138,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 13])"]},"metadata":{},"execution_count":138}]},{"cell_type":"code","source":["dataset_2.toks.long().shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4zUAULKyOvK","executionInfo":{"status":"ok","timestamp":1716763583146,"user_tz":240,"elapsed":58,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"013da7fa-a4a0-4526-9df2-21c77a05d016"},"execution_count":139,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 4])"]},"metadata":{},"execution_count":139}]},{"cell_type":"code","source":["tokens = tokens[:, 1:]"],"metadata":{"id":"QPQ0WXKor-d5","executionInfo":{"status":"ok","timestamp":1716763583146,"user_tz":240,"elapsed":57,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":140,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# this turns string into LIST OF TOKEN IDS\n","tokens = model.to_tokens(reference_text).to(device)\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","\n","# this turns it INTO LIST OF STRINGS WITH SPACE CHAR IN FRONT\n","# each string in list correspond to tokens from token id list\n","model.tokenizer.tokenize(text) # this doesn't use prepend bos\n","```\n","\n"],"metadata":{"id":"aQ9aHRYr5ebD"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","# heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","heads_not_ablate = []  # ablate all heads but not MLPs\n","mlps_not_ablate = []  # ablate all MLPs\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","# CIRCUIT = {}\n","# SEQ_POS_TO_KEEP = {}\n","# for i in range(len(model.tokenizer.tokenize(dataset_2.prompts[0]['text']))):\n","#     CIRCUIT['S'+str(i)] = heads_not_ablate\n","#     if i == len(model.tokenizer.tokenize(dataset_2.prompts[0]['text'])) - 1:\n","#         SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","#     else:\n","#         SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","# model = add_ablation_hook_head(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","# model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","# logits = model(dataset.toks)\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"ubcTjRDf6ETO","executionInfo":{"status":"ok","timestamp":1716763583146,"user_tz":240,"elapsed":57,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a4767de1-f689-49da-d4e2-c6acbcf48115"},"execution_count":141,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' Wednesday'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":141}]},{"cell_type":"code","source":["model.to_string(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3ps_U-OzjHB","executionInfo":{"status":"ok","timestamp":1716763583146,"user_tz":240,"elapsed":56,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4c85b716-badc-4662-b966-70f29251c00e"},"execution_count":142,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['What comes after Monday is Tuesday, and two days after is']"]},"metadata":{},"execution_count":142}]},{"cell_type":"code","source":["next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"n6UuwO5M6Av2","executionInfo":{"status":"ok","timestamp":1716763583146,"user_tz":240,"elapsed":56,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e96136f2-1215-4b8e-bf62-beef2898edd8"},"execution_count":143,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' Wednesday'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":143}]},{"cell_type":"markdown","source":["# Mean ablate for model generation"],"metadata":{"id":"SW9uBUML2gc4"}},{"cell_type":"markdown","source":["First try just adding the hook at all the original text's positions, but not at the new positions"],"metadata":{"id":"c731ajnuGcwt"}},{"cell_type":"code","source":["reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"What comes after X is Y, and two days after is\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","heads_not_ablate = []  # ablate all heads but not MLPs\n","mlps_not_ablate = []  # ablate all MLPs\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    # Define new input sequence, by appending the previously generated token\n","    tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n","    print(tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    # COMMENT THIS OUT FIRST\n","    # model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HkLVFGjLGbwH","executionInfo":{"status":"ok","timestamp":1716763927931,"user_tz":240,"elapsed":159,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b1007688-f504-4f84-cf67-8e2834945cef"},"execution_count":158,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: 'What comes after Monday is Tuesday, and two days after is'\n","13th char = ' Z'\n","torch.Size([1, 13])\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Z'\n","14th char = 'ayn'\n","torch.Size([1, 14])\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Zayn'\n","15th char = ' Malik'\n","torch.Size([1, 15])\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Zayn Malik'\n","16th char = \"'s\"\n","torch.Size([1, 16])\n","Sequence so far: \"What comes after Monday is Tuesday, and two days after is Zayn Malik's\"\n","17th char = ' birthday'\n","torch.Size([1, 17])\n","Sequence so far: \"What comes after Monday is Tuesday, and two days after is Zayn Malik's birthday\"\n"]}]},{"cell_type":"markdown","source":["Now try adding hook in the loop"],"metadata":{"id":"xjSz9rbFGaAe"}},{"cell_type":"markdown","source":["We cannot just do tokens = t.cat because the new corrupted string that adds on the previous token, when tokenized, can have different number of tokens that the clean tokens (tokens)"],"metadata":{"id":"Pq4QTz1kK4P-"}},{"cell_type":"code","source":["reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"What comes after X is Y, and two days after is\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","heads_not_ablate = []  # ablate all heads but not MLPs\n","mlps_not_ablate = []  # ablate all MLPs\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","for iter in range(4):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    # Define new input sequence, by appending the previously generated token\n","    # tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","    # print(iter)\n","    # if iter == 3:\n","    #     pdb.set_trace()\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9XNF5Yx7eYSt","executionInfo":{"status":"ok","timestamp":1716765525881,"user_tz":240,"elapsed":3996,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2b03e7a3-09cd-4085-8f3f-18b6cfe36c74"},"execution_count":170,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: 'What comes after Monday is Tuesday, and two days after is'\n","13th char = ' Z'\n","What comes after Monday is Tuesday, and two days after is Z\n","tokens:  torch.Size([1, 13])\n","What comes after X is Y, and two days after is Z\n","corr_tokens:  torch.Size([1, 14])\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Z'\n","14th char = '?'\n","What comes after Monday is Tuesday, and two days after is Z?\n","tokens:  torch.Size([1, 14])\n","What comes after X is Y, and two days after is Z?\n","corr_tokens:  torch.Size([1, 15])\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Z?'\n","15th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Z?\n","\n","tokens:  torch.Size([1, 15])\n","What comes after X is Y, and two days after is Z?\n","\n","corr_tokens:  torch.Size([1, 16])\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Z?\\n'\n","16th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Z?\n","\n","\n","tokens:  torch.Size([1, 15])\n","What comes after X is Y, and two days after is Z?\n","\n","\n","corr_tokens:  torch.Size([1, 16])\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Z?\\n\\n'\n"]}]},{"cell_type":"code","source":["reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"5 3 9\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","heads_not_ablate = []  # ablate all heads but not MLPs\n","mlps_not_ablate = []  # ablate all MLPs\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czgAuSmSAzYe","executionInfo":{"status":"ok","timestamp":1716766077421,"user_tz":240,"elapsed":4171,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d5851dff-d051-441b-d308-dcd0aeb24ef0"},"execution_count":171,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3'\n","4th char = '.'\n","1 2 3.\n","tokens:  torch.Size([1, 4])\n","5 3 9.\n","corr_tokens:  torch.Size([1, 5])\n","Sequence so far: '1 2 3.'\n","5th char = '5'\n","1 2 3.5\n","tokens:  torch.Size([1, 5])\n","5 3 9.5\n","corr_tokens:  torch.Size([1, 6])\n","Sequence so far: '1 2 3.5'\n","6th char = ' 3'\n","1 2 3.5 3\n","tokens:  torch.Size([1, 6])\n","5 3 9.5 3\n","corr_tokens:  torch.Size([1, 7])\n","Sequence so far: '1 2 3.5 3'\n","7th char = '.'\n","1 2 3.5 3.\n","tokens:  torch.Size([1, 7])\n","5 3 9.5 3.\n","corr_tokens:  torch.Size([1, 8])\n","Sequence so far: '1 2 3.5 3.'\n","8th char = '5'\n","1 2 3.5 3.5\n","tokens:  torch.Size([1, 8])\n","5 3 9.5 3.5\n","corr_tokens:  torch.Size([1, 9])\n","Sequence so far: '1 2 3.5 3.5'\n"]}]},{"cell_type":"markdown","source":["# logit diff for mult tok answers"],"metadata":{"id":"jtaV1q3SBHow"}},{"cell_type":"code","execution_count":188,"metadata":{"executionInfo":{"elapsed":478,"status":"ok","timestamp":1716766641046,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"_4XdrNdtQ-hb"},"outputs":[],"source":["reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","\n","logits, cache = model.run_with_cache(tokens)\n","# probs = logits.softmax(dim=-1)"]},{"cell_type":"code","execution_count":189,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1716766643891,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"801faef0-4308-43b8-bccb-738cd3a666db","id":"4VkHdXtxQ-hi"},"outputs":[{"output_type":"stream","name":"stdout","text":["' 4'\n"]}],"source":["next_token = logits[0, -1].argmax(dim=-1)  # logits have shape [1, sequence_length, vocab_size]\n","next_char = model.to_string(next_token)\n","print(repr(next_char))"]},{"cell_type":"code","execution_count":190,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":874,"status":"ok","timestamp":1716766645656,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"1e3fe04a-fcd7-43ed-df80-259c2f9e8b76","id":"0D_YAm2eQ-hi"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '<|endoftext|>1 2 3'\n","5th char = ' 4'\n","6th char = ' 5'\n","7th char = ' 6'\n","8th char = ' 7'\n","9th char = ' 8'\n","10th char = ' 9'\n","11th char = ' 10'\n","12th char = ' 11'\n","13th char = ' 12'\n","14th char = ' 13'\n"]}],"source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","for i in range(10):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","    # Define new input sequence, by appending the previously generated token\n","    tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)"]},{"cell_type":"markdown","source":["Rerun 'load datasets'"],"metadata":{"id":"E25ECYbREpDU"}},{"cell_type":"code","source":["# prompts_list = generate_prompts_list(1, 2)\n","# pos_dict = {}\n","# for i in range(len(model.tokenizer.tokenize(prompts_list[0]['text']))):\n","#     pos_dict['S'+str(i)] = i\n","# dataset = Dataset(prompts_list, pos_dict, model.tokenizer)\n","# prompts_list"],"metadata":{"id":"T66sarceBMDJ","executionInfo":{"status":"ok","timestamp":1716766495534,"user_tz":240,"elapsed":611,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":181,"outputs":[]},{"cell_type":"code","source":["reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","corr_ans_tokLen = 0\n","corr_ans = ' 5'\n","ans_so_far = ''\n","for i in range(10):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","    # Define new input sequence, by appending the previously generated token\n","    tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","    ans_so_far += next_char\n","    corr_ans_tokLen += 1\n","    if ans_so_far == corr_ans:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WgbtY5fFPb71","executionInfo":{"status":"ok","timestamp":1716766850560,"user_tz":240,"elapsed":958,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b21b15f5-17ef-479a-e9a1-7a6a504cb017"},"execution_count":195,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3'\n","4th char = ' 4'\n"]}]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","logits_minimal = model(dataset.toks)\n","new_score = get_logit_diff(logits_minimal, dataset)\n","new_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMEN0jnkZTWt","executionInfo":{"status":"ok","timestamp":1716763652834,"user_tz":240,"elapsed":1363,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d7e38a39-0343-4a18-ae68-100584b77b34"},"execution_count":150,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(6.0631, device='cuda:0')"]},"metadata":{},"execution_count":150}]},{"cell_type":"code","source":["reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"5 3 9\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","heads_not_ablate = []  # ablate all heads but not MLPs\n","mlps_not_ablate = []  # ablate all MLPs\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bk8UT5lNF0SX","executionInfo":{"status":"ok","timestamp":1716767554138,"user_tz":240,"elapsed":5535,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d89336cb-8b11-4393-80ed-e6ec99d3905e"},"execution_count":200,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3'\n","4th char = '.'\n","1 2 3.\n","5 3 9.\n","\n","\n","Sequence so far: '1 2 3.'\n","New score: 0.8616657257080078\n","tensor(0.8617, device='cuda:0')\n","5th char = '5'\n","1 2 3.5\n","5 3 9.5\n","\n","\n","Sequence so far: '1 2 3.5'\n","New score: 0.861663818359375\n","tensor(1.7233, device='cuda:0')\n","6th char = ' 3'\n","1 2 3.5 3\n","5 3 9.5 3\n","\n","\n","Sequence so far: '1 2 3.5 3'\n","New score: 0.861663818359375\n","tensor(2.5850, device='cuda:0')\n","7th char = '.'\n","1 2 3.5 3.\n","5 3 9.5 3.\n","\n","\n","Sequence so far: '1 2 3.5 3.'\n","New score: 0.8616619110107422\n","tensor(3.4467, device='cuda:0')\n","8th char = '5'\n","1 2 3.5 3.5\n","5 3 9.5 3.5\n","\n","\n","Sequence so far: '1 2 3.5 3.5'\n","New score: 0.8616676330566406\n","tensor(4.3083, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["# output genr ablation expms"],"metadata":{"id":"H5H-d2URUSVJ"}},{"cell_type":"markdown","source":["## ablate just head 9.1 and MLP 9"],"metadata":{"id":"cvDoLG2YR73k"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","heads_not_ablate.remove((9, 1))\n","mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","\n","reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"5 3 9\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716766950127,"user_tz":240,"elapsed":5811,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8bddea71-561b-4c8d-93f8-e313aedcb8a9","id":"QEXQNkTkR-qP"},"execution_count":197,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3'\n","4th char = ' 4'\n","1 2 3 4\n","5 3 9 4\n","\n","\n","Sequence so far: '1 2 3 4'\n","New score: 6.063114166259766\n","tensor(6.0631, device='cuda:0')\n","5th char = ' 5'\n","1 2 3 4 5\n","5 3 9 4 5\n","\n","\n","Sequence so far: '1 2 3 4 5'\n","New score: 6.063114166259766\n","tensor(12.1262, device='cuda:0')\n","6th char = ' 6'\n","1 2 3 4 5 6\n","5 3 9 4 5 6\n","\n","\n","Sequence so far: '1 2 3 4 5 6'\n","New score: 6.063114166259766\n","tensor(18.1893, device='cuda:0')\n","7th char = ' 7'\n","1 2 3 4 5 6 7\n","5 3 9 4 5 6 7\n","\n","\n","Sequence so far: '1 2 3 4 5 6 7'\n","New score: 6.063114166259766\n","tensor(24.2525, device='cuda:0')\n","8th char = ' 8'\n","1 2 3 4 5 6 7 8\n","5 3 9 4 5 6 7 8\n","\n","\n","Sequence so far: '1 2 3 4 5 6 7 8'\n","New score: 6.063114166259766\n","tensor(30.3156, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## ablate 4.4, 7.11, 9.1"],"metadata":{"id":"NXLKleFoSQMO"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","head_to_remove = ([(9, 1), (4,4), (7,11)])\n","heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","mlps_not_ablate = [layer for layer in range(12)]\n","\n","reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"5 3 9\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAIEi0FZT3TN","executionInfo":{"status":"ok","timestamp":1716771023215,"user_tz":240,"elapsed":4947,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a096f7ee-96da-45d9-debf-e48f9f7a6d00"},"execution_count":216,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3'\n","4th char = ' 4'\n","1 2 3 4\n","5 3 9 4\n","\n","\n","Sequence so far: '1 2 3 4'\n","New score: 3.366814136505127\n","tensor(3.3668, device='cuda:0')\n","5th char = ' 5'\n","1 2 3 4 5\n","5 3 9 4 5\n","\n","\n","Sequence so far: '1 2 3 4 5'\n","New score: 3.3668127059936523\n","tensor(6.7336, device='cuda:0')\n","6th char = ' 6'\n","1 2 3 4 5 6\n","5 3 9 4 5 6\n","\n","\n","Sequence so far: '1 2 3 4 5 6'\n","New score: 3.366811752319336\n","tensor(10.1004, device='cuda:0')\n","7th char = ' 7'\n","1 2 3 4 5 6 7\n","5 3 9 4 5 6 7\n","\n","\n","Sequence so far: '1 2 3 4 5 6 7'\n","New score: 3.366806983947754\n","tensor(13.4672, device='cuda:0')\n","8th char = ' 8'\n","1 2 3 4 5 6 7 8\n","5 3 9 4 5 6 7 8\n","\n","\n","Sequence so far: '1 2 3 4 5 6 7 8'\n","New score: 3.3668060302734375\n","tensor(16.8341, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## ablate mlp 9"],"metadata":{"id":"StWTdvI1hz4H"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","# head_to_remove = ([(9, 1), (4,4), (7,11)])\n","# heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","\n","reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"5 3 9\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716771088851,"user_tz":240,"elapsed":5094,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b0d03ba4-e760-41c6-f47b-7b212c1a06ab","id":"HpNmy5JShz4Q"},"execution_count":218,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3'\n","4th char = ' 4'\n","1 2 3 4\n","5 3 9 4\n","\n","\n","Sequence so far: '1 2 3 4'\n","New score: 0.8109745979309082\n","tensor(0.8110, device='cuda:0')\n","5th char = ' 5'\n","1 2 3 4 5\n","5 3 9 4 5\n","\n","\n","Sequence so far: '1 2 3 4 5'\n","New score: 0.8109769821166992\n","tensor(1.6220, device='cuda:0')\n","6th char = ' 6'\n","1 2 3 4 5 6\n","5 3 9 4 5 6\n","\n","\n","Sequence so far: '1 2 3 4 5 6'\n","New score: 0.810976505279541\n","tensor(2.4329, device='cuda:0')\n","7th char = ' 7'\n","1 2 3 4 5 6 7\n","5 3 9 4 5 6 7\n","\n","\n","Sequence so far: '1 2 3 4 5 6 7'\n","New score: 0.8109769821166992\n","tensor(3.2439, device='cuda:0')\n","8th char = ' 8'\n","1 2 3 4 5 6 7 8\n","5 3 9 4 5 6 7 8\n","\n","\n","Sequence so far: '1 2 3 4 5 6 7 8'\n","New score: 0.8109760284423828\n","tensor(4.0549, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## ablate 4.4, 7.11, 9.1 and mlp 9"],"metadata":{"id":"pJ804aWuhqu8"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","head_to_remove = ([(9, 1), (4,4), (7,11)])\n","heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","\n","reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"5 3 9\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716768401638,"user_tz":240,"elapsed":5745,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b6762f4d-b8e1-4768-f381-84fcd84f0fc1","id":"6Yomb5yPhqvJ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3'\n","4th char = ' 3'\n","1 2 3 3\n","5 3 9 3\n","\n","\n","Sequence so far: '1 2 3 3'\n","New score: -1.04522705078125\n","tensor(-1.0452, device='cuda:0')\n","5th char = ' 3'\n","1 2 3 3 3\n","5 3 9 3 3\n","\n","\n","Sequence so far: '1 2 3 3 3'\n","New score: -1.0452289581298828\n","tensor(-2.0905, device='cuda:0')\n","6th char = ' 3'\n","1 2 3 3 3 3\n","5 3 9 3 3 3\n","\n","\n","Sequence so far: '1 2 3 3 3 3'\n","New score: -1.0452284812927246\n","tensor(-3.1357, device='cuda:0')\n","7th char = ' 3'\n","1 2 3 3 3 3 3\n","5 3 9 3 3 3 3\n","\n","\n","Sequence so far: '1 2 3 3 3 3 3'\n","New score: -1.0452260971069336\n","tensor(-4.1809, device='cuda:0')\n","8th char = ' 3'\n","1 2 3 3 3 3 3 3\n","5 3 9 3 3 3 3 3\n","\n","\n","Sequence so far: '1 2 3 3 3 3 3 3'\n","New score: -1.04522705078125\n","tensor(-5.2261, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## 6.2, 4.1, 7.1"],"metadata":{"id":"JSQkrPMHiJYI"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","head_to_remove = ([(6, 2), (4,1), (7,1)])\n","heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","# mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","mlps_not_ablate = [layer for layer in range(12)]\n","\n","reference_text = \"1 2 3\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"7 3 5\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cw9pM5VDhdLu","executionInfo":{"status":"ok","timestamp":1716770965532,"user_tz":240,"elapsed":4152,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"004d2a0b-9cf1-4464-b3aa-bcbfab25604a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '1 2 3'\n","4th char = ' 4'\n","1 2 3 4\n","7 3 5 4\n","\n","\n","Sequence so far: '1 2 3 4'\n","New score: 6.005740165710449\n","tensor(6.0057, device='cuda:0')\n","5th char = ' 5'\n","1 2 3 4 5\n","7 3 5 4 5\n","\n","\n","Sequence so far: '1 2 3 4 5'\n","New score: 6.005738735198975\n","tensor(12.0115, device='cuda:0')\n","6th char = ' 6'\n","1 2 3 4 5 6\n","7 3 5 4 5 6\n","\n","\n","Sequence so far: '1 2 3 4 5 6'\n","New score: 6.005740642547607\n","tensor(18.0172, device='cuda:0')\n","7th char = ' 7'\n","1 2 3 4 5 6 7\n","7 3 5 4 5 6 7\n","\n","\n","Sequence so far: '1 2 3 4 5 6 7'\n","New score: 6.005740165710449\n","tensor(24.0230, device='cuda:0')\n","8th char = ' 8'\n","1 2 3 4 5 6 7 8\n","7 3 5 4 5 6 7 8\n","\n","\n","Sequence so far: '1 2 3 4 5 6 7 8'\n","New score: 6.005738735198975\n","tensor(30.0287, device='cuda:0')\n"]}]},{"cell_type":"code","source":["heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","head_to_remove = ([(9, 1), (4,4), (7,11)])\n","heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","len(heads_not_ablate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YhE8CDftXmMj","executionInfo":{"status":"ok","timestamp":1716768388205,"user_tz":240,"elapsed":408,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"345471f0-241d-4092-c50e-5454a7da7bc1"},"execution_count":204,"outputs":[{"output_type":"execute_result","data":{"text/plain":["141"]},"metadata":{},"execution_count":204}]},{"cell_type":"markdown","source":["# seqcont word problems"],"metadata":{"id":"aB5OqTVcYigX"}},{"cell_type":"markdown","source":["## clean"],"metadata":{"id":"TVyYiX9DZh3V"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","# head_to_remove = ([(9, 1), (4,4), (7,11)])\n","# heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","# mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","mlps_not_ablate = [layer for layer in range(12)]\n","\n","reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"What comes after X is Y, and two days after is\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VI4ucuuBYtOZ","executionInfo":{"status":"ok","timestamp":1716768879801,"user_tz":240,"elapsed":6653,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a944e435-472c-4661-86a6-168ac534a96c"},"execution_count":207,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: 'What comes after Monday is Tuesday, and two days after is'\n","13th char = ' Wednesday'\n","What comes after Monday is Tuesday, and two days after is Wednesday\n","What comes after X is Y, and two days after is Wednesday\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday'\n","New score: -0.49404430389404297\n","tensor(-0.4940, device='cuda:0')\n","14th char = '.'\n","What comes after Monday is Tuesday, and two days after is Wednesday.\n","What comes after X is Y, and two days after is Wednesday.\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday.'\n","New score: -0.49404430389404297\n","tensor(-0.9881, device='cuda:0')\n","15th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Wednesday.\n","\n","What comes after X is Y, and two days after is Wednesday.\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday.\\n'\n","New score: -0.49404430389404297\n","tensor(-1.4821, device='cuda:0')\n","16th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Wednesday.\n","\n","\n","What comes after X is Y, and two days after is Wednesday.\n","\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday.\\n\\n'\n","New score: -0.49404430389404297\n","tensor(-1.9762, device='cuda:0')\n","16th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Wednesday.\n","\n","\n","\n","What comes after X is Y, and two days after is Wednesday.\n","\n","\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday.\\n\\n\\n'\n","New score: -0.49404430389404297\n","tensor(-2.4702, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## corrupt the subcircuit"],"metadata":{"id":"-D7DD-WPZzQr"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","head_to_remove = ([(9, 1), (4,4), (7,11)])\n","heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","\n","reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"What comes after X is Y, and two days after is\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLvOKdGAYh9i","executionInfo":{"status":"ok","timestamp":1716768650720,"user_tz":240,"elapsed":6824,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"20741800-41a3-4ff5-d2f4-e4fffc63737f"},"execution_count":206,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: 'What comes after Monday is Tuesday, and two days after is'\n","13th char = ' the'\n","What comes after Monday is Tuesday, and two days after is the\n","What comes after X is Y, and two days after is the\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the'\n","New score: -0.8908572196960449\n","tensor(-0.8909, device='cuda:0')\n","14th char = ' day'\n","What comes after Monday is Tuesday, and two days after is the day\n","What comes after X is Y, and two days after is the day\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the day'\n","New score: -0.8908572196960449\n","tensor(-1.7817, device='cuda:0')\n","15th char = ' of'\n","What comes after Monday is Tuesday, and two days after is the day of\n","What comes after X is Y, and two days after is the day of\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the day of'\n","New score: -0.8908572196960449\n","tensor(-2.6726, device='cuda:0')\n","16th char = ' the'\n","What comes after Monday is Tuesday, and two days after is the day of the\n","What comes after X is Y, and two days after is the day of the\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the day of the'\n","New score: -0.8908576965332031\n","tensor(-3.5634, device='cuda:0')\n","17th char = ' first'\n","What comes after Monday is Tuesday, and two days after is the day of the first\n","What comes after X is Y, and two days after is the day of the first\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the day of the first'\n","New score: -0.8908586502075195\n","tensor(-4.4543, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## corrupt 9.1 and mlp9"],"metadata":{"id":"r6x7xHH0aa9T"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","heads_not_ablate.remove((9,1))\n","# head_to_remove = ([(9, 1), (4,4), (7,11)])\n","# heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","\n","reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"What comes after X is Y, and two days after is\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716769171949,"user_tz":240,"elapsed":7490,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e8e6be8a-2cde-470a-ab16-c5a56e8796a0","id":"IYAgDtL7aa9e"},"execution_count":209,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: 'What comes after Monday is Tuesday, and two days after is'\n","13th char = ' the'\n","What comes after Monday is Tuesday, and two days after is the\n","What comes after X is Y, and two days after is the\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the'\n","New score: -0.8888366222381592\n","tensor(-0.8888, device='cuda:0')\n","14th char = ' day'\n","What comes after Monday is Tuesday, and two days after is the day\n","What comes after X is Y, and two days after is the day\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the day'\n","New score: -0.8888366222381592\n","tensor(-1.7777, device='cuda:0')\n","15th char = ' of'\n","What comes after Monday is Tuesday, and two days after is the day of\n","What comes after X is Y, and two days after is the day of\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the day of'\n","New score: -0.8888366222381592\n","tensor(-2.6665, device='cuda:0')\n","16th char = ' the'\n","What comes after Monday is Tuesday, and two days after is the day of the\n","What comes after X is Y, and two days after is the day of the\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the day of the'\n","New score: -0.8888368606567383\n","tensor(-3.5553, device='cuda:0')\n","17th char = ' first'\n","What comes after Monday is Tuesday, and two days after is the day of the first\n","What comes after X is Y, and two days after is the day of the first\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is the day of the first'\n","New score: -0.8888368606567383\n","tensor(-4.4442, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## ablate just 9.1"],"metadata":{"id":"che-P0kUa0UK"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","heads_not_ablate.remove((9,1))\n","# head_to_remove = ([(9, 1), (4,4), (7,11)])\n","# heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","# mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","mlps_not_ablate = [layer for layer in range(12)]\n","\n","reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"What comes after X is Y, and two days after is\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w-oJzA2jauAL","executionInfo":{"status":"ok","timestamp":1716769213119,"user_tz":240,"elapsed":6337,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"39f8d7d4-5623-4640-ad67-171dfc8c43b7"},"execution_count":211,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: 'What comes after Monday is Tuesday, and two days after is'\n","13th char = ' Monday'\n","What comes after Monday is Tuesday, and two days after is Monday\n","What comes after X is Y, and two days after is Monday\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Monday'\n","New score: -0.49973535537719727\n","tensor(-0.4997, device='cuda:0')\n","14th char = '.'\n","What comes after Monday is Tuesday, and two days after is Monday.\n","What comes after X is Y, and two days after is Monday.\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Monday.'\n","New score: -0.49973535537719727\n","tensor(-0.9995, device='cuda:0')\n","15th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Monday.\n","\n","What comes after X is Y, and two days after is Monday.\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Monday.\\n'\n","New score: -0.49973535537719727\n","tensor(-1.4992, device='cuda:0')\n","16th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Monday.\n","\n","\n","What comes after X is Y, and two days after is Monday.\n","\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Monday.\\n\\n'\n","New score: -0.49973535537719727\n","tensor(-1.9989, device='cuda:0')\n","16th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Monday.\n","\n","\n","\n","What comes after X is Y, and two days after is Monday.\n","\n","\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Monday.\\n\\n\\n'\n","New score: -0.49973487854003906\n","tensor(-2.4987, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## ablate random head"],"metadata":{"id":"aJTIx8YobKLf"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","heads_not_ablate.remove((6, 2))\n","# head_to_remove = ([(9, 1), (4,4), (7,11)])\n","# heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","# mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","mlps_not_ablate = [layer for layer in range(12)]\n","\n","reference_text = \"What comes after Monday is Tuesday, and two days after is\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"What comes after X is Y, and two days after is\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716769325712,"user_tz":240,"elapsed":6977,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"da812518-1d7a-4ce6-ca2c-74abff868dca","id":"hnxA5BlVbKLh"},"execution_count":212,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: 'What comes after Monday is Tuesday, and two days after is'\n","13th char = ' Wednesday'\n","What comes after Monday is Tuesday, and two days after is Wednesday\n","What comes after X is Y, and two days after is Wednesday\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday'\n","New score: -0.49280452728271484\n","tensor(-0.4928, device='cuda:0')\n","14th char = '.'\n","What comes after Monday is Tuesday, and two days after is Wednesday.\n","What comes after X is Y, and two days after is Wednesday.\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday.'\n","New score: -0.49280452728271484\n","tensor(-0.9856, device='cuda:0')\n","15th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Wednesday.\n","\n","What comes after X is Y, and two days after is Wednesday.\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday.\\n'\n","New score: -0.49280452728271484\n","tensor(-1.4784, device='cuda:0')\n","16th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Wednesday.\n","\n","\n","What comes after X is Y, and two days after is Wednesday.\n","\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday.\\n\\n'\n","New score: -0.49280452728271484\n","tensor(-1.9712, device='cuda:0')\n","16th char = '\\n'\n","What comes after Monday is Tuesday, and two days after is Wednesday.\n","\n","\n","\n","What comes after X is Y, and two days after is Wednesday.\n","\n","\n","\n","\n","\n","Sequence so far: 'What comes after Monday is Tuesday, and two days after is Wednesday.\\n\\n\\n'\n","New score: -0.49280333518981934\n","tensor(-2.4640, device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["# 2 4 6 8"],"metadata":{"id":"v1XVKzlQguwa"}},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","# head_to_remove = ([(6, 2), (4,1), (7,1)])\n","# heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","# mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","mlps_not_ablate = [layer for layer in range(12)]\n","\n","reference_text = \"2 4 6\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"7 3 5\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttJTzl5WiTBT","executionInfo":{"status":"ok","timestamp":1716771197873,"user_tz":240,"elapsed":4906,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2566b191-a7f7-48c4-d924-c2778cd11c9d"},"execution_count":220,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '2 4 6'\n","4th char = ' 6'\n","2 4 6 6\n","7 3 5 6\n","\n","\n","Sequence so far: '2 4 6 6'\n","New score: 0.052596092224121094\n","tensor(0.0526, device='cuda:0')\n","5th char = ' 6'\n","2 4 6 6 6\n","7 3 5 6 6\n","\n","\n","Sequence so far: '2 4 6 6 6'\n","New score: 0.05259895324707031\n","tensor(0.1052, device='cuda:0')\n","6th char = ' 6'\n","2 4 6 6 6 6\n","7 3 5 6 6 6\n","\n","\n","Sequence so far: '2 4 6 6 6 6'\n","New score: 0.0525970458984375\n","tensor(0.1578, device='cuda:0')\n","7th char = ' 6'\n","2 4 6 6 6 6 6\n","7 3 5 6 6 6 6\n","\n","\n","Sequence so far: '2 4 6 6 6 6 6'\n","New score: 0.052596092224121094\n","tensor(0.2104, device='cuda:0')\n","8th char = ' 6'\n","2 4 6 6 6 6 6 6\n","7 3 5 6 6 6 6 6\n","\n","\n","Sequence so far: '2 4 6 6 6 6 6 6'\n","New score: 0.0525970458984375\n","tensor(0.2630, device='cuda:0')\n"]}]},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","head_to_remove = ([(6, 2), (4,1), (7,1)])\n","heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","# mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","mlps_not_ablate = [layer for layer in range(12)]\n","\n","reference_text = \"2 4 6\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"7 3 5\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipvvw0d2hIg-","executionInfo":{"status":"ok","timestamp":1716770899269,"user_tz":240,"elapsed":4673,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cc7b9d14-efe7-487d-bdbd-b49723c82a3e"},"execution_count":214,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '2 4 6'\n","4th char = ' 6'\n","2 4 6 6\n","7 3 5 6\n","\n","\n","Sequence so far: '2 4 6 6'\n","New score: 0.05226325988769531\n","tensor(0.0523, device='cuda:0')\n","5th char = ' 6'\n","2 4 6 6 6\n","7 3 5 6 6\n","\n","\n","Sequence so far: '2 4 6 6 6'\n","New score: 0.05226469039916992\n","tensor(0.1045, device='cuda:0')\n","6th char = ' 6'\n","2 4 6 6 6 6\n","7 3 5 6 6 6\n","\n","\n","Sequence so far: '2 4 6 6 6 6'\n","New score: 0.052263736724853516\n","tensor(0.1568, device='cuda:0')\n","7th char = ' 6'\n","2 4 6 6 6 6 6\n","7 3 5 6 6 6 6\n","\n","\n","Sequence so far: '2 4 6 6 6 6 6'\n","New score: 0.0522613525390625\n","tensor(0.2091, device='cuda:0')\n","8th char = ' 6'\n","2 4 6 6 6 6 6 6\n","7 3 5 6 6 6 6 6\n","\n","\n","Sequence so far: '2 4 6 6 6 6 6 6'\n","New score: 0.052263736724853516\n","tensor(0.2613, device='cuda:0')\n"]}]},{"cell_type":"code","source":["## heads_not_ablate is components to keep\n","heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]  # unablated\n","# heads_not_ablate = [(9, 1)]\n","head_to_remove = ([(9, 1), (4,4), (7,11)])\n","heads_not_ablate = [x for x in heads_not_ablate if (x not in head_to_remove)]\n","mlps_not_ablate = [layer for layer in range(12) if layer != 9]\n","\n","reference_text = \"2 4 6\"\n","tokens = model.to_tokens(reference_text).to(device)\n","prompts_list = generate_prompts_list_longer(reference_text, tokens)\n","\n","corr_text = \"7 3 5\"\n","corr_tokens = model.to_tokens(corr_text).to(device)\n","prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","\n","model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","tokens = tokens[:, 1:] # get rid of prepend bos when using model.to_tokens\n","logits = model(tokens)\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","\n","total_score = 0\n","\n","print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","# for i in range(corr_ans_tokLen):\n","for i in range(5):\n","    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n","\n","    reference_text = reference_text + next_char\n","    tokens = model.to_tokens(reference_text).to(device)\n","    tokens = tokens[:, 1:]\n","    print(reference_text)\n","    # print('tokens: ', tokens.shape)\n","\n","    ##\n","    # get new ablation dataset\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    corr_text = corr_text + next_char\n","    corr_tokens = model.to_tokens(reference_text).to(device)\n","    prompts_list_2 = generate_prompts_list_longer(corr_text, corr_tokens)\n","    print(corr_text)\n","    # print('corr_tokens: ', corr_tokens.shape)\n","\n","    pos_dict = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        pos_dict['S'+str(i)] = i\n","\n","    dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)\n","\n","    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","\n","    ##\n","\n","    # Pass our new sequence through the model, to get new output\n","    logits = model(tokens)\n","    # Get the predicted token at the end of our sequence\n","    next_token = logits[0, -1].argmax(dim=-1)\n","    # Decode and print the result\n","    next_char = model.to_string(next_token)\n","\n","    print('\\n')\n","    print(f\"Sequence so far: {model.to_string(tokens)[0]!r}\")\n","\n","    new_score = get_logit_diff(logits, dataset)\n","    total_score += new_score\n","    print(f\"New score: {new_score}\")\n","    print(total_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-NKP3Zhgy_z","executionInfo":{"status":"ok","timestamp":1716770853266,"user_tz":240,"elapsed":5863,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"58cfd48f-ec8d-4ae3-9f55-d30000db5e34"},"execution_count":213,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence so far: '2 4 6'\n","4th char = ' 6'\n","2 4 6 6\n","7 3 5 6\n","\n","\n","Sequence so far: '2 4 6 6'\n","New score: 1.1727123260498047\n","tensor(1.1727, device='cuda:0')\n","5th char = ' 7'\n","2 4 6 6 7\n","7 3 5 6 7\n","\n","\n","Sequence so far: '2 4 6 6 7'\n","New score: 1.1727099418640137\n","tensor(2.3454, device='cuda:0')\n","6th char = ' 8'\n","2 4 6 6 7 8\n","7 3 5 6 7 8\n","\n","\n","Sequence so far: '2 4 6 6 7 8'\n","New score: 1.1727094650268555\n","tensor(3.5181, device='cuda:0')\n","7th char = ' 9'\n","2 4 6 6 7 8 9\n","7 3 5 6 7 8 9\n","\n","\n","Sequence so far: '2 4 6 6 7 8 9'\n","New score: 1.1727094650268555\n","tensor(4.6908, device='cuda:0')\n","8th char = ' 10'\n","2 4 6 6 7 8 9 10\n","7 3 5 6 7 8 9 10\n","\n","\n","Sequence so far: '2 4 6 6 7 8 9 10'\n","New score: 1.1727147102355957\n","tensor(5.8636, device='cuda:0')\n"]}]}]}